---
id: improving-lakehouse-performance
title: Improving Lakehouse Performance â€” Dos & Donâ€™ts
sidebar_label: Lakehouse Performance
description: Learn the essential performance optimization techniques for the Databricks Lakehouse. A story-driven, SEO-friendly guide covering file management, Z-ORDER, caching, Photon, SQL tuning, and architecture best practices.
keywords:
  - Databricks performance tuning
  - Lakehouse performance best practices
  - Delta Lake optimization
  - Databricks tuning guide
  - data engineering performance
---

# Improving Lakehouse Performance â€” Dos & Donâ€™ts

## âœ¨ Story Time â€” â€œWhy Is the Lakehouse Getting Slower?â€

Ethan is the lead data engineer at a fast-scaling startup.

What began as a simple Lakehouse with 10 tables has now grown to:

- 200+ tables  
- Multiple ETL pipelines  
- Streaming + batch mixed workloads  
- Dashboards hitting data every few minutes  

Suddenly:

- Queries slow down  
- Jobs take longer  
- Costs spike  
- The CTO asks:  
  **â€œIs our Lakehouse breaking?â€**

Ethan takes a deep breath.  
The Lakehouse is fine â€” it just needs proper **performance tuning**.

Letâ€™s learn the **right** and **wrong** ways to do that.

---

## ðŸŸ© DO: Compact Files & Run OPTIMIZE Regularly

Small files = slow reads + high compute cost.

### Recommended:

```sql
OPTIMIZE table_name;
OPTIMIZE table_name ZORDER BY (important_column);
```

### Use Cases:

* High-ingestion tables
* Streaming outputs
* CDC pipelines

**Result:** Faster scans, better data skipping, lower cost.

---

## ðŸŸ¥ DON'T: Let Tiny Files Accumulate

If your table has:

* Thousands of small files
* 10KB or 100KB files
* Heavy streaming writes
* Over-partitioning

â€¦ your performance will crash.

Symptoms:

* Slow queries
* High I/O
* Excessive metadata operations

Prevent file explosion with Auto-Optimize:

```sql
ALTER TABLE my_table
SET TBLPROPERTIES (
  'delta.autoOptimize.optimizeWrite' = 'true',
  'delta.autoOptimize.autoCompact' = 'true'
);
```

---

## ðŸŸ© DO: Use Z-ORDER for Filter Columns

Z-ORDER physically clusters related values.

### Best for queries filtering by:

* customer_id
* event_date
* device_id
* sku or product_id

### Example:

```sql
OPTIMIZE events ZORDER BY (device_id, event_timestamp);
```

**Result:** 5Ã—â€“20Ã— faster filter-based queries.

---

## ðŸŸ¥ DON'T: Z-ORDER Too Many Columns

Z-ORDER works best with **1â€“3 columns**.

Problems with too many columns:

âœ– Slower OPTIMIZE
âœ– Too much data movement
âœ– No performance gain

Keep it simple â†’ only Z-ORDER your primary filter keys.

---

## ðŸŸ© DO: Use Photon for SQL Queries

Photon = Databricksâ€™ ultra-fast C++ execution engine.

### Perfect for:

* BI dashboards
* Large SQL aggregations
* Ad-hoc analytics
* Interactive data exploration

Enable Photon in SQL Warehouses & All-purpose clusters.

---

## ðŸŸ¥ DON'T: Expect Photon to Speed Up Everything

Photon doesnâ€™t accelerate:

âœ– Python UDFs
âœ– R or Scala-heavy workloads
âœ– ML pipelines
âœ– Non-SQL transformations

Use Photon only where it makes sense.

---

## ðŸŸ© DO: Partition For Write Volume, Not Read Volume

Partitioning helps huge ingestion workloads.

Examples:

âœ” `partition by date`
âœ” `partition by region`

---

## ðŸŸ¥ DON'T: Over-Partition

Too many partitions â†’ tiny files â†’ slow queries.

Bad examples:

âœ– Partitioning by customer_id
âœ– Partitioning by 1000+ values

Rule:

> If a partition will contain < 256MB of data â†’ donâ€™t partition by it.

---

## ðŸŸ© DO: Cache Hot Tables

Cache tables used repeatedly:

```sql
CACHE TABLE daily_sales;
```

Great for:

* BI dashboards
* Repeated queries
* Interactive explorations

---

## ðŸŸ¥ DON'T: Cache Large, Rarely Used Tables

Caching is **not storage** â€” itâ€™s memory.

Avoid caching:

âœ– 5 TB tables
âœ– Cold data rarely queried
âœ– Tables that update frequently

---

## ðŸŸ© DO: Tune SQL Endpoints Properly

For BI dashboards:

âœ” Use **Pro** or **Serverless SQL Warehouses**
âœ” Enable autoscaling
âœ” Tune concurrency
âœ” Allow the warehouse to scale up during peak hours

---

## ðŸŸ¥ DON'T: Run Dashboards on Job Clusters

Job clusters are:

* Temporary
* Not cached
* Not optimized for BI
* Slow for dashboards

Use SQL Warehouses instead.

---

## ðŸŸ© DO: Reduce Shuffle & Skew

Data skew leads to:

* Long-running tasks
* Failed jobs
* High compute cost

Fix skew by:

âœ” Salting keys
âœ” Using `REPARTITION`
âœ” Broadcasting small tables
âœ” Reducing unnecessary joins

Example:

```sql
SELECT /*+ BROADCAST(dim_table) */
```

---

## ðŸŸ© DO: Monitor & Profile Queries

Databricks gives you powerful tools:

* **Query Profile**
* **Query History**
* **Spark UI**
* **SQL Query Insights**

Track:

* Shuffle volume
* Scan volume
* Stage execution time
* Skewed partitions
* Photon usage

---

## ðŸ“˜ Summary

Improving Lakehouse performance is a balance of:

### âœ” Storage optimization

* Compact files
* Z-ORDER
* OPTIMIZE
* Correct partitioning

### âœ” Compute optimization

* Photon
* Correct cluster sizing
* Autoscaling

### âœ” Query optimization

* Avoid SELECT *
* Filter early
* Broadcast joins
* Tune SQL endpoints

### âœ” Architecture alignment

* Use SQL Warehouses for BI
* Avoid over-caching
* Prevent small file problems

A well-tuned Lakehouse is fast, cost-efficient, and extremely scalable.

---

# ðŸ‘‰ Next Topic

**Unity Catalog â€” Central Governance Explained**


