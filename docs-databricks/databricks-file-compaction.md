---
id: databricks-file-compaction
title: File Compaction & Delta File Management
sidebar_label: File Compaction
description: Learn how file compaction, small file problems, and Delta Lake file management work in Databricks. Discover how to keep your Lakehouse fast and cost-efficient using optimize techniques and best practices.
keywords:
  - Databricks file compaction
  - Delta small files
  - Delta Lake optimization
  - Databricks file management
  - data lake performance
---

# File Compaction & Delta File Management

## âœ¨ Story Time â€” â€œWhy Are My Queries Slowing Down Every Week?â€

Meet Arjun, a data engineer responsible for maintaining a busy Delta Lake table receiving:

- CDC updates every 5 minutes  
- Batch data every hour  
- Streaming inserts all day  

At first, everything is fast.  
But after a few weeks:

- Queries slow down  
- Dashboards lag  
- Costs increase  
- Data engineers keep asking: *â€œWhy is Delta so slow now?â€*

Arjun opens the Delta table storageâ€¦

He sees **THOUSANDS of tiny files** â€” the dreaded **Small File Problem**.

He smiles again.  
He knows exactly whatâ€™s needed:

âž¡ **File Compaction & Proper Delta File Management**.  

---

## ðŸ§© What Is File Compaction in Delta Lake?

File compaction is the process of **merging many small Delta files into fewer, larger, optimized files**.

Why small files happen:

- Streaming writes produce small batches  
- Frequent micro-batch ingest  
- CDC jobs write small delta chunks  
- Over-partitioning causes tiny files per partition  

Small files = **slow queries + high compute cost + too much metadata**.

Compaction solves this by:

- Reducing file count  
- Increasing file size  
- Improving read performance  
- Reducing metadata overhead  

---

## ðŸ” Why Small Files Hurt Performance

### âŒ More files = More metadata  
Each query has to read metadata for every file â†’ slower planning.

### âŒ More files = More unnecessary reads  
Even if only 1 row matches the filter, Databricks still must scan many files.

### âŒ More files = Higher storage cost  
Many tiny files create version bloat.

### âŒ More files = Slower Z-ORDER & OPTIMIZE  
The more files you have, the heavier maintenance operations become.

**Solution â†’ Compaction through OPTIMIZE.**

---

## âš™ï¸ How Delta Performs File Compaction

### The key command:

```sql
OPTIMIZE my_delta_table;
```

What it does:

1. Scans small files
2. Groups and merges them
3. Writes larger Parquet files (typically 128â€“512MB)
4. Updates Delta transaction log
5. Removes old small files (via VACUUM)

---

## ðŸ” Automatic File Compaction (With Auto-Optimize)

Databricks also offers automated compaction:

```sql
ALTER TABLE my_delta_table
SET TBLPROPERTIES (
  'delta.autoOptimize.optimizeWrite' = 'true',
  'delta.autoOptimize.autoCompact' = 'true'
);
```

### What these do:

| Property        | Action                                   |
| --------------- | ---------------------------------------- |
| `optimizeWrite` | Writes fewer, larger files during ingest |
| `autoCompact`   | Merges files after small batch inserts   |

Perfect for streaming or frequent batches.

---

## ðŸ§ª Real-World Example â€” Before & After Compaction

Arjunâ€™s table (before):

* 8,200 files per partition
* Avg file size: 40KB
* Query runtime: **34 seconds**

After:

```sql
OPTIMIZE sales_data ZORDER BY (customer_id);
VACUUM sales_data RETAIN 168 HOURS;
```

* 320 files per partition
* Avg file size: 300MB
* Query runtime: **5 seconds**

Improved performance, reduced cost, and less pressure on the cluster.

---

## ðŸ“¦ Delta File Management â€” The Full Picture

Delta Lake automatically manages:

* Transaction logs (`_delta_log/`)
* Versioning
* Compaction
* Data skipping
* File pruning
* Data removal with VACUUM

But **you** must manage:

* When to compact
* How often to vacuum
* How to structure partitions
* How to avoid unnecessary file explosion

---

## ðŸŽ¯ Best Practices for File Compaction

### âœ… 1. Compact high-ingestion tables regularly

Daily or weekly, depending on volume.

### âœ… 2. Enable Auto-Optimize for streaming workloads

Reduces small files during writes.

### âœ… 3. Combine OPTIMIZE with Z-ORDER

Boosts data skipping for faster queries.

### âœ… 4. Avoid over-partitioning

Too many partitions â†’ too many tiny files.

### âœ… 5. Use VACUUM after compaction

Clean old files and free storage:

```sql
VACUUM my_delta_table RETAIN 168 HOURS;
```

### âœ… 6. Monitor file count

If files per partition > 1000 â†’ compaction required.

---

## ðŸ“˜ Summary

* File compaction merges small files into large, efficient ones.
* Small files slow down queries, inflate compute cost, and destroy performance.
* OPTIMIZE + Auto-Optimize are the main tools for managing Delta Lake storage.
* Use VACUUM to clear old files after compaction.
* Proper file management makes your Lakehouse **fast, clean, and cost-efficient**.

---

# ðŸ‘‰ Next Topic

**Caching in Databricks â€” Best Practices**

