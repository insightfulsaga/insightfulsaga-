---
id: databricks-alerts-email-slack
title: Alerting â€” Email & Slack Alerts for Job Failures
sidebar_label: Alerts (Email & Slack)
description: A story-driven, practical guide on setting up Databricks alerting using Email, Slack, Webhooks, and on-failure notifications to monitor job failures in production pipelines.
keywords:
  - databricks alerts
  - databricks slack alerts
  - databricks email alerts
  - databricks job failure notifications
  - databricks webhook alerts
  - databricks monitoring
  - workflow alerts databricks
---

# Alerting â€” Email & Slack Alerts for Job Failures

## ğŸ¬ Story Time â€” â€œThe Job Failedâ€¦ and Nobody Knewâ€

Rahul, a data engineer at an e-commerce company, receives a frantic message at 10 AM:

> â€œWhy is todayâ€™s dashboard blank?â€

It turns out:

- ETL pipeline failed at 2:00 AM  
- No one received an alert  
- No monitoring was set up  
- Dashboards showed stale data  

Rahul thinks:

> â€œA pipeline without alerts is like a plane without sensors.â€

He opens **Databricks Workflows** to configure **Email + Slack alerts** for every step.

---

## ğŸ”¥ 1. Why Alerts Matter in Production

Alerts help teams react immediately to failures like:

- Cluster issues  
- Node failures  
- Schema mismatches  
- API rate limits  
- File unavailability  
- Data validation failures  
- Logic bugs in notebooks/scripts  

Without alerts, teams lose hours â€” or worse, publish incorrect data.

---

## ğŸ“§ 2. Email Alerts in Databricks

Email alerts are the simplest and fastest way to get notified.

### How to Add Email Alerts

1. Go to your **Job / Workflow**
2. Click **Alerts**
3. Add:
   - Your email
   - Team distribution email
   - On-call group email

Choose alert type:

- **On Failure**
- **On Success**
- **On Start**
- **On Duration Over Threshold**

### Example â€” Alert Configuration

```

On Failure â†’ [analytics-team@company.com](mailto:analytics-team@company.com)
On Duration Exceeded â†’ [dataops@company.com](mailto:dataops@company.com)

```

Databricks automatically sends:

- Error message  
- Failed task name  
- Logs link  
- Run details  
- Cluster info  

Perfect for morning triage.

---

## ğŸ“¨ 3. Slack Alerts â€” For Real-Time Team Visibility

Most modern teams prefer Slack notifications because:

- Everyone sees alerts  
- Rapid response coordination  
- On-call rotation visibility  
- Faster triage  

### Step 1 â€” Create a Slack Webhook URL

In Slack:

**Apps â†’ Incoming Webhooks â†’ Create New Webhook**

Select channel, e.g., `#data-alerts`.

Copy the webhook URL.

### Step 2 â€” Add Slack Webhook to Databricks Workflows

In the Job configuration:

```

Alerts â†’ Add â†’ Webhook â†’ Paste URL

```

### Step 3 â€” Customize Slack Message (Optional)

Databricks sends structured info like:

- Status  
- Workflow name  
- Link to job run  
- Failed task  
- Failure reason  

But you can also design your own message via a Python task:

```python
import requests

payload = {
    "text": f"ğŸš¨ Databricks Job Failed: {dbutils.jobs.taskValues.get('task_name')}"
}

requests.post(slack_webhook_url, json=payload)
```

Now failures appear instantly in Slack.

---

## â›‘ï¸ 4. Alerts for Multi-Task Workflows (Per Task)

Databricks allows:

### âœ” Alerts for the entire workflow

### âœ” Alerts per individual task

This is extremely helpful when:

* The validation task fails
* But upstream ingestion tasks run fine
* Only the downstream team needs notification

Example:

```
validate_data â†’ On Failure â†’ #quality-alerts
load_gold â†’ On Failure â†’ #data-engineering
```

---

## ğŸ› ï¸ 5. On-Failure Trigger Tasks (Advanced Alerts)

You can create **error handling tasks** inside workflows.

Example:

```
validate â†’ load_gold  
      â†“
  notify_failure
```

The `notify_failure` task runs only when:

```json
{
  "condition": "failed()"
}
```

Inside this task:

```python
requests.post(slack_url, json={"text": "Validation failed in Databricks!"})
```

This enables fully automated error routing.

---

## ğŸ§ª 6. Real Example â€” Notebook Alert on Error

In a notebook:

```python
try:
    df = spark.table("silver.sales")
    assert df.count() > 0
except Exception as e:
    dbutils.notebook.exit(f"ERROR: {str(e)}")
```

Databricks will automatically trigger failure alerts.

---

## ğŸ“Š 7. Alerts With Databricks SQL (Dashboards)

Databricks SQL supports **real-time condition-based alerts**:

* Revenue drop alerts
* Data drift detection
* SLA monitoring
* Missing data alerts

Example:

```
Alert when COUNT(*) < 1000 in daily_sales table
```

Alerts can fire:

* Email
* Slack webhooks
* PagerDuty
* Custom HTTP endpoints

---

## ğŸ§  Best Practices

1. Always configure **on-failure alerts**
2. Use **Slack â†’ primary**, email â†’ secondary
3. Create **separate channels** per pipeline type
4. Add **file-based triggers** + alerts for ingestion issues
5. Include **run URL** in alert message
6. Add **retry logic** + alerts only after retries fail
7. Use **service principals** for webhook authentication

---

## ğŸ‰ Real-World Ending â€” â€œThe Alert Saved the Morningâ€

Next day, at exactly 2:01 AM:

* API returned empty data
* The validation task failed
* Slack alerted the team instantly
* Issue resolved before business hours

At 9:00 AM, dashboards were fresh.

Rahulâ€™s manager said:

> â€œFinallyâ€¦ the pipeline can talk to us when things go wrong.â€

And thatâ€™s the magic of **Databricks Alerts**.

---

## ğŸ“˜ Summary

Databricks supports:

* âœ” Email alerts

* âœ” Slack alerts

* âœ” Webhook-based alerts

* âœ” On-failure tasks

* âœ” SQL alerts

* âœ” Per-task notification targeting

A must-have component for **production-grade pipeline monitoring**.

---

# ğŸ‘‰ Next Topic

**Cluster Policies â€” Cost & Security Enforcement**

