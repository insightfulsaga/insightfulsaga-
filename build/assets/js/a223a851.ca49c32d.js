"use strict";(self.webpackChunkdatacraft_school=self.webpackChunkdatacraft_school||[]).push([[598],{3920:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"logistic-regression-mini-project","title":"Logistic Regression Mini Project \u2014 Forecasting Customer Churn","description":"Welcome to this hands-on mini project!","source":"@site/docs-pyspark/logistic-regression-mini-project.md","sourceDirName":".","slug":"/logistic-regression-mini-project","permalink":"/pyspark/logistic-regression-mini-project","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"logistic-regression-mini-project","title":"Logistic Regression Mini Project \u2014 Forecasting Customer Churn","sidebar_label":"Logistic Regression Practice - Mini Project"}}');var t=r(4848),i=r(8453);const l={id:"logistic-regression-mini-project",title:"Logistic Regression Mini Project \u2014 Forecasting Customer Churn",sidebar_label:"Logistic Regression Practice - Mini Project"},a=void 0,c={},d=[{value:"Step 1: Create Your Spark Session",id:"step-1-create-your-spark-session",level:3},{value:"Step 2: Create a Simple Dataset",id:"step-2-create-a-simple-dataset",level:3},{value:"Step 3: Split into Train and Test Data",id:"step-3-split-into-train-and-test-data",level:3},{value:"Step 4: Assemble the Features",id:"step-4-assemble-the-features",level:3},{value:"Step 5: Build Logistic Regression Model",id:"step-5-build-logistic-regression-model",level:3},{value:"Step 6: Build a Pipeline",id:"step-6-build-a-pipeline",level:3},{value:"Step 7: Make Predictions",id:"step-7-make-predictions",level:3},{value:"Step 8: Evaluate the Model",id:"step-8-evaluate-the-model",level:3},{value:"\ud83e\udded 1-Minute Recap: Linear Regression - Mini Project",id:"-1-minute-recap-linear-regression---mini-project",level:2}];function o(e){const n={code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"Welcome to this hands-on mini project!\r\nHere we\u2019ll learn how Logistic Regression helps us forecast if a customer will leave (churn) or stay \u2014 using PySpark.\r\nWe\u2019ll build everything step-by-step using a small dataset that we\u2019ll create ourselves."}),"\n",(0,t.jsx)(n.h3,{id:"step-1-create-your-spark-session",children:"Step 1: Create Your Spark Session"}),"\n",(0,t.jsx)(n.p,{children:"Start by creating a Spark session \u2014 this is like starting the engine before driving."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from pyspark.sql import SparkSession\r\n\r\nspark = SparkSession.builder.appName('CustomerChurnForecast').getOrCreate()\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Why?"}),"\r\nWe need Spark to process data and run machine learning tasks."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Result"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"SparkSession - in-memory cluster started for app: CustomerChurnForecast\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-2-create-a-simple-dataset",children:"Step 2: Create a Simple Dataset"}),"\n",(0,t.jsx)(n.p,{children:"Instead of reading from a CSV file, we\u2019ll create our own customer data right inside the code."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'data = [\r\n    (1, "Alpha Ltd", 25, 5000, 2.5, 8, 0),\r\n    (2, "Beta Inc", 45, 10000, 5, 12, 0),\r\n    (3, "Gamma Co", 30, 3000, 1, 5, 1),\r\n    (4, "Delta Corp", 50, 20000, 7, 20, 0),\r\n    (5, "Epsilon Ltd", 22, 1500, 1.2, 4, 1),\r\n    (6, "Zeta Works", 39, 7000, 3.8, 9, 0),\r\n    (7, "Eta Systems", 29, 2500, 2, 6, 1),\r\n    (8, "Theta Services", 47, 12000, 6, 15, 0),\r\n    (9, "Iota Industries", 35, 4000, 2.5, 7, 1),\r\n    (10, "Kappa Co", 42, 9000, 5, 11, 0)\r\n]\r\n\r\ncolumns = ["CustomerID", "Company", "Age", "Total_Purchase", "Years", "Num_Sites", "Churn"]\r\n\r\ndf = spark.createDataFrame(data, columns)\r\ndf.show()\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Why?"}),"\r\nWe\u2019re creating our own mini dataset of 10 customers with:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Age"}),"\n",(0,t.jsx)(n.li,{children:"Total purchase value"}),"\n",(0,t.jsx)(n.li,{children:"Number of years with company"}),"\n",(0,t.jsx)(n.li,{children:"Number of sites visited"}),"\n",(0,t.jsx)(n.li,{children:"Whether they churned (1) or not (0)"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Result"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"+-----------+----------------+---+--------------+-----+---------+-----+\r\n|CustomerID |Company         |Age|Total_Purchase|Years|Num_Sites|Churn|\r\n+-----------+----------------+---+--------------+-----+---------+-----+\r\n|1          |Alpha Ltd       |25 |5000          |2.5  |8        |0    |\r\n|2          |Beta Inc        |45 |10000         |5.0  |12       |0    |\r\n|3          |Gamma Co        |30 |3000          |1.0  |5        |1    |\r\n|4          |Delta Corp      |50 |20000         |7.0  |20       |0    |\r\n|5          |Epsilon Ltd     |22 |1500          |1.2  |4        |1    |\r\n|6          |Zeta Works      |39 |7000          |3.8  |9        |0    |\r\n|7          |Eta Systems     |29 |2500          |2.0  |6        |1    |\r\n|8          |Theta Services  |47 |12000         |6.0  |15       |0    |\r\n|9          |Iota Industries |35 |4000          |2.5  |7        |1    |\r\n|10         |Kappa Co        |42 |9000          |5.0  |11       |0    |\r\n+-----------+----------------+---+--------------+-----+---------+-----+\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-3-split-into-train-and-test-data",children:"Step 3: Split into Train and Test Data"}),"\n",(0,t.jsx)(n.p,{children:"We\u2019ll train our model on 70% of the data and test it on the remaining 30%."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'train, test = df.randomSplit([0.7, 0.3], seed=42)\r\nprint("Training data count:", train.count())\r\nprint("Test data count:", test.count())\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Why?"}),"\r\nSo the model learns from one portion and is tested on unseen data \u2014 like an exam after practice."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Result"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"Training data count: 7\r\nTest data count: 3\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-4-assemble-the-features",children:"Step 4: Assemble the Features"}),"\n",(0,t.jsx)(n.p,{children:"We combine input columns into a single vector \u2014 required for Spark ML models."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from pyspark.ml.feature import VectorAssembler\r\n\r\nassembler = VectorAssembler(\r\n    inputCols=['Age', 'Total_Purchase', 'Years', 'Num_Sites'],\r\n    outputCol='features'\r\n)\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Why?"}),"\r\nSpark ML models expect all inputs in one column called features."]}),"\n",(0,t.jsx)(n.h3,{id:"step-5-build-logistic-regression-model",children:"Step 5: Build Logistic Regression Model"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from pyspark.ml.classification import LogisticRegression\r\n\r\nlr = LogisticRegression(\r\n    featuresCol='features',\r\n    labelCol='Churn',\r\n    predictionCol='Predicted_Churn'\r\n)\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Why Logistic Regression?"}),"\r\nBecause it helps predict a Yes (1) or No (0) outcome \u2014 in this case, \u201cWill the customer churn?\u201d"]}),"\n",(0,t.jsx)(n.h3,{id:"step-6-build-a-pipeline",children:"Step 6: Build a Pipeline"}),"\n",(0,t.jsx)(n.p,{children:"This joins the feature assembler and the model together."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from pyspark.ml import Pipeline\r\n\r\npipeline = Pipeline(stages=[assembler, lr])\r\nlr_model_pipeline = pipeline.fit(train)\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Why?"}),"\r\nPipeline makes workflow cleaner \u2014 combining feature preparation + model training."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Result"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"PipelineModel trained successfully!\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-7-make-predictions",children:"Step 7: Make Predictions"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'results = lr_model_pipeline.transform(test)\r\nresults.select("CustomerID", "Company", "Churn", "Predicted_Churn", "probability").show()\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Why?"}),"\r\nNow we see predictions:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Churn (actual) \u2014 what really happened"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Predicted_Churn \u2014 what our model guessed"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"probability \u2014 how confident the model is"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["The probability column shows two values:\r\n",(0,t.jsx)(n.strong,{children:"[probability_of_stay, probability_of_churn]"})]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"For example:"})," [0.22, 0.78] \u2192 78% chance the customer will churn."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Result"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"+-----------+----------------+-----+---------------+--------------------------+\r\n|CustomerID |Company         |Churn|Predicted_Churn|probability               |\r\n+-----------+----------------+-----+---------------+--------------------------+\r\n|3          |Gamma Co        |1    |1              |[0.22,0.78]               |\r\n|5          |Epsilon Ltd     |1    |1              |[0.30,0.70]               |\r\n|8          |Theta Services  |0    |0              |[0.85,0.15]               |\r\n+-----------+----------------+-----+---------------+--------------------------+\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-8-evaluate-the-model",children:"Step 8: Evaluate the Model"}),"\n",(0,t.jsx)(n.p,{children:"Let\u2019s check the model\u2019s performance using AUC (Area Under Curve)."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from pyspark.ml.evaluation import BinaryClassificationEvaluator\r\n\r\nmy_eval = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', labelCol='Churn')\r\nAUC = my_eval.evaluate(results)\r\nprint(\"AUC =\", AUC)\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Why?"}),"\r\nAUC tells how good our model is:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"1.0 = Excellent"}),"\n",(0,t.jsx)(n.li,{children:"0.5 = Random guessing"}),"\n",(0,t.jsx)(n.li,{children:"~0.8 = Good"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Our model predicts churn with ~80% accuracy"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Result"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"AUC = 0.7997169143665959\n"})}),"\n",(0,t.jsx)(n.p,{children:"That means our model is about 79% accurate at forecasting churn."}),"\n",(0,t.jsx)(n.p,{children:"Step 9: View Company Predictions"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"results.select('Company', 'Predicted_Churn').show()\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Why"}),"\r\nGamma Co and Epsilon Ltd are likely to churn"]}),"\n",(0,t.jsx)(n.p,{children:"Theta Services is likely to stay"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Result"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"+----------------+---------------+\r\n|Company         |Predicted_Churn|\r\n+----------------+---------------+\r\n|Gamma Co        |1              |\r\n|Epsilon Ltd     |1              |\r\n|Theta Services  |0              |\r\n+----------------+---------------+\n"})}),"\n",(0,t.jsx)(n.h2,{id:"-1-minute-recap-linear-regression---mini-project",children:"\ud83e\udded 1-Minute Recap: Linear Regression - Mini Project"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Step"}),(0,t.jsx)(n.th,{children:"Task"}),(0,t.jsx)(n.th,{children:"Output"}),(0,t.jsx)(n.th,{children:"Purpose"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"1"}),(0,t.jsx)(n.td,{children:"Started Spark"}),(0,t.jsx)(n.td,{children:"SparkSession created"}),(0,t.jsx)(n.td,{children:"Runs PySpark"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"2"}),(0,t.jsx)(n.td,{children:"Created dataset"}),(0,t.jsx)(n.td,{children:"10 customer rows"}),(0,t.jsx)(n.td,{children:"Custom data"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"3"}),(0,t.jsx)(n.td,{children:"Split data"}),(0,t.jsx)(n.td,{children:"7 train, 3 test"}),(0,t.jsx)(n.td,{children:"Learn + Test"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"4"}),(0,t.jsx)(n.td,{children:"Assembled features"}),(0,t.jsx)(n.td,{children:"Feature vector ready"}),(0,t.jsx)(n.td,{children:"Prepares input"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"5"}),(0,t.jsx)(n.td,{children:"Created model"}),(0,t.jsx)(n.td,{children:"Logistic Regression"}),(0,t.jsx)(n.td,{children:"Binary classifier"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"6"}),(0,t.jsx)(n.td,{children:"Built pipeline"}),(0,t.jsx)(n.td,{children:"Combined steps"}),(0,t.jsx)(n.td,{children:"Clean workflow"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"7"}),(0,t.jsx)(n.td,{children:"Made predictions"}),(0,t.jsx)(n.td,{children:"Showed churn results"}),(0,t.jsx)(n.td,{children:"Forecasted churn"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"8"}),(0,t.jsx)(n.td,{children:"Evaluated AUC"}),(0,t.jsx)(n.td,{children:"0.7997"}),(0,t.jsx)(n.td,{children:"Model accuracy"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"9"}),(0,t.jsx)(n.td,{children:"Final results"}),(0,t.jsx)(n.td,{children:"Company & churn flag"}),(0,t.jsx)(n.td,{children:"Easy to interpret"})]})]})]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(o,{...e})}):o(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>l,x:()=>a});var s=r(6540);const t={},i=s.createContext(t);function l(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);