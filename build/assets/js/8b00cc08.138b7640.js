"use strict";(self.webpackChunkdatacraft_school=self.webpackChunkdatacraft_school||[]).push([[1383],{4548:(e,a,r)=>{r.r(a),r.d(a,{assets:()=>o,contentTitle:()=>l,default:()=>p,frontMatter:()=>i,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"df-create-csv","title":"Creating DataFrames from CSV, JSON, Parquet & Hive Tables","description":"Learn how to create PySpark DataFrames from CSV, JSON, Parquet files, and Hive tables using Databricks and Spark best practices.","source":"@site/docs-pyspark/df-create-csv.md","sourceDirName":".","slug":"/df-create-csv","permalink":"/pyspark/df-create-csv","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"PySpark","permalink":"/pyspark/tags/py-spark"},{"inline":true,"label":"Apache Spark","permalink":"/pyspark/tags/apache-spark"},{"inline":true,"label":"Big Data","permalink":"/pyspark/tags/big-data"},{"inline":true,"label":"Spark Basics","permalink":"/pyspark/tags/spark-basics"},{"inline":true,"label":"Cluster Computing","permalink":"/pyspark/tags/cluster-computing"},{"inline":true,"label":"Spark Architecture","permalink":"/pyspark/tags/spark-architecture"},{"inline":true,"label":"Driver Program","permalink":"/pyspark/tags/driver-program"},{"inline":true,"label":"Executors","permalink":"/pyspark/tags/executors"},{"inline":true,"label":"Cluster Manager","permalink":"/pyspark/tags/cluster-manager"},{"inline":true,"label":"SparkSession","permalink":"/pyspark/tags/spark-session"},{"inline":true,"label":"SparkContext","permalink":"/pyspark/tags/spark-context"},{"inline":true,"label":"RDD","permalink":"/pyspark/tags/rdd"},{"inline":true,"label":"RDD Transformations","permalink":"/pyspark/tags/rdd-transformations"},{"inline":true,"label":"RDD Actions","permalink":"/pyspark/tags/rdd-actions"},{"inline":true,"label":"Key-Value RDD","permalink":"/pyspark/tags/key-value-rdd"},{"inline":true,"label":"RDD Caching","permalink":"/pyspark/tags/rdd-caching"},{"inline":true,"label":"DataFrame","permalink":"/pyspark/tags/data-frame"},{"inline":true,"label":"DataFrame API","permalink":"/pyspark/tags/data-frame-api"},{"inline":true,"label":"Column Operations","permalink":"/pyspark/tags/column-operations"},{"inline":true,"label":"DataFrame Joins","permalink":"/pyspark/tags/data-frame-joins"},{"inline":true,"label":"Aggregations","permalink":"/pyspark/tags/aggregations"},{"inline":true,"label":"GroupBy","permalink":"/pyspark/tags/group-by"},{"inline":true,"label":"Window Functions","permalink":"/pyspark/tags/window-functions"},{"inline":true,"label":"Missing Data Handling","permalink":"/pyspark/tags/missing-data-handling"},{"inline":true,"label":"Spark SQL","permalink":"/pyspark/tags/spark-sql"},{"inline":true,"label":"Temp Views","permalink":"/pyspark/tags/temp-views"},{"inline":true,"label":"Spark SQL Functions","permalink":"/pyspark/tags/spark-sql-functions"},{"inline":true,"label":"UDF","permalink":"/pyspark/tags/udf"},{"inline":true,"label":"UDAF","permalink":"/pyspark/tags/udaf"}],"version":"current","frontMatter":{"id":"df-create-csv","title":"Creating DataFrames from CSV, JSON, Parquet & Hive Tables","sidebar_label":"Creating DataFrames","description":"Learn how to create PySpark DataFrames from CSV, JSON, Parquet files, and Hive tables using Databricks and Spark best practices.","keywords":["PySpark DataFrame creation","Spark read CSV","Spark read JSON","Spark read Parquet","DataFrame from Hive","Databricks read files"],"tags":["PySpark","Apache Spark","Big Data","Spark Basics","Cluster Computing","Spark Architecture","Driver Program","Executors","Cluster Manager","SparkSession","SparkContext","RDD","RDD Transformations","RDD Actions","Key-Value RDD","RDD Caching","DataFrame","DataFrame API","Column Operations","DataFrame Joins","Aggregations","GroupBy","Window Functions","Missing Data Handling","Spark SQL","Temp Views","Spark SQL Functions","UDF","UDAF"]},"sidebar":"tutorialSidebar","previous":{"title":"RDD Persistence & Caching","permalink":"/pyspark/rdd-persistence-caching"},"next":{"title":"DataFrame API","permalink":"/pyspark/df-api"}}');var n=r(4848),t=r(8453);const i={id:"df-create-csv",title:"Creating DataFrames from CSV, JSON, Parquet & Hive Tables",sidebar_label:"Creating DataFrames",description:"Learn how to create PySpark DataFrames from CSV, JSON, Parquet files, and Hive tables using Databricks and Spark best practices.",keywords:["PySpark DataFrame creation","Spark read CSV","Spark read JSON","Spark read Parquet","DataFrame from Hive","Databricks read files"],tags:["PySpark","Apache Spark","Big Data","Spark Basics","Cluster Computing","Spark Architecture","Driver Program","Executors","Cluster Manager","SparkSession","SparkContext","RDD","RDD Transformations","RDD Actions","Key-Value RDD","RDD Caching","DataFrame","DataFrame API","Column Operations","DataFrame Joins","Aggregations","GroupBy","Window Functions","Missing Data Handling","Spark SQL","Temp Views","Spark SQL Functions","UDF","UDAF"]},l="Creating DataFrames from CSV, JSON, Parquet & Hive Tables",o={},c=[{value:"Why File Formats Matter",id:"why-file-formats-matter",level:2},{value:"1. Creating DataFrames from CSV Files",id:"1-creating-dataframes-from-csv-files",level:2},{value:"\u2714 When to Use CSV",id:"-when-to-use-csv",level:3},{value:"\u274c Avoid for big data",id:"-avoid-for-big-data",level:3},{value:"2. Creating DataFrames from JSON Files",id:"2-creating-dataframes-from-json-files",level:2},{value:"\u2714 Best for",id:"-best-for",level:3},{value:"Story Example",id:"story-example",level:3},{value:"3. Creating DataFrames from Parquet Files (Best Practice)",id:"3-creating-dataframes-from-parquet-files-best-practice",level:2},{value:"\u2714 Best Format For",id:"-best-format-for",level:3},{value:"4. Creating DataFrames from Hive Tables",id:"4-creating-dataframes-from-hive-tables",level:2},{value:"\u2714 Helpful When",id:"-helpful-when",level:3},{value:"5. Summary",id:"5-summary",level:2}];function d(e){const a={br:"br",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.header,{children:(0,n.jsx)(a.h1,{id:"creating-dataframes-from-csv-json-parquet--hive-tables",children:"Creating DataFrames from CSV, JSON, Parquet & Hive Tables"})}),"\n",(0,n.jsxs)(a.p,{children:["Every analytics pipeline at ",(0,n.jsx)(a.strong,{children:"NeoMart"}),", our growing e-commerce platform, starts with one step: ",(0,n.jsx)(a.strong,{children:"loading data into Spark"}),".",(0,n.jsx)(a.br,{}),"\n","Whether it comes from mobile apps, warehouses, partners, or machine logs, your first job as a data engineer is to convert this raw data into a ",(0,n.jsx)(a.strong,{children:"DataFrame"})," \u2014 Spark\u2019s most widely used data structure."]}),"\n",(0,n.jsxs)(a.p,{children:["DataFrames provide schema, structure, column-level operations, and optimization through Catalyst.",(0,n.jsx)(a.br,{}),"\n","But how you ",(0,n.jsx)(a.em,{children:"create"})," a DataFrame depends on the ",(0,n.jsx)(a.strong,{children:"file format"})," you\u2019re working with."]}),"\n",(0,n.jsxs)(a.p,{children:["Let\u2019s explore the four most common formats: ",(0,n.jsx)(a.strong,{children:"CSV, JSON, Parquet, and Hive tables"}),"."]}),"\n",(0,n.jsx)(a.hr,{}),"\n",(0,n.jsx)(a.h2,{id:"why-file-formats-matter",children:"Why File Formats Matter"}),"\n",(0,n.jsxs)(a.p,{children:["Not all file formats behave the same.",(0,n.jsx)(a.br,{}),"\n","Some are slow but simple (CSV), others lightning fast (Parquet), and some ideal for semi-structured workloads (JSON)."]}),"\n",(0,n.jsxs)(a.p,{children:["Choosing the right format can easily save ",(0,n.jsx)(a.strong,{children:"minutes or even hours"})," in large-scale ETL jobs."]}),"\n",(0,n.jsx)(a.hr,{}),"\n",(0,n.jsx)(a.h2,{id:"1-creating-dataframes-from-csv-files",children:"1. Creating DataFrames from CSV Files"}),"\n",(0,n.jsx)(a.p,{children:"CSV files are widely used but come with limitations \u2014 no schema, no compression, and slow parsing."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-python",children:'df = spark.read \\\r\n    .option("header", True) \\\r\n    .option("inferSchema", True) \\\r\n    .csv("/mnt/data/sales.csv")\n'})}),"\n",(0,n.jsx)(a.h3,{id:"-when-to-use-csv",children:"\u2714 When to Use CSV"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"During initial ingestion"}),"\n",(0,n.jsx)(a.li,{children:"When partners/vendors deliver small datasets"}),"\n",(0,n.jsx)(a.li,{children:"For debugging and quick data inspection"}),"\n"]}),"\n",(0,n.jsx)(a.h3,{id:"-avoid-for-big-data",children:"\u274c Avoid for big data"}),"\n",(0,n.jsx)(a.p,{children:"CSV parsing becomes slow as data volume increases."}),"\n",(0,n.jsx)(a.hr,{}),"\n",(0,n.jsx)(a.h2,{id:"2-creating-dataframes-from-json-files",children:"2. Creating DataFrames from JSON Files"}),"\n",(0,n.jsx)(a.p,{children:"JSON is perfect for logs, nested attributes, and NoSQL-like structures."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-python",children:'df = spark.read \\\r\n    .option("multiline", True) \\\r\n    .json("/mnt/data/events.json")\n'})}),"\n",(0,n.jsx)(a.h3,{id:"-best-for",children:"\u2714 Best for"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Clickstream logs"}),"\n",(0,n.jsx)(a.li,{children:"IoT events"}),"\n",(0,n.jsx)(a.li,{children:"User activity streams"}),"\n"]}),"\n",(0,n.jsx)(a.h3,{id:"story-example",children:"Story Example"}),"\n",(0,n.jsx)(a.p,{children:"NeoMart\u2019s mobile app sends events like:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-json",children:'{\r\n  "user": "123",\r\n  "actions": ["view", "add_to_cart"]\r\n}\n'})}),"\n",(0,n.jsx)(a.p,{children:"JSON allows nested data, which Spark can parse easily."}),"\n",(0,n.jsx)(a.hr,{}),"\n",(0,n.jsx)(a.h2,{id:"3-creating-dataframes-from-parquet-files-best-practice",children:"3. Creating DataFrames from Parquet Files (Best Practice)"}),"\n",(0,n.jsxs)(a.p,{children:["Parquet is the ",(0,n.jsx)(a.strong,{children:"default"})," format for big data because of:"]}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Columnar storage"}),"\n",(0,n.jsx)(a.li,{children:"Built-in compression"}),"\n",(0,n.jsx)(a.li,{children:"Predicate pushdown"}),"\n",(0,n.jsx)(a.li,{children:"Fast read/write"}),"\n"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-python",children:'df = spark.read.parquet("/mnt/data/transactions/")\n'})}),"\n",(0,n.jsx)(a.h3,{id:"-best-format-for",children:"\u2714 Best Format For"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Analytics"}),"\n",(0,n.jsx)(a.li,{children:"Large-scale ETL"}),"\n",(0,n.jsx)(a.li,{children:"Machine learning pipelines"}),"\n",(0,n.jsx)(a.li,{children:"Databricks Delta workflows"}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"This is NeoMart\u2019s recommended storage format for raw, clean, and analytics layers."}),"\n",(0,n.jsx)(a.hr,{}),"\n",(0,n.jsx)(a.h2,{id:"4-creating-dataframes-from-hive-tables",children:"4. Creating DataFrames from Hive Tables"}),"\n",(0,n.jsx)(a.p,{children:"Hive tables allow you to store structured datasets with metadata (schema, partitions)."}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-python",children:'df = spark.table("analytics.daily_orders")\n'})}),"\n",(0,n.jsx)(a.p,{children:"or using SQL:"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-python",children:'df = spark.sql("SELECT * FROM analytics.daily_orders")\n'})}),"\n",(0,n.jsx)(a.h3,{id:"-helpful-when",children:"\u2714 Helpful When"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Working with enterprise data warehouses"}),"\n",(0,n.jsx)(a.li,{children:"Using Databricks metastore"}),"\n",(0,n.jsx)(a.li,{children:"Structuring data by partitions (date, region, etc.)"}),"\n"]}),"\n",(0,n.jsx)(a.hr,{}),"\n",(0,n.jsx)(a.h2,{id:"5-summary",children:"5. Summary"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"CSV"})," \u2192 simple, human-readable, but slow"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"JSON"})," \u2192 perfect for nested & semi-structured data"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Parquet"})," \u2192 fastest & most efficient (recommended for big data)"]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Hive Tables"})," \u2192 ideal for enterprise-scale structured storage"]}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"Proper DataFrame creation lays the foundation for the entire transformation pipeline \u2014 ensuring performance, accuracy, and scalability."}),"\n",(0,n.jsx)(a.hr,{}),"\n",(0,n.jsxs)(a.p,{children:["Next up, we\u2019ll master the ",(0,n.jsx)(a.strong,{children:"DataFrame API \u2014 Select, Filter, WithColumn, Drop"}),", the core tools used to transform raw data into analytics-ready datasets."]})]})}function p(e={}){const{wrapper:a}={...(0,t.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},8453:(e,a,r)=>{r.d(a,{R:()=>i,x:()=>l});var s=r(6540);const n={},t=s.createContext(n);function i(e){const a=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function l(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:i(e.components),s.createElement(t.Provider,{value:a},e.children)}}}]);