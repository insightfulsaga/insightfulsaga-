"use strict";(self.webpackChunkdatacraft_school=self.webpackChunkdatacraft_school||[]).push([[7556],{2769:(n,s,e)=>{e.r(s),e.d(s,{assets:()=>l,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"pyspark-functions-udfs","title":"PySpark Functions & UDFs \u2014 Complete Beginner Guide","description":"Learn how to use PySpark built-in functions, User Defined Functions (UDFs), and Pandas UDFs for efficient data transformations. Step-by-step examples and best practices for beginners.","source":"@site/docs-pyspark/pyspark-functions-udfs.md","sourceDirName":".","slug":"/pyspark/functions-udfs","permalink":"/pyspark/pyspark/functions-udfs","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"PySpark","permalink":"/pyspark/tags/py-spark"},{"inline":true,"label":"DataFrames","permalink":"/pyspark/tags/data-frames"},{"inline":true,"label":"Functions","permalink":"/pyspark/tags/functions"},{"inline":true,"label":"UDFs","permalink":"/pyspark/tags/ud-fs"},{"inline":true,"label":"Pandas UDFs","permalink":"/pyspark/tags/pandas-ud-fs"},{"inline":true,"label":"ETL","permalink":"/pyspark/tags/etl"},{"inline":true,"label":"Beginner","permalink":"/pyspark/tags/beginner"}],"version":"current","frontMatter":{"id":"pyspark-functions-udfs","title":"PySpark Functions & UDFs \u2014 Complete Beginner Guide","sidebar_label":"Functions & UDFs","slug":"/pyspark/functions-udfs","description":"Learn how to use PySpark built-in functions, User Defined Functions (UDFs), and Pandas UDFs for efficient data transformations. Step-by-step examples and best practices for beginners.","keywords":["pyspark functions","pyspark udfs","pyspark pandas udf","spark dataframe functions","pyspark custom transformations","spark udf example","pyspark beginner guide"],"og:title":"PySpark Functions & UDFs \u2014 Beginner-Friendly Guide","og:description":"Master PySpark Functions, UDFs, and Pandas UDFs. Learn how to apply built-in functions, write custom logic, and optimize transformations on large datasets.","tags":["PySpark","DataFrames","Functions","UDFs","Pandas UDFs","ETL","Beginner"]}}');var r=e(4848),t=e(8453);const a={id:"pyspark-functions-udfs",title:"PySpark Functions & UDFs \u2014 Complete Beginner Guide",sidebar_label:"Functions & UDFs",slug:"/pyspark/functions-udfs",description:"Learn how to use PySpark built-in functions, User Defined Functions (UDFs), and Pandas UDFs for efficient data transformations. Step-by-step examples and best practices for beginners.",keywords:["pyspark functions","pyspark udfs","pyspark pandas udf","spark dataframe functions","pyspark custom transformations","spark udf example","pyspark beginner guide"],"og:title":"PySpark Functions & UDFs \u2014 Beginner-Friendly Guide","og:description":"Master PySpark Functions, UDFs, and Pandas UDFs. Learn how to apply built-in functions, write custom logic, and optimize transformations on large datasets.",tags:["PySpark","DataFrames","Functions","UDFs","Pandas UDFs","ETL","Beginner"]},o="PySpark Functions & UDFs \u2014 Complete Beginner Guide",l={},d=[{value:"Why Use Functions &amp; UDFs in PySpark?",id:"why-use-functions--udfs-in-pyspark",level:2},{value:"1\ufe0f\u20e3 Using Built-In PySpark Functions",id:"1\ufe0f\u20e3-using-built-in-pyspark-functions",level:2},{value:"Common Built-In Functions",id:"common-built-in-functions",level:3},{value:"Example: Applying Built-In Functions",id:"example-applying-built-in-functions",level:3},{value:"2\ufe0f\u20e3 User Defined Functions (UDFs) in PySpark",id:"2\ufe0f\u20e3-user-defined-functions-udfs-in-pyspark",level:2},{value:"Example: Custom Discount Logic",id:"example-custom-discount-logic",level:3},{value:"3\ufe0f\u20e3 Pandas UDFs (Vectorized UDFs)",id:"3\ufe0f\u20e3-pandas-udfs-vectorized-udfs",level:2},{value:"Example: Batch Discount Using Pandas UDF",id:"example-batch-discount-using-pandas-udf",level:3},{value:"4\ufe0f\u20e3 Real-World Example: Combining Functions &amp; UDFs",id:"4\ufe0f\u20e3-real-world-example-combining-functions--udfs",level:2},{value:"5\ufe0f\u20e3 Key Takeaways for PySpark Functions &amp; UDFs",id:"5\ufe0f\u20e3-key-takeaways-for-pyspark-functions--udfs",level:2}];function c(n){const s={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(s.header,{children:(0,r.jsx)(s.h1,{id:"pyspark-functions--udfs--complete-beginner-guide",children:"PySpark Functions & UDFs \u2014 Complete Beginner Guide"})}),"\n",(0,r.jsxs)(s.p,{children:["Imagine running a ",(0,r.jsx)(s.strong,{children:"high-tech data lab"})," where data comes from CSVs, APIs, streaming sensors, and cloud buckets. Sometimes, PySpark\u2019s built-in functions are enough. Other times, you need ",(0,r.jsx)(s.strong,{children:"custom Python logic"})," for special cases."]}),"\n",(0,r.jsxs)(s.p,{children:["In PySpark, these custom transformations are handled with ",(0,r.jsx)(s.strong,{children:"Functions, User Defined Functions (UDFs), and Pandas UDFs"}),", allowing you to combine ",(0,r.jsx)(s.strong,{children:"Spark\u2019s distributed computing power"})," with ",(0,r.jsx)(s.strong,{children:"Python flexibility"}),"."]}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsx)(s.h2,{id:"why-use-functions--udfs-in-pyspark",children:"Why Use Functions & UDFs in PySpark?"}),"\n",(0,r.jsxs)(s.p,{children:["PySpark DataFrames are powerful but ",(0,r.jsx)(s.strong,{children:"cannot solve every problem out-of-the-box"}),". Functions and UDFs allow you to:"]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:["Perform ",(0,r.jsx)(s.strong,{children:"data transformations at scale"})," on millions of rows"]}),"\n",(0,r.jsxs)(s.li,{children:["Write ",(0,r.jsx)(s.strong,{children:"clean, reusable Python code"})]}),"\n",(0,r.jsxs)(s.li,{children:["Combine ",(0,r.jsx)(s.strong,{children:"built-in optimization"})," with ",(0,r.jsx)(s.strong,{children:"custom logic"})," for complex transformations"]}),"\n"]}),"\n",(0,r.jsx)(s.p,{children:"Think of it this way:"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Built-in functions:"})," Fast, optimized, Spark-native"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"UDFs:"})," Row-wise custom Python logic"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Pandas UDFs:"})," High-performance, batch-friendly transformations"]}),"\n"]}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsx)(s.h2,{id:"1\ufe0f\u20e3-using-built-in-pyspark-functions",children:"1\ufe0f\u20e3 Using Built-In PySpark Functions"}),"\n",(0,r.jsxs)(s.p,{children:["PySpark provides ",(0,r.jsx)(s.strong,{children:"hundreds of optimized built-in functions"})," for ",(0,r.jsx)(s.strong,{children:"string, numeric, date, and aggregation operations"}),". Always prefer these before writing UDFs\u2014they\u2019re faster and cluster-optimized."]}),"\n",(0,r.jsx)(s.h3,{id:"common-built-in-functions",children:"Common Built-In Functions"}),"\n",(0,r.jsxs)(s.table,{children:[(0,r.jsx)(s.thead,{children:(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.th,{children:"Function"}),(0,r.jsx)(s.th,{children:"Purpose"}),(0,r.jsx)(s.th,{children:"Example"})]})}),(0,r.jsxs)(s.tbody,{children:[(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.code,{children:"col()"})}),(0,r.jsx)(s.td,{children:"Reference a DataFrame column"}),(0,r.jsx)(s.td,{children:(0,r.jsx)(s.code,{children:'col("amount")'})})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.code,{children:"lit()"})}),(0,r.jsx)(s.td,{children:"Create a constant column"}),(0,r.jsx)(s.td,{children:(0,r.jsx)(s.code,{children:'lit("USD")'})})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsxs)(s.td,{children:[(0,r.jsx)(s.code,{children:"upper()"}),", ",(0,r.jsx)(s.code,{children:"lower()"})]}),(0,r.jsx)(s.td,{children:"Convert text case"}),(0,r.jsx)(s.td,{children:(0,r.jsx)(s.code,{children:'upper(col("region"))'})})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.code,{children:"concat()"})}),(0,r.jsx)(s.td,{children:"Combine multiple columns"}),(0,r.jsx)(s.td,{children:(0,r.jsx)(s.code,{children:'concat(col("first"), col("last"))'})})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsxs)(s.td,{children:[(0,r.jsx)(s.code,{children:"avg()"}),", ",(0,r.jsx)(s.code,{children:"sum()"}),", ",(0,r.jsx)(s.code,{children:"count()"})]}),(0,r.jsx)(s.td,{children:"Aggregation functions"}),(0,r.jsx)(s.td,{children:(0,r.jsx)(s.code,{children:'df.groupBy("region").avg("amount")'})})]}),(0,r.jsxs)(s.tr,{children:[(0,r.jsx)(s.td,{children:(0,r.jsx)(s.code,{children:"when()"})}),(0,r.jsx)(s.td,{children:"Conditional column logic"}),(0,r.jsx)(s.td,{children:(0,r.jsx)(s.code,{children:'when(col("amount")>1000,"High").otherwise("Low")'})})]})]})]}),"\n",(0,r.jsx)(s.h3,{id:"example-applying-built-in-functions",children:"Example: Applying Built-In Functions"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:'from pyspark.sql import SparkSession\r\nfrom pyspark.sql.functions import col, lit, upper, when, avg\r\n\r\nspark = SparkSession.builder.appName("Built-in Functions").getOrCreate()\r\n\r\ndf = spark.read.csv("data/sales.csv", header=True, inferSchema=True)\r\n\r\ndf = df.withColumn("currency", lit("USD")) \\\r\n       .withColumn("region_upper", upper(col("region"))) \\\r\n       .withColumn("high_value", when(col("amount") > 1000, "Yes").otherwise("No"))\r\n\r\ndf.groupBy("region").agg(avg("amount").alias("avg_sales")).show()\n'})}),"\n",(0,r.jsxs)(s.p,{children:["\ud83d\udca1 ",(0,r.jsx)(s.strong,{children:"Tip:"})," Always check if a ",(0,r.jsx)(s.strong,{children:"built-in function"})," can solve your problem before creating a UDF."]}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsx)(s.h2,{id:"2\ufe0f\u20e3-user-defined-functions-udfs-in-pyspark",children:"2\ufe0f\u20e3 User Defined Functions (UDFs) in PySpark"}),"\n",(0,r.jsxs)(s.p,{children:["When built-in functions are insufficient, use ",(0,r.jsx)(s.strong,{children:"UDFs"})," to implement ",(0,r.jsx)(s.strong,{children:"custom Python logic"})," that runs on Spark DataFrames."]}),"\n",(0,r.jsx)(s.h3,{id:"example-custom-discount-logic",children:"Example: Custom Discount Logic"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:'from pyspark.sql.functions import udf\r\nfrom pyspark.sql.types import DoubleType\r\n\r\ndef discount(amount):\r\n    return amount * 0.9 if amount > 1000 else amount\r\n\r\ndiscount_udf = udf(discount, DoubleType())\r\ndf = df.withColumn("discounted_amount", discount_udf(col("amount")))\r\ndf.show(5)\n'})}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.strong,{children:"Explanation:"})}),"\n",(0,r.jsxs)(s.ol,{children:["\n",(0,r.jsxs)(s.li,{children:["Define a Python function (",(0,r.jsx)(s.code,{children:"discount"}),") for row-wise logic."]}),"\n",(0,r.jsx)(s.li,{children:"Wrap it as a PySpark UDF specifying the return type."}),"\n",(0,r.jsxs)(s.li,{children:["Apply it to a DataFrame column using ",(0,r.jsx)(s.code,{children:"withColumn"}),"."]}),"\n"]}),"\n",(0,r.jsxs)(s.p,{children:["\ud83d\udca1 ",(0,r.jsx)(s.strong,{children:"Pro Tip:"})," Regular UDFs are ",(0,r.jsx)(s.strong,{children:"slower than built-in functions"})," due to Python-JVM serialization overhead."]}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsx)(s.h2,{id:"3\ufe0f\u20e3-pandas-udfs-vectorized-udfs",children:"3\ufe0f\u20e3 Pandas UDFs (Vectorized UDFs)"}),"\n",(0,r.jsxs)(s.p,{children:[(0,r.jsx)(s.strong,{children:"Pandas UDFs"})," use ",(0,r.jsx)(s.strong,{children:"batch processing"})," with Apache Arrow, making them ",(0,r.jsx)(s.strong,{children:"much faster than regular UDFs"})," for large datasets."]}),"\n",(0,r.jsx)(s.h3,{id:"example-batch-discount-using-pandas-udf",children:"Example: Batch Discount Using Pandas UDF"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:'from pyspark.sql.functions import pandas_udf\r\nimport pandas as pd\r\n\r\n@pandas_udf("double")\r\ndef discount_pandas(amount: pd.Series) -> pd.Series:\r\n    return amount.apply(lambda x: x * 0.9 if x > 1000 else x)\r\n\r\ndf = df.withColumn("discounted_batch", discount_pandas(col("amount")))\r\ndf.show(5)\n'})}),"\n",(0,r.jsxs)(s.p,{children:["\ud83d\udca1 ",(0,r.jsx)(s.strong,{children:"Explanation:"})]}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:["Input: ",(0,r.jsx)(s.code,{children:"pd.Series"})," (batch of rows)"]}),"\n",(0,r.jsxs)(s.li,{children:["Output: ",(0,r.jsx)(s.code,{children:"pd.Series"})," of transformed values"]}),"\n",(0,r.jsxs)(s.li,{children:["Benefit: Reduces Python-JVM overhead for ",(0,r.jsx)(s.strong,{children:"large-scale transformations"})]}),"\n"]}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsx)(s.h2,{id:"4\ufe0f\u20e3-real-world-example-combining-functions--udfs",children:"4\ufe0f\u20e3 Real-World Example: Combining Functions & UDFs"}),"\n",(0,r.jsx)(s.pre,{children:(0,r.jsx)(s.code,{className:"language-python",children:'from pyspark.sql.functions import upper, col, udf, pandas_udf\r\nfrom pyspark.sql.types import StringType\r\nimport pandas as pd\r\n\r\n# Built-in function\r\ndf = df.withColumn("region_upper", upper(col("region")))\r\n\r\n# Regular UDF\r\ndef premium_flag(amount):\r\n    return "Premium" if amount > 2000 else "Standard"\r\npremium_udf = udf(premium_flag, StringType())\r\ndf = df.withColumn("customer_type", premium_udf(col("amount")))\r\n\r\n# Pandas UDF for batch discount\r\n@pandas_udf("double")\r\ndef batch_discount(amount: pd.Series) -> pd.Series:\r\n    return amount.apply(lambda x: x * 0.85 if x > 2000 else x)\r\n\r\ndf = df.withColumn("discounted_batch", batch_discount(col("amount")))\r\ndf.show()\n'})}),"\n",(0,r.jsxs)(s.p,{children:["\ud83d\udca1 ",(0,r.jsx)(s.strong,{children:"Tip:"})," Rule of Thumb:"]}),"\n",(0,r.jsxs)(s.ol,{children:["\n",(0,r.jsxs)(s.li,{children:["Prefer ",(0,r.jsx)(s.strong,{children:"built-in functions"})," first."]}),"\n",(0,r.jsxs)(s.li,{children:["Use ",(0,r.jsx)(s.strong,{children:"Pandas UDFs"})," for batch processing."]}),"\n",(0,r.jsxs)(s.li,{children:["Resort to ",(0,r.jsx)(s.strong,{children:"regular UDFs"})," only when necessary."]}),"\n"]}),"\n",(0,r.jsx)(s.hr,{}),"\n",(0,r.jsx)(s.h2,{id:"5\ufe0f\u20e3-key-takeaways-for-pyspark-functions--udfs",children:"5\ufe0f\u20e3 Key Takeaways for PySpark Functions & UDFs"}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Built-in functions:"})," Fast, optimized, Spark-native"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"UDFs:"})," Custom Python logic, slower, use sparingly"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Pandas UDFs:"})," Vectorized, batch-friendly, ideal for large datasets"]}),"\n",(0,r.jsxs)(s.li,{children:[(0,r.jsx)(s.strong,{children:"Best Practice:"})," Always attempt built-ins first, then Pandas UDFs, and finally regular UDFs if needed"]}),"\n"]}),"\n",(0,r.jsx)(s.p,{children:(0,r.jsx)(s.strong,{children:"Use Cases:"})}),"\n",(0,r.jsxs)(s.ul,{children:["\n",(0,r.jsx)(s.li,{children:"Data cleaning & transformations"}),"\n",(0,r.jsx)(s.li,{children:"Conditional business logic"}),"\n",(0,r.jsx)(s.li,{children:"Feature engineering for ML"}),"\n",(0,r.jsx)(s.li,{children:"Large-scale batch computations"}),"\n"]})]})}function u(n={}){const{wrapper:s}={...(0,t.R)(),...n.components};return s?(0,r.jsx)(s,{...n,children:(0,r.jsx)(c,{...n})}):c(n)}},8453:(n,s,e)=>{e.d(s,{R:()=>a,x:()=>o});var i=e(6540);const r={},t=i.createContext(r);function a(n){const s=i.useContext(t);return i.useMemo(function(){return"function"==typeof n?n(s):{...s,...n}},[s,n])}function o(n){let s;return s=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:a(n.components),i.createElement(t.Provider,{value:s},n.children)}}}]);