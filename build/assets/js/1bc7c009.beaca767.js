"use strict";(self.webpackChunkdatacraft_school=self.webpackChunkdatacraft_school||[]).push([[6452],{5998:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>t,default:()=>p,frontMatter:()=>i,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"df-api","title":"DataFrame API \u2014 Select, Filter, WithColumn & Drop","description":"Learn how to use PySpark DataFrame API operations such as select, filter, withColumn, and drop through practical Databricks examples and real-world use cases.","source":"@site/docs-pyspark/df-api.md","sourceDirName":".","slug":"/df-api","permalink":"/pyspark/df-api","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"PySpark","permalink":"/pyspark/tags/py-spark"},{"inline":true,"label":"Apache Spark","permalink":"/pyspark/tags/apache-spark"},{"inline":true,"label":"Big Data","permalink":"/pyspark/tags/big-data"},{"inline":true,"label":"Spark Basics","permalink":"/pyspark/tags/spark-basics"},{"inline":true,"label":"Cluster Computing","permalink":"/pyspark/tags/cluster-computing"},{"inline":true,"label":"Spark Architecture","permalink":"/pyspark/tags/spark-architecture"},{"inline":true,"label":"Driver Program","permalink":"/pyspark/tags/driver-program"},{"inline":true,"label":"Executors","permalink":"/pyspark/tags/executors"},{"inline":true,"label":"Cluster Manager","permalink":"/pyspark/tags/cluster-manager"},{"inline":true,"label":"SparkSession","permalink":"/pyspark/tags/spark-session"},{"inline":true,"label":"SparkContext","permalink":"/pyspark/tags/spark-context"}],"version":"current","frontMatter":{"id":"df-api","title":"DataFrame API \u2014 Select, Filter, WithColumn & Drop","sidebar_label":"DataFrame API","description":"Learn how to use PySpark DataFrame API operations such as select, filter, withColumn, and drop through practical Databricks examples and real-world use cases.","keywords":["PySpark DataFrame API","PySpark select filter","withColumn examples","drop column PySpark","Databricks DataFrame transformations"],"tags":["PySpark","Apache Spark","Big Data","Spark Basics","Cluster Computing","Spark Architecture","Driver Program","Executors","Cluster Manager","SparkSession","SparkContext"]},"sidebar":"tutorialSidebar","previous":{"title":"Creating DataFrames","permalink":"/pyspark/df-create-csv"},"next":{"title":"DataFrame Joins","permalink":"/pyspark/pyspark/joins"}}');var a=r(4848),l=r(8453);const i={id:"df-api",title:"DataFrame API \u2014 Select, Filter, WithColumn & Drop",sidebar_label:"DataFrame API",description:"Learn how to use PySpark DataFrame API operations such as select, filter, withColumn, and drop through practical Databricks examples and real-world use cases.",keywords:["PySpark DataFrame API","PySpark select filter","withColumn examples","drop column PySpark","Databricks DataFrame transformations"],tags:["PySpark","Apache Spark","Big Data","Spark Basics","Cluster Computing","Spark Architecture","Driver Program","Executors","Cluster Manager","SparkSession","SparkContext"]},t="DataFrame API \u2014 Select, Filter, WithColumn & Drop",o={},c=[{value:"Why DataFrame API Matters",id:"why-dataframe-api-matters",level:2},{value:"1. select() \u2014 Choose Columns or Expressions",id:"1-select--choose-columns-or-expressions",level:2},{value:"Basic selection",id:"basic-selection",level:3},{value:"With expressions",id:"with-expressions",level:3},{value:"Rename columns",id:"rename-columns",level:3},{value:"Story Example",id:"story-example",level:3},{value:"2. filter() / where() \u2014 Keep Only Matching Rows",id:"2-filter--where--keep-only-matching-rows",level:2},{value:"Using column expressions",id:"using-column-expressions",level:3},{value:"Using SQL-style string filters",id:"using-sql-style-string-filters",level:3},{value:"Story Example",id:"story-example-1",level:3},{value:"3. withColumn() \u2014 Add or Modify Columns",id:"3-withcolumn--add-or-modify-columns",level:2},{value:"Create a new column",id:"create-a-new-column",level:3},{value:"Modify an existing column",id:"modify-an-existing-column",level:3},{value:"Add conditional logic",id:"add-conditional-logic",level:3},{value:"Story Example",id:"story-example-2",level:3},{value:"4. drop() \u2014 Remove Unneeded Columns",id:"4-drop--remove-unneeded-columns",level:2},{value:"Drop a single column",id:"drop-a-single-column",level:3},{value:"Drop multiple columns",id:"drop-multiple-columns",level:3},{value:"Story Example",id:"story-example-3",level:3},{value:"Putting It All Together \u2014 Real ETL Example",id:"putting-it-all-together--real-etl-example",level:2},{value:"Summary \u2014 Your Core Transformation Toolkit",id:"summary--your-core-transformation-toolkit",level:2}];function d(e){const n={br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"dataframe-api--select-filter-withcolumn--drop",children:"DataFrame API \u2014 Select, Filter, WithColumn & Drop"})}),"\n",(0,a.jsxs)(n.p,{children:["Imagine you're working at ",(0,a.jsx)(n.strong,{children:"NeoMart"}),", where millions of product views, clicks, sessions, and purchases are being collected every day. The raw data is overwhelming, and your job is to convert it into ",(0,a.jsx)(n.strong,{children:"clean, structured, meaningful insights"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["This is where the ",(0,a.jsx)(n.strong,{children:"PySpark DataFrame API"})," becomes your most powerful tool.",(0,a.jsx)(n.br,{}),"\n","With just a few transformations \u2014 ",(0,a.jsx)(n.strong,{children:"select"}),", ",(0,a.jsx)(n.strong,{children:"filter"}),", ",(0,a.jsx)(n.strong,{children:"withColumn"}),", and ",(0,a.jsx)(n.strong,{children:"drop"})," \u2014 you can shape your dataset into a ready-to-analyze form."]}),"\n",(0,a.jsx)(n.p,{children:"These operations form the foundation of every ETL pipeline in Spark and Databricks."}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"why-dataframe-api-matters",children:"Why DataFrame API Matters"}),"\n",(0,a.jsx)(n.p,{children:"DataFrames are:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Optimized"})," through Catalyst engine"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Faster"})," than RDDs"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Easier"})," to use with SQL expressions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Scalable"})," to billions of rows"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"These core functions help transform raw data into analytics-ready data in a clean and efficient way."}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"1-select--choose-columns-or-expressions",children:"1. select() \u2014 Choose Columns or Expressions"}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"select()"})," function allows you to pick specific columns or create new ones using expressions."]}),"\n",(0,a.jsx)(n.h3,{id:"basic-selection",children:"Basic selection"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'df.select("product_id", "price").show()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"with-expressions",children:"With expressions"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'df.select(col("price") * 0.9).show()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"rename-columns",children:"Rename columns"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'df.select(col("price").alias("discounted_price"))\n'})}),"\n",(0,a.jsx)(n.h3,{id:"story-example",children:"Story Example"}),"\n",(0,a.jsx)(n.p,{children:"NeoMart wants only product and revenue:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'sales_df.select("product_id", "revenue").show()\n'})}),"\n",(0,a.jsx)(n.p,{children:"Simple and clean."}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"2-filter--where--keep-only-matching-rows",children:"2. filter() / where() \u2014 Keep Only Matching Rows"}),"\n",(0,a.jsxs)(n.p,{children:["Use ",(0,a.jsx)(n.code,{children:"filter()"})," to keep rows that meet certain conditions."]}),"\n",(0,a.jsx)(n.h3,{id:"using-column-expressions",children:"Using column expressions"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"df.filter(df.price > 100).show()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"using-sql-style-string-filters",children:"Using SQL-style string filters"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"df.where(\"price > 100 AND category = 'electronics'\").show()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"story-example-1",children:"Story Example"}),"\n",(0,a.jsx)(n.p,{children:"NeoMart wants orders worth above $500:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'orders_df.filter(col("amount") > 500).show()\n'})}),"\n",(0,a.jsx)(n.p,{children:"This reduces millions of transactions to just the high-value insights."}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"3-withcolumn--add-or-modify-columns",children:"3. withColumn() \u2014 Add or Modify Columns"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"withColumn()"})," is used to:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Add new fields"}),"\n",(0,a.jsx)(n.li,{children:"Transform existing ones"}),"\n",(0,a.jsx)(n.li,{children:"Apply calculations"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"create-a-new-column",children:"Create a new column"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'df2 = df.withColumn("price_usd", col("price") * 0.013)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"modify-an-existing-column",children:"Modify an existing column"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'df2 = df.withColumn("quantity", col("quantity") + 1)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"add-conditional-logic",children:"Add conditional logic"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'df.withColumn(\r\n    "is_expensive",\r\n    when(col("price") > 1000, True).otherwise(False)\r\n)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"story-example-2",children:"Story Example"}),"\n",(0,a.jsx)(n.p,{children:"NeoMart wants to tag premium products:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'products_df.withColumn(\r\n    "premium_flag",\r\n    col("price") > 1500\r\n)\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"4-drop--remove-unneeded-columns",children:"4. drop() \u2014 Remove Unneeded Columns"}),"\n",(0,a.jsx)(n.p,{children:"Clean up your dataset by removing unnecessary fields."}),"\n",(0,a.jsx)(n.h3,{id:"drop-a-single-column",children:"Drop a single column"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'df.drop("internal_notes")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"drop-multiple-columns",children:"Drop multiple columns"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'df.drop("temp_col", "backup_col")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"story-example-3",children:"Story Example"}),"\n",(0,a.jsx)(n.p,{children:"After processing, NeoMart removes unnecessary metadata:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'events_df.drop("raw_payload", "old_timestamp")\n'})}),"\n",(0,a.jsx)(n.p,{children:"This reduces storage, memory use, and shuffle size."}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"putting-it-all-together--real-etl-example",children:"Putting It All Together \u2014 Real ETL Example"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'clean_df = (\r\n    raw_df\r\n    .select("user_id", "event_type", "amount", "timestamp")\r\n    .filter(col("amount") > 0)\r\n    .withColumn("amount_usd", col("amount") * 0.013)\r\n    .drop("timestamp")  # if not needed for downstream analytics\r\n)\n'})}),"\n",(0,a.jsx)(n.p,{children:"This pipeline:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Picks relevant fields"}),"\n",(0,a.jsx)(n.li,{children:"Filters invalid data"}),"\n",(0,a.jsx)(n.li,{children:"Adds conversion logic"}),"\n",(0,a.jsx)(n.li,{children:"Cleans unnecessary columns"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Exactly what a real-world data engineer does daily."}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"summary--your-core-transformation-toolkit",children:"Summary \u2014 Your Core Transformation Toolkit"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"select()"})," \u2014 choose columns or apply expressions"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"filter() / where()"})," \u2014 remove unwanted rows"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"withColumn()"})," \u2014 add or modify fields"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"drop()"})," \u2014 clean the dataset"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["These core DataFrame operations are the building blocks of every Spark transformation pipeline.\r\nMastering them prepares you for more advanced transformations like ",(0,a.jsx)(n.strong,{children:"joins"}),", ",(0,a.jsx)(n.strong,{children:"aggregations"}),", and ",(0,a.jsx)(n.strong,{children:"window functions"}),"."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsxs)(n.p,{children:["Next, we\u2019ll dive into ",(0,a.jsx)(n.strong,{children:"DataFrame Joins \u2014 Inner, Left, Right & Full Outer"}),", where we connect multiple datasets and unlock relational insights."]})]})}function p(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>t});var s=r(6540);const a={},l=s.createContext(a);function i(e){const n=s.useContext(l);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),s.createElement(l.Provider,{value:n},e.children)}}}]);