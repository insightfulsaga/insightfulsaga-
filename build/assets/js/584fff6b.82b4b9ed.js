"use strict";(self.webpackChunkdatacraft_school=self.webpackChunkdatacraft_school||[]).push([[5758],{6473:(e,a,r)=>{r.r(a),r.d(a,{assets:()=>o,contentTitle:()=>d,default:()=>c,frontMatter:()=>t,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"pyspark-data-io","title":"PySpark Data I/O \u2014 Reading, Writing & Optimizing Big Data","description":"Learn how to efficiently read, write, and process data in PySpark including CSV, JSON, Parquet, ORC, JDBC databases, cloud storage, streaming, and compression. A complete guide for beginners and data engineers.","source":"@site/docs-pyspark/pyspark-data-io.md","sourceDirName":".","slug":"/pyspark/data-io","permalink":"/pyspark/pyspark/data-io","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"PySpark","permalink":"/pyspark/tags/py-spark"},{"inline":true,"label":"Data I/O","permalink":"/pyspark/tags/data-i-o"},{"inline":true,"label":"Big Data","permalink":"/pyspark/tags/big-data"},{"inline":true,"label":"ETL","permalink":"/pyspark/tags/etl"},{"inline":true,"label":"Streaming","permalink":"/pyspark/tags/streaming"}],"version":"current","frontMatter":{"id":"pyspark-data-io","title":"PySpark Data I/O \u2014 Reading, Writing & Optimizing Big Data","sidebar_label":"Data I/O","slug":"/pyspark/data-io","description":"Learn how to efficiently read, write, and process data in PySpark including CSV, JSON, Parquet, ORC, JDBC databases, cloud storage, streaming, and compression. A complete guide for beginners and data engineers.","keywords":["pyspark data io","pyspark read csv","pyspark write parquet","pyspark jdbc","pyspark read json","pyspark streaming data","pyspark cloud storage"],"og:title":"PySpark Data I/O \u2014 Complete Guide for Reading & Writing Data","og:description":"Master PySpark Data I/O \u2014 Learn how to read and write CSV, JSON, Parquet, ORC, connect to databases, handle streaming data, optimize performance with partitioning, bucketing, and compression.","tags":["PySpark","Data I/O","Big Data","ETL","Streaming"]}}');var n=r(4848),i=r(8453);const t={id:"pyspark-data-io",title:"PySpark Data I/O \u2014 Reading, Writing & Optimizing Big Data",sidebar_label:"Data I/O",slug:"/pyspark/data-io",description:"Learn how to efficiently read, write, and process data in PySpark including CSV, JSON, Parquet, ORC, JDBC databases, cloud storage, streaming, and compression. A complete guide for beginners and data engineers.",keywords:["pyspark data io","pyspark read csv","pyspark write parquet","pyspark jdbc","pyspark read json","pyspark streaming data","pyspark cloud storage"],"og:title":"PySpark Data I/O \u2014 Complete Guide for Reading & Writing Data","og:description":"Master PySpark Data I/O \u2014 Learn how to read and write CSV, JSON, Parquet, ORC, connect to databases, handle streaming data, optimize performance with partitioning, bucketing, and compression.",tags:["PySpark","Data I/O","Big Data","ETL","Streaming"]},d="PySpark Data I/O: Read, Write & Optimize Big Data Efficiently",o={},l=[{value:"What is Data I/O in PySpark?",id:"what-is-data-io-in-pyspark",level:2},{value:"1\ufe0f\u20e3 Reading and Writing CSV Files in PySpark",id:"1\ufe0f\u20e3-reading-and-writing-csv-files-in-pyspark",level:2},{value:"Example: Reading &amp; Writing CSV",id:"example-reading--writing-csv",level:3},{value:"Pro Tips for CSV:",id:"pro-tips-for-csv",level:3},{value:"2\ufe0f\u20e3 Parquet &amp; ORC: Columnar Data Formats",id:"2\ufe0f\u20e3-parquet--orc-columnar-data-formats",level:2},{value:"Example: Working with Parquet",id:"example-working-with-parquet",level:3},{value:"Pro Tips:",id:"pro-tips",level:3},{value:"3\ufe0f\u20e3 Reading Data from Databases with JDBC",id:"3\ufe0f\u20e3-reading-data-from-databases-with-jdbc",level:2},{value:"Example: Reading from PostgreSQL",id:"example-reading-from-postgresql",level:3},{value:"Advanced Tips:",id:"advanced-tips",level:3},{value:"4\ufe0f\u20e3 Writing Data to External Storage",id:"4\ufe0f\u20e3-writing-data-to-external-storage",level:2},{value:"Example: Writing to S3 and Databases",id:"example-writing-to-s3-and-databases",level:3},{value:"Pro Tips:",id:"pro-tips-1",level:3},{value:"5\ufe0f\u20e3 Advanced Data I/O Scenarios in PySpark",id:"5\ufe0f\u20e3-advanced-data-io-scenarios-in-pyspark",level:2},{value:"5.1 JSON &amp; Nested Data",id:"51-json--nested-data",level:3},{value:"5.2 Streaming Data Input",id:"52-streaming-data-input",level:3},{value:"5.3 Compression for Storage Optimization",id:"53-compression-for-storage-optimization",level:3},{value:"\ud83d\udd11 PySpark Data I/O Summary",id:"-pyspark-data-io-summary",level:2}];function p(e){const a={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.header,{children:(0,n.jsx)(a.h1,{id:"pyspark-data-io-read-write--optimize-big-data-efficiently",children:"PySpark Data I/O: Read, Write & Optimize Big Data Efficiently"})}),"\n",(0,n.jsxs)(a.p,{children:["Imagine standing in a massive data library: CSV files, Parquet tables, JSON logs, and databases all around you. Your mission is clear\u2014read, process, and save data efficiently without hitting memory limits. This is where ",(0,n.jsx)(a.strong,{children:"PySpark Data I/O"})," comes in, combining Python simplicity with Spark's power for scalable data workflows."]}),"\n",(0,n.jsx)(a.p,{children:"Whether your data is local, in the cloud, or inside a database, PySpark provides tools to manage it all efficiently."}),"\n",(0,n.jsx)(a.hr,{}),"\n",(0,n.jsx)(a.h2,{id:"what-is-data-io-in-pyspark",children:"What is Data I/O in PySpark?"}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Data I/O (Input/Output)"})," is a critical part of any ",(0,n.jsx)(a.strong,{children:"big data pipeline"}),". It involves:"]}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Input (I):"})," Reading data from files, databases, or streaming sources."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.strong,{children:"Output (O):"})," Writing processed data to files, cloud storage, or databases for analysis."]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["With PySpark, you can handle datasets from ",(0,n.jsx)(a.strong,{children:"thousands to billions of rows"}),", making your data engineering tasks scalable and reliable."]}),"\n",(0,n.jsx)(a.hr,{}),"\n",(0,n.jsx)(a.h2,{id:"1\ufe0f\u20e3-reading-and-writing-csv-files-in-pyspark",children:"1\ufe0f\u20e3 Reading and Writing CSV Files in PySpark"}),"\n",(0,n.jsxs)(a.p,{children:["CSV files are ubiquitous in data analytics\u2014they are often the first format you encounter. PySpark makes ",(0,n.jsx)(a.strong,{children:"reading and writing CSVs fast, scalable, and reliable"}),"."]}),"\n",(0,n.jsx)(a.h3,{id:"example-reading--writing-csv",children:"Example: Reading & Writing CSV"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-python",children:'from pyspark.sql import SparkSession\r\n\r\n# Start Spark Session\r\nspark = SparkSession.builder.appName("CSV Example").getOrCreate()\r\n\r\n# Read CSV\r\ndf = spark.read.csv("data/sales.csv", header=True, inferSchema=True)\r\ndf.show(5)\r\ndf.printSchema()\r\n\r\n# Filter rows\r\nfiltered_df = df.filter(df["amount"] > 500)\r\n\r\n# Write CSV\r\nfiltered_df.write.csv("output/sales_filtered.csv", header=True, mode="overwrite")\n'})}),"\n",(0,n.jsx)(a.h3,{id:"pro-tips-for-csv",children:"Pro Tips for CSV:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["Use ",(0,n.jsx)(a.code,{children:'option("delimiter", "\\t")'})," for TSV files."]}),"\n",(0,n.jsxs)(a.li,{children:["Apply ",(0,n.jsx)(a.code,{children:"repartition()"})," before writing ",(0,n.jsx)(a.strong,{children:"large CSV files"})," for better parallelism."]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["\ud83d\udca1 ",(0,n.jsx)(a.strong,{children:"Why it matters:"})," Even millions of rows that would crash Excel can now be processed efficiently."]}),"\n",(0,n.jsx)(a.hr,{}),"\n",(0,n.jsx)(a.h2,{id:"2\ufe0f\u20e3-parquet--orc-columnar-data-formats",children:"2\ufe0f\u20e3 Parquet & ORC: Columnar Data Formats"}),"\n",(0,n.jsxs)(a.p,{children:["For large-scale analytics, ",(0,n.jsx)(a.strong,{children:"columnar formats like Parquet and ORC"})," are essential. They improve ",(0,n.jsx)(a.strong,{children:"query performance, reduce storage size"}),", and enable scalable big data workflows."]}),"\n",(0,n.jsx)(a.h3,{id:"example-working-with-parquet",children:"Example: Working with Parquet"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-python",children:'# Read Parquet\r\nparquet_df = spark.read.parquet("data/sales.parquet")\r\n\r\n# Filter and aggregate\r\nhigh_value_sales = parquet_df.filter(parquet_df["amount"] > 1000)\r\nsummary = high_value_sales.groupBy("region").sum("amount")\r\nsummary.show()\r\n\r\n# Write Parquet\r\nsummary.write.parquet("output/sales_summary.parquet", mode="overwrite")\n'})}),"\n",(0,n.jsx)(a.h3,{id:"pro-tips",children:"Pro Tips:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["Use ",(0,n.jsx)(a.code,{children:'partitionBy("column")'})," to speed up queries."]}),"\n",(0,n.jsxs)(a.li,{children:["Leverage ",(0,n.jsx)(a.strong,{children:"column pruning"})," to reduce memory usage during reads."]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["\ud83d\udca1 ",(0,n.jsx)(a.strong,{children:"Why it matters:"})," Parquet and ORC make storing and querying massive datasets faster and more cost-efficient."]}),"\n",(0,n.jsx)(a.hr,{}),"\n",(0,n.jsx)(a.h2,{id:"3\ufe0f\u20e3-reading-data-from-databases-with-jdbc",children:"3\ufe0f\u20e3 Reading Data from Databases with JDBC"}),"\n",(0,n.jsxs)(a.p,{children:["Sometimes data is stored in relational databases. PySpark integrates seamlessly with ",(0,n.jsx)(a.strong,{children:"JDBC"})," to read tables into ",(0,n.jsx)(a.strong,{children:"Spark DataFrames"}),"."]}),"\n",(0,n.jsx)(a.h3,{id:"example-reading-from-postgresql",children:"Example: Reading from PostgreSQL"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-python",children:'jdbc_url = "jdbc:postgresql://localhost:5432/salesdb"\r\nproperties = {"user": "postgres", "password": "mypassword", "driver": "org.postgresql.Driver"}\r\n\r\n# Load table\r\ndb_df = spark.read.jdbc(url=jdbc_url, table="transactions", properties=properties)\r\ndb_df.show(5)\n'})}),"\n",(0,n.jsx)(a.h3,{id:"advanced-tips",children:"Advanced Tips:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["Use ",(0,n.jsx)(a.strong,{children:"pushdown queries"})," to filter rows before loading:"]}),"\n"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-python",children:'query = "(SELECT * FROM transactions WHERE amount > 1000) AS high_sales"\r\ndf_filtered = spark.read.jdbc(url=jdbc_url, table=query, properties=properties)\n'})}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["Partition ",(0,n.jsx)(a.strong,{children:"large tables"})," by numeric or date columns for faster parallel reads."]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["\ud83d\udca1 ",(0,n.jsx)(a.strong,{children:"Why it matters:"})," Handle millions of database rows efficiently across Spark clusters."]}),"\n",(0,n.jsx)(a.hr,{}),"\n",(0,n.jsx)(a.h2,{id:"4\ufe0f\u20e3-writing-data-to-external-storage",children:"4\ufe0f\u20e3 Writing Data to External Storage"}),"\n",(0,n.jsxs)(a.p,{children:["After processing, save your results reliably\u2014whether to ",(0,n.jsx)(a.strong,{children:"cloud storage, HDFS, or databases"}),"."]}),"\n",(0,n.jsx)(a.h3,{id:"example-writing-to-s3-and-databases",children:"Example: Writing to S3 and Databases"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-python",children:'# Write to S3\r\nfiltered_df.write.parquet("s3://my-bucket/sales_filtered/", mode="overwrite")\r\n\r\n# Append back to database\r\nfiltered_df.write.jdbc(url=jdbc_url, table="filtered_transactions", mode="append", properties=properties)\n'})}),"\n",(0,n.jsx)(a.h3,{id:"pro-tips-1",children:"Pro Tips:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["Use ",(0,n.jsx)(a.code,{children:"bucketBy()"})," and ",(0,n.jsx)(a.code,{children:"sortBy()"})," for optimized storage and queries."]}),"\n",(0,n.jsxs)(a.li,{children:[(0,n.jsx)(a.code,{children:'mode="overwrite"'})," replaces datasets, ",(0,n.jsx)(a.code,{children:"append"})," adds new data incrementally."]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["\ud83d\udca1 ",(0,n.jsx)(a.strong,{children:"Why it matters:"})," Store insights efficiently for dashboards, reporting, or further analysis."]}),"\n",(0,n.jsx)(a.hr,{}),"\n",(0,n.jsx)(a.h2,{id:"5\ufe0f\u20e3-advanced-data-io-scenarios-in-pyspark",children:"5\ufe0f\u20e3 Advanced Data I/O Scenarios in PySpark"}),"\n",(0,n.jsx)(a.h3,{id:"51-json--nested-data",children:"5.1 JSON & Nested Data"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-python",children:'json_df = spark.read.json("data/events.json", multiLine=True)\r\njson_df.select("user.id", "event.type").show()\n'})}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["Handles ",(0,n.jsx)(a.strong,{children:"nested and complex structures"}),"."]}),"\n",(0,n.jsxs)(a.li,{children:["Ideal for ",(0,n.jsx)(a.strong,{children:"logs or API outputs"}),"."]}),"\n"]}),"\n",(0,n.jsx)(a.h3,{id:"52-streaming-data-input",children:"5.2 Streaming Data Input"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-python",children:'stream_df = spark.readStream.format("socket").option("host", "localhost").option("port", 9999).load()\r\nstream_df.printSchema()\n'})}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["Enables ",(0,n.jsx)(a.strong,{children:"real-time analytics"})," on live data streams."]}),"\n"]}),"\n",(0,n.jsx)(a.h3,{id:"53-compression-for-storage-optimization",children:"5.3 Compression for Storage Optimization"}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{className:"language-python",children:'df.write.option("compression", "snappy").parquet("output/compressed_sales.parquet")\n'})}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Reduces storage costs and speeds up I/O operations."}),"\n"]}),"\n",(0,n.jsx)(a.hr,{}),"\n",(0,n.jsx)(a.h2,{id:"-pyspark-data-io-summary",children:"\ud83d\udd11 PySpark Data I/O Summary"}),"\n",(0,n.jsx)(a.p,{children:"PySpark Data I/O allows you to:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsxs)(a.li,{children:["\ud83d\udcc4 Read and write ",(0,n.jsx)(a.strong,{children:"CSV, JSON, Parquet, ORC"})," effortlessly."]}),"\n",(0,n.jsxs)(a.li,{children:["\ud83c\udfdb\ufe0f Connect to ",(0,n.jsx)(a.strong,{children:"databases"})," and query large datasets efficiently."]}),"\n",(0,n.jsxs)(a.li,{children:["\u2601\ufe0f Save data to ",(0,n.jsx)(a.strong,{children:"cloud storage or distributed systems"})," reliably."]}),"\n",(0,n.jsxs)(a.li,{children:["\u23f1\ufe0f Process ",(0,n.jsx)(a.strong,{children:"streaming data"})," and handle ",(0,n.jsx)(a.strong,{children:"compressed formats"}),"."]}),"\n",(0,n.jsxs)(a.li,{children:["\u26a1 Optimize ",(0,n.jsx)(a.strong,{children:"performance"})," using partitioning, bucketing, and columnar storage."]}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:[(0,n.jsx)(a.strong,{children:"Why it\u2019s powerful:"})," Handle millions or billions of rows smoothly, focus on ",(0,n.jsx)(a.strong,{children:"data analysis, transformation, and insights"}),", and forget about memory limitations or format constraints."]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{})})]})}function c(e={}){const{wrapper:a}={...(0,i.R)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(p,{...e})}):p(e)}},8453:(e,a,r)=>{r.d(a,{R:()=>t,x:()=>d});var s=r(6540);const n={},i=s.createContext(n);function t(e){const a=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function d(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:t(e.components),s.createElement(i.Provider,{value:a},e.children)}}}]);