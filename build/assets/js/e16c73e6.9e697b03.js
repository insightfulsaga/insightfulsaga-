"use strict";(self.webpackChunkdatacraft_school=self.webpackChunkdatacraft_school||[]).push([[8591],{5607:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>o,contentTitle:()=>l,default:()=>d,frontMatter:()=>t,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"df-aggregations","title":"Aggregations & GroupBy \u2014 Sum, Count, Avg, Max & Min","description":"Learn how to perform aggregations in PySpark using groupBy, sum, count, average, max, and min functions with practical Databricks examples.","source":"@site/docs-pyspark/df-aggregations.md","sourceDirName":".","slug":"/df-aggregations","permalink":"/pyspark/df-aggregations","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"PySpark","permalink":"/pyspark/tags/py-spark"},{"inline":true,"label":"Apache Spark","permalink":"/pyspark/tags/apache-spark"},{"inline":true,"label":"Big Data","permalink":"/pyspark/tags/big-data"},{"inline":true,"label":"Spark Basics","permalink":"/pyspark/tags/spark-basics"},{"inline":true,"label":"Cluster Computing","permalink":"/pyspark/tags/cluster-computing"},{"inline":true,"label":"Spark Architecture","permalink":"/pyspark/tags/spark-architecture"},{"inline":true,"label":"Driver Program","permalink":"/pyspark/tags/driver-program"},{"inline":true,"label":"Executors","permalink":"/pyspark/tags/executors"},{"inline":true,"label":"Cluster Manager","permalink":"/pyspark/tags/cluster-manager"},{"inline":true,"label":"SparkSession","permalink":"/pyspark/tags/spark-session"},{"inline":true,"label":"SparkContext","permalink":"/pyspark/tags/spark-context"}],"version":"current","frontMatter":{"id":"df-aggregations","title":"Aggregations & GroupBy \u2014 Sum, Count, Avg, Max & Min","sidebar_label":"Aggregations & GroupBy","description":"Learn how to perform aggregations in PySpark using groupBy, sum, count, average, max, and min functions with practical Databricks examples.","tags":["PySpark","Apache Spark","Big Data","Spark Basics","Cluster Computing","Spark Architecture","Driver Program","Executors","Cluster Manager","SparkSession","SparkContext"]}}');var s=n(4848),i=n(8453);const t={id:"df-aggregations",title:"Aggregations & GroupBy \u2014 Sum, Count, Avg, Max & Min",sidebar_label:"Aggregations & GroupBy",description:"Learn how to perform aggregations in PySpark using groupBy, sum, count, average, max, and min functions with practical Databricks examples.",tags:["PySpark","Apache Spark","Big Data","Spark Basics","Cluster Computing","Spark Architecture","Driver Program","Executors","Cluster Manager","SparkSession","SparkContext"]},l="Aggregations & GroupBy \u2014 Sum, Count, Avg, Max & Min",o={},c=[{value:"Why Aggregations Matter",id:"why-aggregations-matter",level:2},{value:"1. Basic Aggregations with groupBy",id:"1-basic-aggregations-with-groupby",level:2},{value:"Example \u2014 Total revenue by category",id:"example--total-revenue-by-category",level:3},{value:"Story Example",id:"story-example",level:3},{value:"2. Quick Aggregation Functions",id:"2-quick-aggregation-functions",level:2},{value:"Example \u2014 Product-level statistics",id:"example--product-level-statistics",level:3},{value:"3. Aggregation with Multiple Columns",id:"3-aggregation-with-multiple-columns",level:2},{value:"Story Example",id:"story-example-1",level:3},{value:"4. Using SQL for Aggregations (Optional)",id:"4-using-sql-for-aggregations-optional",level:2},{value:"Summary",id:"summary",level:2}];function g(e){const r={br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(r.header,{children:(0,s.jsx)(r.h1,{id:"aggregations--groupby--sum-count-avg-max--min",children:"Aggregations & GroupBy \u2014 Sum, Count, Avg, Max & Min"})}),"\n",(0,s.jsxs)(r.p,{children:["At ",(0,s.jsx)(r.strong,{children:"NeoMart"}),", raw data is generated every second \u2014 orders, clicks, sessions, and product interactions.",(0,s.jsx)(r.br,{}),"\n","To make sense of billions of rows, analysts need ",(0,s.jsx)(r.strong,{children:"aggregated insights"}),": total revenue, number of orders, average cart value, highest-selling products."]}),"\n",(0,s.jsxs)(r.p,{children:["This is where ",(0,s.jsx)(r.strong,{children:"groupBy()"})," and ",(0,s.jsx)(r.strong,{children:"aggregations"})," in PySpark become essential."]}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"why-aggregations-matter",children:"Why Aggregations Matter"}),"\n",(0,s.jsx)(r.p,{children:"Aggregations help you:"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsx)(r.li,{children:"Summarize large datasets efficiently"}),"\n",(0,s.jsx)(r.li,{children:"Generate key metrics for dashboards"}),"\n",(0,s.jsx)(r.li,{children:"Provide input for machine learning"}),"\n",(0,s.jsx)(r.li,{children:"Analyze trends across categories, regions, or time"}),"\n"]}),"\n",(0,s.jsxs)(r.p,{children:["Without aggregation, data remains just a ",(0,s.jsx)(r.strong,{children:"massive raw table"}),"."]}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"1-basic-aggregations-with-groupby",children:"1. Basic Aggregations with groupBy"}),"\n",(0,s.jsxs)(r.p,{children:["The ",(0,s.jsx)(r.code,{children:"groupBy()"})," method allows you to group rows by one or more columns and apply aggregate functions."]}),"\n",(0,s.jsx)(r.h3,{id:"example--total-revenue-by-category",children:"Example \u2014 Total revenue by category"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'from pyspark.sql.functions import sum, avg, count, max, min\r\n\r\ndf.groupBy("category").agg(\r\n    sum("revenue").alias("total_revenue"),\r\n    avg("revenue").alias("avg_revenue"),\r\n    count("*").alias("total_orders")\r\n).show()\n'})}),"\n",(0,s.jsx)(r.h3,{id:"story-example",children:"Story Example"}),"\n",(0,s.jsxs)(r.p,{children:["NeoMart wants ",(0,s.jsx)(r.strong,{children:"daily sales metrics by category"}),":"]}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsx)(r.li,{children:"Electronics: $1M total revenue, 5,000 orders"}),"\n",(0,s.jsx)(r.li,{children:"Clothing: $500K total revenue, 3,200 orders"}),"\n"]}),"\n",(0,s.jsx)(r.p,{children:"Aggregation converts raw transactions into actionable metrics."}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"2-quick-aggregation-functions",children:"2. Quick Aggregation Functions"}),"\n",(0,s.jsxs)(r.table,{children:[(0,s.jsx)(r.thead,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.th,{children:"Function"}),(0,s.jsx)(r.th,{children:"Example"}),(0,s.jsx)(r.th,{children:"Description"})]})}),(0,s.jsxs)(r.tbody,{children:[(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"sum()"})}),(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:'sum("sales")'})}),(0,s.jsx)(r.td,{children:"Total of a numeric column"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"avg()"})}),(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:'avg("price")'})}),(0,s.jsx)(r.td,{children:"Average value"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"count()"})}),(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:'count("*")'})}),(0,s.jsx)(r.td,{children:"Count of rows"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"max()"})}),(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:'max("revenue")'})}),(0,s.jsx)(r.td,{children:"Maximum value"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"min()"})}),(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:'min("discount")'})}),(0,s.jsx)(r.td,{children:"Minimum value"})]})]})]}),"\n",(0,s.jsx)(r.h3,{id:"example--product-level-statistics",children:"Example \u2014 Product-level statistics"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'df.groupBy("product_id").agg(\r\n    max("price").alias("max_price"),\r\n    min("price").alias("min_price"),\r\n    avg("price").alias("avg_price")\r\n).show()\n'})}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"3-aggregation-with-multiple-columns",children:"3. Aggregation with Multiple Columns"}),"\n",(0,s.jsx)(r.p,{children:"You can group by multiple columns to analyze intersections:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'df.groupBy("category", "region").agg(\r\n    sum("revenue").alias("total_revenue"),\r\n    count("*").alias("orders_count")\r\n).show()\n'})}),"\n",(0,s.jsx)(r.h3,{id:"story-example-1",children:"Story Example"}),"\n",(0,s.jsxs)(r.p,{children:["NeoMart wants revenue by ",(0,s.jsx)(r.strong,{children:"category and region"})," to plan inventory and marketing campaigns."]}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"4-using-sql-for-aggregations-optional",children:"4. Using SQL for Aggregations (Optional)"}),"\n",(0,s.jsx)(r.p,{children:"PySpark allows SQL-style aggregation:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'df.createOrReplaceTempView("sales")\r\nspark.sql("""\r\n    SELECT category, SUM(revenue) AS total_revenue, COUNT(*) AS orders\r\n    FROM sales\r\n    GROUP BY category\r\n""").show()\n'})}),"\n",(0,s.jsx)(r.p,{children:"This is useful for analysts comfortable with SQL syntax."}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsx)(r.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsxs)(r.p,{children:["Aggregations transform ",(0,s.jsx)(r.strong,{children:"raw, row-level data"})," into ",(0,s.jsx)(r.strong,{children:"business insights"}),":"]}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.code,{children:"groupBy()"})," + ",(0,s.jsx)(r.code,{children:"agg()"})," is the foundation"]}),"\n",(0,s.jsxs)(r.li,{children:["Functions like ",(0,s.jsx)(r.strong,{children:"sum, count, avg, max, min"})," generate metrics"]}),"\n",(0,s.jsx)(r.li,{children:"Multi-column grouping allows fine-grained analysis"}),"\n",(0,s.jsx)(r.li,{children:"SQL syntax provides flexibility for analysts"}),"\n"]}),"\n",(0,s.jsxs)(r.p,{children:["In short, aggregation is ",(0,s.jsx)(r.strong,{children:"where raw Spark tables turn into intelligence"})," for dashboards, ML, and reporting."]}),"\n",(0,s.jsx)(r.hr,{}),"\n",(0,s.jsxs)(r.p,{children:["Next, we\u2019ll explore ",(0,s.jsx)(r.strong,{children:"Window Functions in PySpark DataFrames"}),", enabling running totals, rankings, and time-based calculations."]}),"\n",(0,s.jsx)(r.p,{children:"`"})]})}function d(e={}){const{wrapper:r}={...(0,i.R)(),...e.components};return r?(0,s.jsx)(r,{...e,children:(0,s.jsx)(g,{...e})}):g(e)}},8453:(e,r,n)=>{n.d(r,{R:()=>t,x:()=>l});var a=n(6540);const s={},i=a.createContext(s);function t(e){const r=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function l(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),a.createElement(i.Provider,{value:r},e.children)}}}]);