"use strict";(self.webpackChunkdatacraft_school=self.webpackChunkdatacraft_school||[]).push([[501],{2483:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>p,frontMatter:()=>i,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"rdd-basics","title":"RDD Basics \u2014 Creation, Transformation & Actions","description":"Learn the fundamentals of RDDs in Apache Spark, including how to create them, apply transformations, trigger actions, and understand their importance in distributed data processing.","source":"@site/docs-pyspark/rdd-basics.md","sourceDirName":".","slug":"/rdd-basics","permalink":"/pyspark/rdd-basics","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"PySpark","permalink":"/pyspark/tags/py-spark"},{"inline":true,"label":"Apache Spark","permalink":"/pyspark/tags/apache-spark"},{"inline":true,"label":"Big Data","permalink":"/pyspark/tags/big-data"},{"inline":true,"label":"Spark Basics","permalink":"/pyspark/tags/spark-basics"},{"inline":true,"label":"Cluster Computing","permalink":"/pyspark/tags/cluster-computing"},{"inline":true,"label":"Spark Architecture","permalink":"/pyspark/tags/spark-architecture"},{"inline":true,"label":"Driver Program","permalink":"/pyspark/tags/driver-program"},{"inline":true,"label":"Executors","permalink":"/pyspark/tags/executors"},{"inline":true,"label":"Cluster Manager","permalink":"/pyspark/tags/cluster-manager"},{"inline":true,"label":"SparkSession","permalink":"/pyspark/tags/spark-session"},{"inline":true,"label":"SparkContext","permalink":"/pyspark/tags/spark-context"},{"inline":true,"label":"RDD","permalink":"/pyspark/tags/rdd"},{"inline":true,"label":"RDD Transformations","permalink":"/pyspark/tags/rdd-transformations"},{"inline":true,"label":"RDD Actions","permalink":"/pyspark/tags/rdd-actions"},{"inline":true,"label":"Key-Value RDD","permalink":"/pyspark/tags/key-value-rdd"},{"inline":true,"label":"RDD Caching","permalink":"/pyspark/tags/rdd-caching"},{"inline":true,"label":"DataFrame","permalink":"/pyspark/tags/data-frame"},{"inline":true,"label":"DataFrame API","permalink":"/pyspark/tags/data-frame-api"},{"inline":true,"label":"Column Operations","permalink":"/pyspark/tags/column-operations"},{"inline":true,"label":"DataFrame Joins","permalink":"/pyspark/tags/data-frame-joins"},{"inline":true,"label":"Aggregations","permalink":"/pyspark/tags/aggregations"},{"inline":true,"label":"GroupBy","permalink":"/pyspark/tags/group-by"},{"inline":true,"label":"Window Functions","permalink":"/pyspark/tags/window-functions"},{"inline":true,"label":"Missing Data Handling","permalink":"/pyspark/tags/missing-data-handling"},{"inline":true,"label":"Spark SQL","permalink":"/pyspark/tags/spark-sql"},{"inline":true,"label":"Temp Views","permalink":"/pyspark/tags/temp-views"},{"inline":true,"label":"Spark SQL Functions","permalink":"/pyspark/tags/spark-sql-functions"},{"inline":true,"label":"UDF","permalink":"/pyspark/tags/udf"},{"inline":true,"label":"UDAF","permalink":"/pyspark/tags/udaf"}],"version":"current","frontMatter":{"id":"rdd-basics","title":"RDD Basics \u2014 Creation, Transformation & Actions","sidebar_label":"RDD Basics","description":"Learn the fundamentals of RDDs in Apache Spark, including how to create them, apply transformations, trigger actions, and understand their importance in distributed data processing.","keywords":["RDD basics","Spark RDD tutorial","PySpark RDD transformations","PySpark RDD actions","Apache Spark distributed computing"],"tags":["PySpark","Apache Spark","Big Data","Spark Basics","Cluster Computing","Spark Architecture","Driver Program","Executors","Cluster Manager","SparkSession","SparkContext","RDD","RDD Transformations","RDD Actions","Key-Value RDD","RDD Caching","DataFrame","DataFrame API","Column Operations","DataFrame Joins","Aggregations","GroupBy","Window Functions","Missing Data Handling","Spark SQL","Temp Views","Spark SQL Functions","UDF","UDAF"]},"sidebar":"tutorialSidebar","previous":{"title":"First PySpark Job","permalink":"/pyspark/pyspark-first-job"},"next":{"title":"Map, FlatMap & Filter","permalink":"/pyspark/rdd-map-flatmap-filter"}}');var a=r(4848),t=r(8453);const i={id:"rdd-basics",title:"RDD Basics \u2014 Creation, Transformation & Actions",sidebar_label:"RDD Basics",description:"Learn the fundamentals of RDDs in Apache Spark, including how to create them, apply transformations, trigger actions, and understand their importance in distributed data processing.",keywords:["RDD basics","Spark RDD tutorial","PySpark RDD transformations","PySpark RDD actions","Apache Spark distributed computing"],tags:["PySpark","Apache Spark","Big Data","Spark Basics","Cluster Computing","Spark Architecture","Driver Program","Executors","Cluster Manager","SparkSession","SparkContext","RDD","RDD Transformations","RDD Actions","Key-Value RDD","RDD Caching","DataFrame","DataFrame API","Column Operations","DataFrame Joins","Aggregations","GroupBy","Window Functions","Missing Data Handling","Spark SQL","Temp Views","Spark SQL Functions","UDF","UDAF"]},l="RDD Basics \u2014 Creation, Transformation & Actions",o={},d=[{value:"What is an RDD?",id:"what-is-an-rdd",level:2},{value:"Story Example: NeoMart and the Log Explosion",id:"story-example-neomart-and-the-log-explosion",level:2},{value:"Creating RDDs in PySpark",id:"creating-rdds-in-pyspark",level:2},{value:"1. From Existing Collections (Parallelizing)",id:"1-from-existing-collections-parallelizing",level:3},{value:"2. From External Storage",id:"2-from-external-storage",level:3},{value:"3. From Transformations on Other RDDs",id:"3-from-transformations-on-other-rdds",level:3},{value:"RDD Transformations \u2014 Building the Data Pipeline",id:"rdd-transformations--building-the-data-pipeline",level:2},{value:"\ud83d\udd39 <code>map()</code> \u2014 Transform Each Element",id:"-map--transform-each-element",level:3},{value:"\ud83d\udd39 <code>flatMap()</code> \u2014 Flatten Nested Outputs",id:"-flatmap--flatten-nested-outputs",level:3},{value:"\ud83d\udd39 <code>filter()</code> \u2014 Keep Only Matching Elements",id:"-filter--keep-only-matching-elements",level:3},{value:"Lazy Evaluation in Action (Story Twist)",id:"lazy-evaluation-in-action-story-twist",level:3},{value:"RDD Actions \u2014 Triggering the Execution",id:"rdd-actions--triggering-the-execution",level:2},{value:"Common Actions",id:"common-actions",level:3},{value:"Example: Triggering Execution",id:"example-triggering-execution",level:3},{value:"Behind the Scenes \u2014 Why RDDs Are Fast",id:"behind-the-scenes--why-rdds-are-fast",level:2},{value:"Summary \u2014 RDDs: The Foundation of Spark",id:"summary--rdds-the-foundation-of-spark",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"rdd-basics--creation-transformation--actions",children:"RDD Basics \u2014 Creation, Transformation & Actions"})}),"\n",(0,a.jsxs)(n.p,{children:["Imagine you\u2019re working as a data engineer at ",(0,a.jsx)(n.strong,{children:"NeoMart"}),", an e-commerce giant. Every minute, tens of thousands of product views, clicks, and purchases stream into your system. The analytics team wants insights ",(0,a.jsx)(n.strong,{children:"immediately"}),", but your Python script struggles \u2014 looping through millions of records takes forever."]}),"\n",(0,a.jsxs)(n.p,{children:["To solve this, you step into the world of ",(0,a.jsx)(n.strong,{children:"RDDs (Resilient Distributed Datasets)"})," \u2014 the ",(0,a.jsx)(n.strong,{children:"core programming abstraction of Apache Spark"}),". With RDDs, datasets are ",(0,a.jsx)(n.strong,{children:"broken into distributed chunks"}),", processed in parallel across a cluster, and brought back together to deliver insights ",(0,a.jsx)(n.strong,{children:"faster than any traditional Python workflow"}),"."]}),"\n",(0,a.jsx)(n.p,{children:"Welcome to the foundation of Spark."}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"what-is-an-rdd",children:"What is an RDD?"}),"\n",(0,a.jsxs)(n.p,{children:["An ",(0,a.jsx)(n.strong,{children:"RDD (Resilient Distributed Dataset)"})," is a ",(0,a.jsx)(n.strong,{children:"fault-tolerant"}),", ",(0,a.jsx)(n.strong,{children:"distributed collection of data"})," in Spark that you can process in parallel."]}),"\n",(0,a.jsx)(n.p,{children:"RDDs are:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Immutable"})," \u2014 once created, they never change"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Partitioned"})," \u2014 split across clusters for parallelism"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Lazy evaluated"})," \u2014 operations run only when needed"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Fault tolerant"})," \u2014 can recover from node failures using lineage"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Even though DataFrames dominate modern Spark workflows, ",(0,a.jsx)(n.strong,{children:"RDDs still matter"}),", especially for low-level transformations, custom logic, or working with unstructured data."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"story-example-neomart-and-the-log-explosion",children:"Story Example: NeoMart and the Log Explosion"}),"\n",(0,a.jsx)(n.p,{children:"NeoMart stores its clickstream logs in thousands of text files. Using Python alone:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Processing takes ",(0,a.jsx)(n.strong,{children:"hours"})]}),"\n",(0,a.jsx)(n.li,{children:"Memory errors happen frequently"}),"\n",(0,a.jsx)(n.li,{children:"Scaling to more data means rewriting your scripts"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Using Spark RDDs:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Files are read in ",(0,a.jsx)(n.strong,{children:"parallel"})]}),"\n",(0,a.jsx)(n.li,{children:"Processing is distributed across the cluster"}),"\n",(0,a.jsxs)(n.li,{children:["Results are produced in ",(0,a.jsx)(n.strong,{children:"minutes"})," instead of hours"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"This is the power of RDDs."}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"creating-rdds-in-pyspark",children:"Creating RDDs in PySpark"}),"\n",(0,a.jsx)(n.p,{children:"You can create RDDs in three main ways:"}),"\n",(0,a.jsx)(n.h3,{id:"1-from-existing-collections-parallelizing",children:"1. From Existing Collections (Parallelizing)"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"data = [1, 2, 3, 4, 5]\r\nrdd = spark.sparkContext.parallelize(data)\n"})}),"\n",(0,a.jsx)(n.h3,{id:"2-from-external-storage",children:"2. From External Storage"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'rdd = spark.sparkContext.textFile("/mnt/logs/clickstream.txt")\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Supports ",(0,a.jsx)(n.strong,{children:"HDFS"}),", ",(0,a.jsx)(n.strong,{children:"S3"}),", ",(0,a.jsx)(n.strong,{children:"Azure Blob"}),", ",(0,a.jsx)(n.strong,{children:"ADLS"}),", and local files."]}),"\n",(0,a.jsx)(n.h3,{id:"3-from-transformations-on-other-rdds",children:"3. From Transformations on Other RDDs"}),"\n",(0,a.jsx)(n.p,{children:"(covered below)"}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"rdd-transformations--building-the-data-pipeline",children:"RDD Transformations \u2014 Building the Data Pipeline"}),"\n",(0,a.jsxs)(n.p,{children:["Transformations create ",(0,a.jsx)(n.strong,{children:"new RDDs"})," from existing ones. They are ",(0,a.jsx)(n.strong,{children:"lazy"}),", meaning nothing runs until an action is called."]}),"\n",(0,a.jsxs)(n.h3,{id:"-map--transform-each-element",children:["\ud83d\udd39 ",(0,a.jsx)(n.code,{children:"map()"})," \u2014 Transform Each Element"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"numbers = sc.parallelize([1, 2, 3])\r\nmapped = numbers.map(lambda x: x * 10)\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Output: ",(0,a.jsx)(n.code,{children:"[10, 20, 30]"})]}),"\n",(0,a.jsxs)(n.h3,{id:"-flatmap--flatten-nested-outputs",children:["\ud83d\udd39 ",(0,a.jsx)(n.code,{children:"flatMap()"})," \u2014 Flatten Nested Outputs"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'lines = sc.parallelize(["a b", "c d"])\r\nwords = lines.flatMap(lambda x: x.split(" "))\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Output: ",(0,a.jsx)(n.code,{children:'["a", "b", "c", "d"]'})]}),"\n",(0,a.jsxs)(n.h3,{id:"-filter--keep-only-matching-elements",children:["\ud83d\udd39 ",(0,a.jsx)(n.code,{children:"filter()"})," \u2014 Keep Only Matching Elements"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"filtered = numbers.filter(lambda x: x % 2 == 0)\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Output: ",(0,a.jsx)(n.code,{children:"[2]"})]}),"\n",(0,a.jsx)(n.h3,{id:"lazy-evaluation-in-action-story-twist",children:"Lazy Evaluation in Action (Story Twist)"}),"\n",(0,a.jsx)(n.p,{children:"NeoMart wants to extract only successful purchases:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'logs = sc.textFile("/mnt/logs/events.txt")\r\n\r\npurchases = logs \\\r\n    .filter(lambda x: "purchase" in x) \\\r\n    .map(lambda x: x.split(",")[2])  # extract product ID\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Even though two transformations are defined, ",(0,a.jsx)(n.strong,{children:"nothing executes yet"})," \u2014 Spark waits for a final action."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"rdd-actions--triggering-the-execution",children:"RDD Actions \u2014 Triggering the Execution"}),"\n",(0,a.jsxs)(n.p,{children:["Actions ",(0,a.jsx)(n.strong,{children:"execute the lineage"})," of transformations and return a result."]}),"\n",(0,a.jsx)(n.h3,{id:"common-actions",children:"Common Actions"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Action"}),(0,a.jsx)(n.th,{children:"Description"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"collect()"})}),(0,a.jsx)(n.td,{children:"Returns all elements to the driver"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"take(n)"})}),(0,a.jsx)(n.td,{children:"Returns first n elements"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"count()"})}),(0,a.jsx)(n.td,{children:"Counts number of elements"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"first()"})}),(0,a.jsx)(n.td,{children:"Returns the first element"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"reduce(func)"})}),(0,a.jsx)(n.td,{children:"Aggregates RDD to a single value"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"saveAsTextFile()"})}),(0,a.jsx)(n.td,{children:"Writes output to storage"})]})]})]}),"\n",(0,a.jsx)(n.h3,{id:"example-triggering-execution",children:"Example: Triggering Execution"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"result = purchases.take(5)\r\nprint(result)\n"})}),"\n",(0,a.jsx)(n.p,{children:"Now Spark runs the entire pipeline across the cluster."}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"behind-the-scenes--why-rdds-are-fast",children:"Behind the Scenes \u2014 Why RDDs Are Fast"}),"\n",(0,a.jsx)(n.p,{children:"RDDs use:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"Parallelization"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"In-memory storage"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"Partition-based processing"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"Fault tolerance through lineage graphs"})}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"This enables high-speed analytics on massive datasets \u2014 perfect for NeoMart\u2019s high-volume logs."}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"summary--rdds-the-foundation-of-spark",children:"Summary \u2014 RDDs: The Foundation of Spark"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["RDDs are ",(0,a.jsx)(n.strong,{children:"distributed, immutable, and fault-tolerant"})," datasets."]}),"\n",(0,a.jsxs)(n.li,{children:["They are created from ",(0,a.jsx)(n.strong,{children:"collections, files, or transformations"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:["Transformations like ",(0,a.jsx)(n.code,{children:"map"}),", ",(0,a.jsx)(n.code,{children:"flatMap"}),", ",(0,a.jsx)(n.code,{children:"filter"})," build your pipeline."]}),"\n",(0,a.jsxs)(n.li,{children:["Actions like ",(0,a.jsx)(n.code,{children:"collect"}),", ",(0,a.jsx)(n.code,{children:"take"}),", and ",(0,a.jsx)(n.code,{children:"reduce"})," ",(0,a.jsx)(n.strong,{children:"trigger execution"}),"."]}),"\n",(0,a.jsx)(n.li,{children:"RDDs remain essential for low-level transformations and high-performance custom logic."}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["RDDs are the ",(0,a.jsx)(n.strong,{children:"engine beneath the hood"})," of Spark \u2014 understanding them gives you complete control over distributed computation."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsxs)(n.p,{children:["Next up: ",(0,a.jsx)(n.strong,{children:"Map, FlatMap, Filter \u2014 Detailed Examples"})," where we'll go deeper into each transformation with more real-world scenarios and Databricks-focused insights."]})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>l});var s=r(6540);const a={},t=s.createContext(a);function i(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);