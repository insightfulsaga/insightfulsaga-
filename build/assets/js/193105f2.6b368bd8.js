"use strict";(self.webpackChunkdatacraft_school=self.webpackChunkdatacraft_school||[]).push([[8744],{6041:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>l,contentTitle:()=>t,default:()=>c,frontMatter:()=>o,metadata:()=>s,toc:()=>p});const s=JSON.parse('{"id":"pyspark-interview-questions-part4","title":"Essential PySpark Interview Question & Answer(Explained Through Real-World Stories) \u2013 Part 4","description":"26. How do you use regular expressions in PySpark?","source":"@site/docs-pyspark/pyspark-interview-questions-part4.md","sourceDirName":".","slug":"/pyspark-interview-questions-part4","permalink":"/pyspark/pyspark-interview-questions-part4","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"pyspark-intro","permalink":"/pyspark/tags/pyspark-intro"},{"inline":true,"label":"pyspark dataframe basics","permalink":"/pyspark/tags/pyspark-dataframe-basics"},{"inline":true,"label":"pyspark dataframe basics2","permalink":"/pyspark/tags/pyspark-dataframe-basics-2"},{"inline":true,"label":"pyspark filtering","permalink":"/pyspark/tags/pyspark-filtering"},{"inline":true,"label":"pyspark aggregation","permalink":"/pyspark/tags/pyspark-aggregation"},{"inline":true,"label":"pyspark missing","permalink":"/pyspark/tags/pyspark-missing"},{"inline":true,"label":"pyspark dates","permalink":"/pyspark/tags/pyspark-dates"},{"inline":true,"label":"pyspark joins","permalink":"/pyspark/tags/pyspark-joins"},{"inline":true,"label":"pyspark one liners","permalink":"/pyspark/tags/pyspark-one-liners"},{"inline":true,"label":"linear regression model","permalink":"/pyspark/tags/linear-regression-model"},{"inline":true,"label":"linear regression math","permalink":"/pyspark/tags/linear-regression-math"},{"inline":true,"label":"house price linear regression","permalink":"/pyspark/tags/house-price-linear-regression"},{"inline":true,"label":"logistic regression model","permalink":"/pyspark/tags/logistic-regression-model"},{"inline":true,"label":"logistic regression query","permalink":"/pyspark/tags/logistic-regression-query"},{"inline":true,"label":"logistic regression mini project","permalink":"/pyspark/tags/logistic-regression-mini-project"},{"inline":true,"label":"pyspark-interview-questions-part1","permalink":"/pyspark/tags/pyspark-interview-questions-part-1"},{"inline":true,"label":"pyspark-interview-questions-part2","permalink":"/pyspark/tags/pyspark-interview-questions-part-2"},{"inline":true,"label":"pyspark-interview-questions-part3","permalink":"/pyspark/tags/pyspark-interview-questions-part-3"}],"version":"current","sidebarPosition":4,"frontMatter":{"id":"pyspark-interview-questions-part4","title":"Essential PySpark Interview Question & Answer(Explained Through Real-World Stories) \u2013 Part 4","sidebar_label":"PySpark Interview Q&A(Story-Based) - 4","tags":["pyspark-intro","pyspark dataframe basics","pyspark dataframe basics2","pyspark filtering","pyspark aggregation","pyspark missing","pyspark dates","pyspark joins","pyspark one liners","linear regression model","linear regression math","house price linear regression","logistic regression model","logistic regression query","logistic regression mini project","pyspark-interview-questions-part1","pyspark-interview-questions-part2","pyspark-interview-questions-part3"],"keywords":["pyspark-intro","pyspark dataframe basics","pyspark dataframe basics2","pyspark filtering","pyspark aggregation","pyspark missing","pyspark dates","pyspark joins","pyspark one liners","linear regression model","linear regression math","house price linear regression","logistic regression model","logistic regression query","logistic regression mini project","pyspark-interview-questions-part1","pyspark-interview-questions-part2","pyspark-interview-questions-part3"],"sidebar_position":4}}');var i=n(4848),a=n(8453);const o={id:"pyspark-interview-questions-part4",title:"Essential PySpark Interview Question & Answer(Explained Through Real-World Stories) \u2013 Part 4",sidebar_label:"PySpark Interview Q&A(Story-Based) - 4",tags:["pyspark-intro","pyspark dataframe basics","pyspark dataframe basics2","pyspark filtering","pyspark aggregation","pyspark missing","pyspark dates","pyspark joins","pyspark one liners","linear regression model","linear regression math","house price linear regression","logistic regression model","logistic regression query","logistic regression mini project","pyspark-interview-questions-part1","pyspark-interview-questions-part2","pyspark-interview-questions-part3"],keywords:["pyspark-intro","pyspark dataframe basics","pyspark dataframe basics2","pyspark filtering","pyspark aggregation","pyspark missing","pyspark dates","pyspark joins","pyspark one liners","linear regression model","linear regression math","house price linear regression","logistic regression model","logistic regression query","logistic regression mini project","pyspark-interview-questions-part1","pyspark-interview-questions-part2","pyspark-interview-questions-part3"],sidebar_position:4},t=void 0,l={},p=[{value:"<strong>26. How do you use regular expressions in PySpark?</strong>",id:"26-how-do-you-use-regular-expressions-in-pyspark",level:3},{value:"<strong>27. What are Spark SQL temporary views and how do you create one?</strong>",id:"27-what-are-spark-sql-temporary-views-and-how-do-you-create-one",level:3},{value:"<strong>28. How do you add a monotonically increasing ID to a DataFrame?</strong>",id:"28-how-do-you-add-a-monotonically-increasing-id-to-a-dataframe",level:3},{value:"<strong>29. How do you explode nested arrays or structs?</strong>",id:"29-how-do-you-explode-nested-arrays-or-structs",level:3},{value:"<strong>30. What is the difference between repartition and coalesce?</strong>",id:"30-what-is-the-difference-between-repartition-and-coalesce",level:3},{value:"<strong>31. How do you cache a DataFrame and why is it useful?</strong>",id:"31-how-do-you-cache-a-dataframe-and-why-is-it-useful",level:3},{value:"<strong>32. What is the difference between <code>persist</code> and <code>cache</code>?</strong>",id:"32-what-is-the-difference-between-persist-and-cache",level:3},{value:"<strong>33. How do you handle string operations in PySpark?</strong>",id:"33-how-do-you-handle-string-operations-in-pyspark",level:3},{value:"<strong>34. How do you handle date and timestamp operations?</strong>",id:"34-how-do-you-handle-date-and-timestamp-operations",level:3},{value:"<strong>35. Explain <code>dropna</code>, <code>fillna</code>, and <code>replace</code> in PySpark</strong>",id:"35-explain-dropna-fillna-and-replace-in-pyspark",level:3}];function d(e){const r={code:"code",h3:"h3",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(r.h3,{id:"26-how-do-you-use-regular-expressions-in-pyspark",children:(0,i.jsx)(r.strong,{children:"26. How do you use regular expressions in PySpark?"})}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nThink of regular expressions (regex) as a powerful searchlight in a massive library\u2014you can find all books that match a pattern like \u201ctitles starting with A\u201d or \u201cemails ending with .com\u201d instantly."]}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Professional Explanation:"}),"\r\nIn PySpark, regular expressions allow you to perform pattern-based transformations or filtering on string columns. Functions like ",(0,i.jsx)(r.code,{children:"regexp_extract"})," and ",(0,i.jsx)(r.code,{children:"regexp_replace"})," help you extract or modify strings efficiently, making them ideal for cleaning and parsing messy data."]}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Example:"})}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'from pyspark.sql.functions import regexp_extract, regexp_replace\r\n\r\n# Extract domain from email\r\ndf.withColumn("domain", regexp_extract("email", r"@(\\w+\\.\\w+)", 1))\r\n\r\n# Replace non-alphanumeric characters\r\ndf.withColumn("clean_name", regexp_replace("name", r"[^a-zA-Z0-9]", ""))\n'})}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h3,{id:"27-what-are-spark-sql-temporary-views-and-how-do-you-create-one",children:(0,i.jsx)(r.strong,{children:"27. What are Spark SQL temporary views and how do you create one?"})}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nTemporary views are like \u201cguest passes\u201d for a VIP lounge\u2014they exist for your session and let you query data like a table using SQL without permanently saving it."]}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Professional Explanation:"}),"\r\nA temporary view allows you to expose a DataFrame as a SQL table within your Spark session. It\u2019s useful for running SQL queries on DataFrames and combining SQL and PySpark operations seamlessly."]}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Example:"})}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'df.createOrReplaceTempView("employee_view")\r\nspark.sql("SELECT department, AVG(salary) FROM employee_view GROUP BY department").show()\n'})}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h3,{id:"28-how-do-you-add-a-monotonically-increasing-id-to-a-dataframe",children:(0,i.jsx)(r.strong,{children:"28. How do you add a monotonically increasing ID to a DataFrame?"})}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nAdding a monotonically increasing ID is like assigning ticket numbers at a concert\u2014you give every row a unique, ever-increasing number without duplicates."]}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Professional Explanation:"}),"\r\nUse ",(0,i.jsx)(r.code,{children:"monotonically_increasing_id()"})," to generate unique, increasing IDs for DataFrame rows. This is especially useful for indexing or joining data when no natural ID exists."]}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Example:"})}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'from pyspark.sql.functions import monotonically_increasing_id\r\n\r\ndf.withColumn("id", monotonically_increasing_id()).show()\n'})}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h3,{id:"29-how-do-you-explode-nested-arrays-or-structs",children:(0,i.jsx)(r.strong,{children:"29. How do you explode nested arrays or structs?"})}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nImagine a Russian nesting doll\u2014",(0,i.jsx)(r.code,{children:"explode()"})," opens up the doll so you can see each piece individually. Similarly, nested arrays or structs are flattened so every element becomes its own row."]}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Professional Explanation:"}),"\r\n",(0,i.jsx)(r.code,{children:"explode()"})," converts array or map elements into separate rows, making nested data accessible for further transformations or analysis."]}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Example:"})}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'from pyspark.sql.functions import explode\r\n\r\ndf.select("name", explode("skills").alias("skill")).show()\n'})}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h3,{id:"30-what-is-the-difference-between-repartition-and-coalesce",children:(0,i.jsx)(r.strong,{children:"30. What is the difference between repartition and coalesce?"})}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nRepartitioning is like completely reorganizing a library\u2019s shelves, spreading books evenly. Coalesce is like combining a few shelves to reduce clutter, keeping most of the original order intact."]}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Professional Explanation:"})}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.code,{children:"repartition(n)"}),": Creates ",(0,i.jsx)(r.strong,{children:"n partitions"})," and redistributes data across nodes (full shuffle)."]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.code,{children:"coalesce(n)"}),": Reduces the number of partitions ",(0,i.jsx)(r.strong,{children:"without full shuffle"}),", optimized for smaller reductions.\r\nUse ",(0,i.jsx)(r.code,{children:"repartition"})," for load balancing, ",(0,i.jsx)(r.code,{children:"coalesce"})," for efficiency when reducing partitions."]}),"\n"]}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Example:"})}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"df.repartition(10)  # Shuffle to 10 partitions\r\ndf.coalesce(2)      # Reduce partitions to 2\n"})}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h3,{id:"31-how-do-you-cache-a-dataframe-and-why-is-it-useful",children:(0,i.jsx)(r.strong,{children:"31. How do you cache a DataFrame and why is it useful?"})}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nCaching is like keeping frequently used documents on your desk instead of fetching them from the archive each time\u2014it speeds up repeated operations."]}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Professional Explanation:"}),"\r\nCaching a DataFrame stores it in memory, reducing recomputation of expensive transformations. It\u2019s useful for iterative algorithms, repeated queries, or machine learning pipelines."]}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Example:"})}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"df.cache()\r\ndf.show()\n"})}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h3,{id:"32-what-is-the-difference-between-persist-and-cache",children:(0,i.jsxs)(r.strong,{children:["32. What is the difference between ",(0,i.jsx)(r.code,{children:"persist"})," and ",(0,i.jsx)(r.code,{children:"cache"}),"?"]})}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nCaching is like putting a file on your desk (memory), while persisting is like storing it in a chosen medium\u2014desk, shelf, or cloud (memory, disk, or both)."]}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Professional Explanation:"})}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.code,{children:"cache()"}),": Stores DataFrame in memory only (",(0,i.jsx)(r.code,{children:"MEMORY_AND_DISK"})," by default in newer Spark versions)."]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.code,{children:"persist(storageLevel)"}),": Allows control over storage level\u2014memory, disk, or both. Use ",(0,i.jsx)(r.code,{children:"persist"})," when caching in memory alone may not be enough."]}),"\n"]}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Example:"})}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"from pyspark import StorageLevel\r\n\r\ndf.persist(StorageLevel.MEMORY_AND_DISK)\r\ndf.cache()  # Equivalent to persist() with default MEMORY_AND_DISK\n"})}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h3,{id:"33-how-do-you-handle-string-operations-in-pyspark",children:(0,i.jsx)(r.strong,{children:"33. How do you handle string operations in PySpark?"})}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nHandling strings in PySpark is like using a Swiss Army knife for text\u2014you can slice, search, replace, convert, and format with precision."]}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Professional Explanation:"}),"\r\nPySpark provides multiple functions in ",(0,i.jsx)(r.code,{children:"pyspark.sql.functions"})," to manipulate strings: ",(0,i.jsx)(r.code,{children:"substring"}),", ",(0,i.jsx)(r.code,{children:"concat"}),", ",(0,i.jsx)(r.code,{children:"length"}),", ",(0,i.jsx)(r.code,{children:"upper/lower"}),", ",(0,i.jsx)(r.code,{children:"trim"}),", ",(0,i.jsx)(r.code,{children:"regexp_replace"}),", etc. These functions are vectorized and optimized for distributed processing."]}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Example:"})}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'from pyspark.sql.functions import upper, concat_ws, length, trim\r\n\r\ndf.withColumn("upper_name", upper("name")) \\\r\n  .withColumn("name_length", length("name")) \\\r\n  .withColumn("full_info", concat_ws("-", "name", "department"))\n'})}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h3,{id:"34-how-do-you-handle-date-and-timestamp-operations",children:(0,i.jsx)(r.strong,{children:"34. How do you handle date and timestamp operations?"})}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nDates and timestamps in PySpark are like scheduling apps\u2014you can calculate durations, find weekdays, or extract months to make data actionable."]}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Professional Explanation:"}),"\r\nPySpark provides date and timestamp functions like ",(0,i.jsx)(r.code,{children:"current_date"}),", ",(0,i.jsx)(r.code,{children:"date_add"}),", ",(0,i.jsx)(r.code,{children:"year"}),", ",(0,i.jsx)(r.code,{children:"month"}),", ",(0,i.jsx)(r.code,{children:"datediff"}),", ",(0,i.jsx)(r.code,{children:"to_date"}),", and ",(0,i.jsx)(r.code,{children:"unix_timestamp"}),". These allow transformations, filtering, and time-based analysis on distributed data efficiently."]}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Example:"})}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'from pyspark.sql.functions import current_date, datediff, to_date, year\r\n\r\ndf.withColumn("today", current_date()) \\\r\n  .withColumn("days_diff", datediff(current_date(), "join_date")) \\\r\n  .withColumn("join_year", year("join_date"))\n'})}),"\n",(0,i.jsx)(r.hr,{}),"\n",(0,i.jsx)(r.h3,{id:"35-explain-dropna-fillna-and-replace-in-pyspark",children:(0,i.jsxs)(r.strong,{children:["35. Explain ",(0,i.jsx)(r.code,{children:"dropna"}),", ",(0,i.jsx)(r.code,{children:"fillna"}),", and ",(0,i.jsx)(r.code,{children:"replace"})," in PySpark"]})}),"\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nThink of data cleaning like tidying your room:"]}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.code,{children:"dropna"})," \u2192 throw away missing items"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.code,{children:"fillna"})," \u2192 fill empty spaces with default values"]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.code,{children:"replace"})," \u2192 swap incorrect items with the right ones"]}),"\n"]}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Professional Explanation:"})}),"\n",(0,i.jsxs)(r.ul,{children:["\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.code,{children:"dropna()"}),": Removes rows containing nulls."]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.code,{children:"fillna()"}),": Replaces nulls with a specified value."]}),"\n",(0,i.jsxs)(r.li,{children:[(0,i.jsx)(r.code,{children:"replace()"}),": Replaces specific values with new values.\r\nThese are crucial for preparing data for analysis or machine learning."]}),"\n"]}),"\n",(0,i.jsx)(r.p,{children:(0,i.jsx)(r.strong,{children:"Example:"})}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'df.dropna(subset=["salary"]).show()           # Drop rows with null salary\r\ndf.fillna({"salary": 0, "department": "NA"}).show()\r\ndf.replace({"IT": "Information Technology"}).show()\n'})})]})}function c(e={}){const{wrapper:r}={...(0,a.R)(),...e.components};return r?(0,i.jsx)(r,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,r,n)=>{n.d(r,{R:()=>o,x:()=>t});var s=n(6540);const i={},a=s.createContext(i);function o(e){const r=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function t(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(a.Provider,{value:r},e.children)}}}]);