"use strict";(self.webpackChunkdatacraft_school=self.webpackChunkdatacraft_school||[]).push([[5406],{3253:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>p,contentTitle:()=>t,default:()=>u,frontMatter:()=>r,metadata:()=>i,toc:()=>o});const i=JSON.parse('{"id":"pyspark-missing","title":"Handling Missing Data in PySpark DataFrames (Complete Guide)","description":"Learn all techniques for handling missing or null data in PySpark DataFrames including dropping nulls, filling values, conditional replacement, and computing statistics.","source":"@site/docs-pyspark/pyspark-missing.md","sourceDirName":".","slug":"/pyspark/missing-data","permalink":"/pyspark/pyspark/missing-data","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"PySpark","permalink":"/pyspark/tags/py-spark"},{"inline":true,"label":"Apache Spark","permalink":"/pyspark/tags/apache-spark"},{"inline":true,"label":"Big Data","permalink":"/pyspark/tags/big-data"},{"inline":true,"label":"Spark Basics","permalink":"/pyspark/tags/spark-basics"},{"inline":true,"label":"Cluster Computing","permalink":"/pyspark/tags/cluster-computing"},{"inline":true,"label":"Spark Architecture","permalink":"/pyspark/tags/spark-architecture"},{"inline":true,"label":"Driver Program","permalink":"/pyspark/tags/driver-program"},{"inline":true,"label":"Executors","permalink":"/pyspark/tags/executors"},{"inline":true,"label":"Cluster Manager","permalink":"/pyspark/tags/cluster-manager"},{"inline":true,"label":"SparkSession","permalink":"/pyspark/tags/spark-session"},{"inline":true,"label":"SparkContext","permalink":"/pyspark/tags/spark-context"},{"inline":true,"label":"RDD","permalink":"/pyspark/tags/rdd"},{"inline":true,"label":"RDD Transformations","permalink":"/pyspark/tags/rdd-transformations"},{"inline":true,"label":"RDD Actions","permalink":"/pyspark/tags/rdd-actions"},{"inline":true,"label":"Key-Value RDD","permalink":"/pyspark/tags/key-value-rdd"},{"inline":true,"label":"RDD Caching","permalink":"/pyspark/tags/rdd-caching"},{"inline":true,"label":"DataFrame","permalink":"/pyspark/tags/data-frame"},{"inline":true,"label":"DataFrame API","permalink":"/pyspark/tags/data-frame-api"},{"inline":true,"label":"Column Operations","permalink":"/pyspark/tags/column-operations"},{"inline":true,"label":"DataFrame Joins","permalink":"/pyspark/tags/data-frame-joins"},{"inline":true,"label":"Aggregations","permalink":"/pyspark/tags/aggregations"},{"inline":true,"label":"GroupBy","permalink":"/pyspark/tags/group-by"},{"inline":true,"label":"Window Functions","permalink":"/pyspark/tags/window-functions"},{"inline":true,"label":"Missing Data Handling","permalink":"/pyspark/tags/missing-data-handling"},{"inline":true,"label":"Spark SQL","permalink":"/pyspark/tags/spark-sql"},{"inline":true,"label":"Temp Views","permalink":"/pyspark/tags/temp-views"},{"inline":true,"label":"Spark SQL Functions","permalink":"/pyspark/tags/spark-sql-functions"},{"inline":true,"label":"UDF","permalink":"/pyspark/tags/udf"},{"inline":true,"label":"UDAF","permalink":"/pyspark/tags/udaf"},{"inline":true,"label":"Explode","permalink":"/pyspark/tags/explode"},{"inline":true,"label":"Arrays","permalink":"/pyspark/tags/arrays"},{"inline":true,"label":"StructType","permalink":"/pyspark/tags/struct-type"},{"inline":true,"label":"Complex Data Types","permalink":"/pyspark/tags/complex-data-types"},{"inline":true,"label":"Pivot","permalink":"/pyspark/tags/pivot"},{"inline":true,"label":"Unpivot","permalink":"/pyspark/tags/unpivot"},{"inline":true,"label":"Join Optimization","permalink":"/pyspark/tags/join-optimization"},{"inline":true,"label":"Sorting","permalink":"/pyspark/tags/sorting"},{"inline":true,"label":"Sampling","permalink":"/pyspark/tags/sampling"},{"inline":true,"label":"Partitioning","permalink":"/pyspark/tags/partitioning"},{"inline":true,"label":"Bucketing","permalink":"/pyspark/tags/bucketing"},{"inline":true,"label":"CSV","permalink":"/pyspark/tags/csv"},{"inline":true,"label":"JSON","permalink":"/pyspark/tags/json"},{"inline":true,"label":"Parquet","permalink":"/pyspark/tags/parquet"},{"inline":true,"label":"Avro","permalink":"/pyspark/tags/avro"},{"inline":true,"label":"Delta Lake","permalink":"/pyspark/tags/delta-lake"},{"inline":true,"label":"Performance Tuning","permalink":"/pyspark/tags/performance-tuning"},{"inline":true,"label":"Shuffle","permalink":"/pyspark/tags/shuffle"},{"inline":true,"label":"Narrow vs Wide Transformations","permalink":"/pyspark/tags/narrow-vs-wide-transformations"},{"inline":true,"label":"Spark UI","permalink":"/pyspark/tags/spark-ui"},{"inline":true,"label":"Catalyst Optimizer","permalink":"/pyspark/tags/catalyst-optimizer"},{"inline":true,"label":"Tungsten","permalink":"/pyspark/tags/tungsten"},{"inline":true,"label":"Repartition","permalink":"/pyspark/tags/repartition"},{"inline":true,"label":"Coalesce","permalink":"/pyspark/tags/coalesce"},{"inline":true,"label":"Broadcast Join","permalink":"/pyspark/tags/broadcast-join"},{"inline":true,"label":"Memory Management","permalink":"/pyspark/tags/memory-management"},{"inline":true,"label":"Structured Streaming","permalink":"/pyspark/tags/structured-streaming"},{"inline":true,"label":"Real-Time Data","permalink":"/pyspark/tags/real-time-data"},{"inline":true,"label":"Kafka","permalink":"/pyspark/tags/kafka"},{"inline":true,"label":"Streaming Sinks","permalink":"/pyspark/tags/streaming-sinks"},{"inline":true,"label":"Watermarking","permalink":"/pyspark/tags/watermarking"},{"inline":true,"label":"Checkpoints","permalink":"/pyspark/tags/checkpoints"},{"inline":true,"label":"MLlib","permalink":"/pyspark/tags/m-llib"},{"inline":true,"label":"Machine Learning","permalink":"/pyspark/tags/machine-learning"},{"inline":true,"label":"Regression","permalink":"/pyspark/tags/regression"},{"inline":true,"label":"Classification","permalink":"/pyspark/tags/classification"},{"inline":true,"label":"Clustering","permalink":"/pyspark/tags/clustering"},{"inline":true,"label":"Recommendation Systems","permalink":"/pyspark/tags/recommendation-systems"},{"inline":true,"label":"Feature Engineering","permalink":"/pyspark/tags/feature-engineering"},{"inline":true,"label":"Snowflake","permalink":"/pyspark/tags/snowflake"},{"inline":true,"label":"Hive","permalink":"/pyspark/tags/hive"},{"inline":true,"label":"Databricks","permalink":"/pyspark/tags/databricks"},{"inline":true,"label":"AWS EMR","permalink":"/pyspark/tags/aws-emr"},{"inline":true,"label":"GCP Dataproc","permalink":"/pyspark/tags/gcp-dataproc"},{"inline":true,"label":"Azure Synapse","permalink":"/pyspark/tags/azure-synapse"},{"inline":true,"label":"ETL","permalink":"/pyspark/tags/etl"},{"inline":true,"label":"Data Pipelines","permalink":"/pyspark/tags/data-pipelines"},{"inline":true,"label":"Data Processing","permalink":"/pyspark/tags/data-processing"},{"inline":true,"label":"Workflow Automation","permalink":"/pyspark/tags/workflow-automation"},{"inline":true,"label":"Data Quality","permalink":"/pyspark/tags/data-quality"},{"inline":true,"label":"Semi-Structured Data","permalink":"/pyspark/tags/semi-structured-data"},{"inline":true,"label":"Production Pipelines","permalink":"/pyspark/tags/production-pipelines"},{"inline":true,"label":"Use Cases","permalink":"/pyspark/tags/use-cases"}],"version":"current","frontMatter":{"id":"pyspark-missing","title":"Handling Missing Data in PySpark DataFrames (Complete Guide)","sidebar_label":"Handling Missing Data","slug":"/pyspark/missing-data","description":"Learn all techniques for handling missing or null data in PySpark DataFrames including dropping nulls, filling values, conditional replacement, and computing statistics.","keywords":["pyspark missing data","pyspark null handling","pyspark na functions","spark drop null","pyspark fill null","big data cleaning","spark dataframe cleaning"],"og:title":"Handling Missing Data in PySpark \u2014 Complete Tutorial","og:description":"A full guide explaining how to drop, fill, and replace null values in PySpark using df.na.drop, df.na.fill, subset operations, thresholds, and mean imputation.","tags":["PySpark","Apache Spark","Big Data","Spark Basics","Cluster Computing","Spark Architecture","Driver Program","Executors","Cluster Manager","SparkSession","SparkContext","RDD","RDD Transformations","RDD Actions","Key-Value RDD","RDD Caching","DataFrame","DataFrame API","Column Operations","DataFrame Joins","Aggregations","GroupBy","Window Functions","Missing Data Handling","Spark SQL","Temp Views","Spark SQL Functions","UDF","UDAF","Explode","Arrays","StructType","Complex Data Types","Pivot","Unpivot","Join Optimization","Sorting","Sampling","Partitioning","Bucketing","CSV","JSON","Parquet","Avro","Delta Lake","Performance Tuning","Shuffle","Narrow vs Wide Transformations","Spark UI","Catalyst Optimizer","Tungsten","Repartition","Coalesce","Broadcast Join","Memory Management","Structured Streaming","Real-Time Data","Kafka","Streaming Sinks","Watermarking","Checkpoints","MLlib","Machine Learning","Regression","Classification","Clustering","Recommendation Systems","Feature Engineering","Snowflake","Hive","Databricks","AWS EMR","GCP Dataproc","Azure Synapse","ETL","Data Pipelines","Data Processing","Workflow Automation","Data Quality","Semi-Structured Data","Production Pipelines","Use Cases"]},"sidebar":"tutorialSidebar","previous":{"title":"Window Functions","permalink":"/pyspark/df-window-functions"},"next":{"title":"Dates & Time","permalink":"/pyspark/pyspark/dates-and-timestamps"}}');var l=a(4848),s=a(8453);const r={id:"pyspark-missing",title:"Handling Missing Data in PySpark DataFrames (Complete Guide)",sidebar_label:"Handling Missing Data",slug:"/pyspark/missing-data",description:"Learn all techniques for handling missing or null data in PySpark DataFrames including dropping nulls, filling values, conditional replacement, and computing statistics.",keywords:["pyspark missing data","pyspark null handling","pyspark na functions","spark drop null","pyspark fill null","big data cleaning","spark dataframe cleaning"],"og:title":"Handling Missing Data in PySpark \u2014 Complete Tutorial","og:description":"A full guide explaining how to drop, fill, and replace null values in PySpark using df.na.drop, df.na.fill, subset operations, thresholds, and mean imputation.",tags:["PySpark","Apache Spark","Big Data","Spark Basics","Cluster Computing","Spark Architecture","Driver Program","Executors","Cluster Manager","SparkSession","SparkContext","RDD","RDD Transformations","RDD Actions","Key-Value RDD","RDD Caching","DataFrame","DataFrame API","Column Operations","DataFrame Joins","Aggregations","GroupBy","Window Functions","Missing Data Handling","Spark SQL","Temp Views","Spark SQL Functions","UDF","UDAF","Explode","Arrays","StructType","Complex Data Types","Pivot","Unpivot","Join Optimization","Sorting","Sampling","Partitioning","Bucketing","CSV","JSON","Parquet","Avro","Delta Lake","Performance Tuning","Shuffle","Narrow vs Wide Transformations","Spark UI","Catalyst Optimizer","Tungsten","Repartition","Coalesce","Broadcast Join","Memory Management","Structured Streaming","Real-Time Data","Kafka","Streaming Sinks","Watermarking","Checkpoints","MLlib","Machine Learning","Regression","Classification","Clustering","Recommendation Systems","Feature Engineering","Snowflake","Hive","Databricks","AWS EMR","GCP Dataproc","Azure Synapse","ETL","Data Pipelines","Data Processing","Workflow Automation","Data Quality","Semi-Structured Data","Production Pipelines","Use Cases"]},t="Handling Missing Data in PySpark DataFrames",p={},o=[{value:"\u2714\ufe0f What This Does",id:"\ufe0f-what-this-does",level:3},{value:"\u2714\ufe0f What This Does",id:"\ufe0f-what-this-does-1",level:3},{value:"\u2714\ufe0f What This Does",id:"\ufe0f-what-this-does-2",level:3},{value:"\u2714\ufe0f What This Does",id:"\ufe0f-what-this-does-3",level:3},{value:"\u2714\ufe0f What This Does",id:"\ufe0f-what-this-does-4",level:3},{value:"\u2714\ufe0f What This Does",id:"\ufe0f-what-this-does-5",level:3},{value:"\u2714\ufe0f What This Does",id:"\ufe0f-what-this-does-6",level:3},{value:"\u2714\ufe0f What This Does",id:"\ufe0f-what-this-does-7",level:3}];function d(n){const e={br:"br",code:"code",em:"em",h1:"h1",h3:"h3",header:"header",hr:"hr",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,s.R)(),...n.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(e.header,{children:(0,l.jsx)(e.h1,{id:"handling-missing-data-in-pyspark-dataframes",children:"Handling Missing Data in PySpark DataFrames"})}),"\n",(0,l.jsxs)(e.p,{children:["Handling missing values is an essential part of data cleaning, ETL pipelines, machine learning prep, and large-scale analytics.",(0,l.jsx)(e.br,{}),"\n","PySpark provides powerful ",(0,l.jsx)(e.code,{children:"DataFrame.na"})," functions to detect, drop, fill, and impute missing values."]}),"\n",(0,l.jsxs)(e.p,{children:["Below is a complete guide using a sample ",(0,l.jsx)(e.strong,{children:"Royalty & Sales"})," dataset."]}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h1,{id:"load-dataset",children:"Load Dataset"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-python",children:"from pyspark.sql import SparkSession\r\n\r\nspark = SparkSession.builder.appName('missing-data').getOrCreate()\r\n\r\ndf = spark.read.csv('/path/to/royalty.csv', header=True, inferSchema=True)\n"})}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h1,{id:"show-raw-data",children:"Show Raw Data"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-python",children:"df.show()\n"})}),"\n",(0,l.jsx)(e.p,{children:"Output:"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-sql",children:"+------------+------------------+------+--------+\r\n| Author     | Book             | Sales| Royalty|\r\n+------------+------------------+------+--------+\r\n| John Smith | Deep Learning    | 5000 | 800    |\r\n| Jane Doe   | AI Ethics        | null | 600    |\r\n| null       | Quantum Computing| 3500 | 550    |\r\n| Alex Lee   | Big Data         | 4500 | null   |\r\n| Anna Ray   | null             | null | null   |\r\n+------------+------------------+------+--------+\n"})}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h1,{id:"1-drop-rows-with-any-null-values",children:"1. Drop Rows With Any Null Values"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-python",children:"df.na.drop().show()\n"})}),"\n",(0,l.jsx)(e.p,{children:"Output:"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-sql",children:"+-----------+--------------+-----+--------+\r\n| Author    | Book         |Sales|Royalty |\r\n+-----------+--------------+-----+--------+\r\n| John Smith|Deep Learning | 5000|   800  |\r\n+-----------+--------------+-----+--------+\n"})}),"\n",(0,l.jsx)(e.h3,{id:"\ufe0f-what-this-does",children:"\u2714\ufe0f What This Does"}),"\n",(0,l.jsxs)(e.p,{children:["Removes any row that contains ",(0,l.jsx)(e.strong,{children:"at least one"})," null value."]}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h1,{id:"2-keep-rows-only-if--2-non-null-values",children:"2. Keep Rows Only if \u2265 2 Non-Null Values"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-python",children:"df.na.drop(thresh=2).show()\n"})}),"\n",(0,l.jsx)(e.p,{children:"Output:"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-sql",children:"+-----------+------------------+-----+--------+\r\n| Author    | Book             |Sales|Royalty |\r\n+-----------+------------------+-----+--------+\r\n| John Smith|Deep Learning     |5000 | 800    |\r\n| Jane Doe  |AI Ethics         |null | 600    |\r\n| null      |Quantum Computing |3500 | 550    |\r\n| Alex Lee  |Big Data          |4500 | null   |\r\n+-----------+------------------+-----+--------+\n"})}),"\n",(0,l.jsx)(e.h3,{id:"\ufe0f-what-this-does-1",children:"\u2714\ufe0f What This Does"}),"\n",(0,l.jsxs)(e.p,{children:["Keeps rows that have ",(0,l.jsx)(e.strong,{children:"at least 2 non-null values"}),"."]}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h1,{id:"3-drop-row-only-if-every-column-is-null",children:"3. Drop Row Only If Every Column Is Null"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-python",children:"df.na.drop(how='all').show()\n"})}),"\n",(0,l.jsx)(e.p,{children:"Output:"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-sql",children:"+-----------+------------------+-----+--------+\r\n| Author    | Book             |Sales|Royalty |\r\n+-----------+------------------+-----+--------+\r\n| John Smith|Deep Learning     |5000 |800     |\r\n| Jane Doe  |AI Ethics         |null |600     |\r\n| null      |Quantum Computing |3500 |550     |\r\n| Alex Lee  |Big Data          |4500 |null    |\r\n| Anna Ray  |null              |null |null    |\r\n+-----------+------------------+-----+--------+\n"})}),"\n",(0,l.jsx)(e.h3,{id:"\ufe0f-what-this-does-2",children:"\u2714\ufe0f What This Does"}),"\n",(0,l.jsxs)(e.p,{children:["Removes rows ",(0,l.jsx)(e.strong,{children:"only if every column"})," has null \u2192 none in this dataset."]}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h1,{id:"4-drop-rows-if-any-column-is-null",children:"4. Drop Rows If Any Column Is Null"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-python",children:"df.na.drop(how='any').show()\n"})}),"\n",(0,l.jsx)(e.p,{children:"Output is the same as example (only one row remains)."}),"\n",(0,l.jsxs)(e.p,{children:["\u2714\ufe0f Equivalent to ",(0,l.jsx)(e.code,{children:"df.na.drop()"}),"."]}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h1,{id:"5-drop-rows-where-a-specific-column-sales-is-null",children:"5. Drop Rows Where a Specific Column (Sales) Is Null"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-python",children:"df.na.drop(subset=['Sales']).show()\n"})}),"\n",(0,l.jsx)(e.p,{children:"Output:"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-sql",children:"+-----------+------------------+-----+--------+\r\n| Author    | Book             |Sales|Royalty |\r\n+-----------+------------------+-----+--------+\r\n| John Smith|Deep Learning     |5000 | 800    |\r\n| null      |Quantum Computing |3500 | 550    |\r\n| Alex Lee  |Big Data          |4500 | null   |\r\n+-----------+------------------+-----+--------+\n"})}),"\n",(0,l.jsx)(e.h3,{id:"\ufe0f-what-this-does-3",children:"\u2714\ufe0f What This Does"}),"\n",(0,l.jsxs)(e.p,{children:["Keeps only rows where ",(0,l.jsx)(e.strong,{children:"Sales is not null"}),"."]}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h1,{id:"6-fill-all-nulls-with-0",children:"6. Fill All Nulls With 0"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-python",children:"df.na.fill(0).show()\n"})}),"\n",(0,l.jsx)(e.p,{children:"Output:"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-sql",children:"+-----------+------------------+-----+--------+\r\n| Author    | Book             |Sales|Royalty |\r\n+-----------+------------------+-----+--------+\r\n| John Smith|Deep Learning     |5000 | 800    |\r\n| Jane Doe  |AI Ethics         |  0  | 600    |\r\n| null      |Quantum Computing |3500 | 550    |\r\n| Alex Lee  |Big Data          |4500 | 0      |\r\n| Anna Ray  |null              |  0  | 0      |\r\n+-----------+------------------+-----+--------+\n"})}),"\n",(0,l.jsx)(e.h3,{id:"\ufe0f-what-this-does-4",children:"\u2714\ufe0f What This Does"}),"\n",(0,l.jsxs)(e.p,{children:["Replaces ",(0,l.jsx)(e.strong,{children:"all nulls"})," (numbers, strings) with ",(0,l.jsx)(e.code,{children:"0"}),"."]}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsxs)(e.h1,{id:"7-fill-all-nulls-with-unknown",children:["7. Fill All Nulls With ",(0,l.jsx)(e.code,{children:'"unknown"'})]}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-python",children:"df.na.fill('unknown').show()\n"})}),"\n",(0,l.jsx)(e.p,{children:"Output:"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-sql",children:"+-----------+------------------+--------+--------+\r\n| Author    | Book             | Sales  |Royalty |\r\n+-----------+------------------+--------+--------+\r\n| John Smith| Deep Learning    | 5000   | 800    |\r\n| Jane Doe  | AI Ethics        | unknown| 600    |\r\n| unknown   | Quantum Computing| 3500   | 550    |\r\n| Alex Lee  | Big Data         | 4500   |unknown |\r\n| Anna Ray  | unknown          |unknown |unknown |\r\n+-----------+------------------+--------+--------+\n"})}),"\n",(0,l.jsx)(e.h3,{id:"\ufe0f-what-this-does-5",children:"\u2714\ufe0f What This Does"}),"\n",(0,l.jsxs)(e.p,{children:["Fills every missing cell with ",(0,l.jsx)(e.code,{children:'"unknown"'})," regardless of data type."]}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h1,{id:"8-fill-nulls-only-in-the-author-column",children:"8. Fill Nulls Only in the Author Column"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-python",children:"df.na.fill('unknown', subset=['Author']).show()\n"})}),"\n",(0,l.jsx)(e.p,{children:"Output:"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-sql",children:"+-----------+------------------+------+--------+\r\n| Author    | Book             |Sales |Royalty |\r\n+-----------+------------------+------+--------+\r\n| John Smith|Deep Learning     | 5000 |  800   |\r\n| Jane Doe  |AI Ethics         | null |  600   |\r\n| unknown   |Quantum Computing | 3500 |  550   |\r\n| Alex Lee  |Big Data          | 4500 | null   |\r\n| Anna Ray  |null              | null | null   |\r\n+-----------+------------------+------+--------+\n"})}),"\n",(0,l.jsx)(e.h3,{id:"\ufe0f-what-this-does-6",children:"\u2714\ufe0f What This Does"}),"\n",(0,l.jsx)(e.p,{children:"Only fills missing author names."}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h1,{id:"9-fill-missing-sales-values-with-the-mean",children:"9. Fill Missing Sales Values With the Mean"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-python",children:"from pyspark.sql.functions import mean\r\n\r\nmean_val = df.select(mean(df['Sales'])).collect()\r\nmean_sales = mean_val[0][0]\r\n\r\ndf.na.fill(mean_sales, subset=['Sales']).show()\n"})}),"\n",(0,l.jsx)(e.p,{children:"Output:"}),"\n",(0,l.jsx)(e.pre,{children:(0,l.jsx)(e.code,{className:"language-sql",children:"+-----------+------------------+------+--------+\r\n| Author    | Book             |Sales |Royalty |\r\n+-----------+------------------+------+--------+\r\n| John Smith|Deep Learning     | 5000 | 800    |\r\n| Jane Doe  |AI Ethics         | 4250 | 600    |\r\n| null      |Quantum Computing | 3500 | 550    |\r\n| Alex Lee  |Big Data          | 4500 | null   |\r\n| Anna Ray  |null              | 4250 | null   |\r\n+-----------+------------------+------+--------+\n"})}),"\n",(0,l.jsx)(e.h3,{id:"\ufe0f-what-this-does-7",children:"\u2714\ufe0f What This Does"}),"\n",(0,l.jsxs)(e.p,{children:["Replaces missing Sales values with the calculated ",(0,l.jsx)(e.strong,{children:"average = 4250"}),"."]}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.h1,{id:"-1-minute-summary--handling-missing-data-in-pyspark",children:"\ud83d\udfe6 1-Minute Summary \u2014 Handling Missing Data in PySpark"}),"\n",(0,l.jsxs)(e.table,{children:[(0,l.jsx)(e.thead,{children:(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.th,{children:"Code Example"}),(0,l.jsx)(e.th,{children:"Meaning / Use Case"})]})}),(0,l.jsxs)(e.tbody,{children:[(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:(0,l.jsx)(e.code,{children:"df.na.drop()"})}),(0,l.jsxs)(e.td,{children:["Drop rows with ",(0,l.jsx)(e.em,{children:"any"})," null values"]})]}),(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:(0,l.jsx)(e.code,{children:"df.na.drop(thresh=2)"})}),(0,l.jsx)(e.td,{children:"Keep rows with \u2265 2 non-null values"})]}),(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:(0,l.jsx)(e.code,{children:"df.na.drop(how='all')"})}),(0,l.jsxs)(e.td,{children:["Drop only rows where ",(0,l.jsx)(e.em,{children:"all"})," columns are null"]})]}),(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:(0,l.jsx)(e.code,{children:"df.na.drop(subset=['Sales'])"})}),(0,l.jsx)(e.td,{children:"Drop rows with nulls in specific column(s)"})]}),(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:(0,l.jsx)(e.code,{children:"df.na.fill(0)"})}),(0,l.jsxs)(e.td,{children:["Replace all nulls with ",(0,l.jsx)(e.code,{children:"0"})]})]}),(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:(0,l.jsx)(e.code,{children:"df.na.fill('unknown')"})}),(0,l.jsxs)(e.td,{children:["Replace all nulls with ",(0,l.jsx)(e.code,{children:'"unknown"'})]})]}),(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:(0,l.jsx)(e.code,{children:"df.na.fill('unknown', subset=['Author'])"})}),(0,l.jsxs)(e.td,{children:["Replace nulls only in the ",(0,l.jsx)(e.code,{children:"Author"})," column"]})]}),(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:(0,l.jsx)(e.code,{children:"mean(df['Sales'])"})}),(0,l.jsx)(e.td,{children:"Compute the mean for imputation"})]}),(0,l.jsxs)(e.tr,{children:[(0,l.jsx)(e.td,{children:(0,l.jsx)(e.code,{children:"df.na.fill(mean_sales, subset=['Sales'])"})}),(0,l.jsx)(e.td,{children:"Fill Sales nulls using mean-based imputation"})]})]})]}),"\n",(0,l.jsx)(e.hr,{}),"\n",(0,l.jsx)(e.p,{children:"Next, we\u2019ll cover Working with Dates and Timestamps in PySpark DataFrames (Full Guide)"})]})}function u(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,l.jsx)(e,{...n,children:(0,l.jsx)(d,{...n})}):d(n)}},8453:(n,e,a)=>{a.d(e,{R:()=>r,x:()=>t});var i=a(6540);const l={},s=i.createContext(l);function r(n){const e=i.useContext(s);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(l):n.components||l:r(n.components),i.createElement(s.Provider,{value:e},n.children)}}}]);