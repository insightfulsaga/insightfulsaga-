"use strict";(self.webpackChunkdatacraft_school=self.webpackChunkdatacraft_school||[]).push([[8538],{1790:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>i,metadata:()=>a,toc:()=>o});const a=JSON.parse('{"id":"delta-lake-core","title":"Delta Lake \u2013 Foundation of Reliable Data Pipelines","description":"Story Driven","source":"@site/docs-databricks/delta-lake-core.md","sourceDirName":".","slug":"/delta-lake-core","permalink":"/databricks/delta-lake-core","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"delta-lake-core","title":"Delta Lake \u2013 Foundation of Reliable Data Pipelines","sidebar_label":"Delta Lake Overview"}}');var s=r(4848),t=r(8453);const i={id:"delta-lake-core",title:"Delta Lake \u2013 Foundation of Reliable Data Pipelines",sidebar_label:"Delta Lake Overview"},l=void 0,d={},o=[{value:"Code : Real-World Scenario",id:"code--real-world-scenario",level:2},{value:"Upserts",id:"upserts",level:2},{value:"Schema Evolution \u2013 Real-World Scenario",id:"schema-evolution--real-world-scenario",level:2},{value:"Time Travel",id:"time-travel",level:2},{value:"Summary table",id:"summary-table",level:2},{value:"\ud83d\udd11 1-Minute Summary:  Delta Lake \u2013 Foundation of Reliable Data Pipelines",id:"-1-minute-summary--delta-lake--foundation-of-reliable-data-pipelines",level:2}];function c(e){const n={br:"br",code:"code",h2:"h2",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Story Driven"})}),"\n",(0,s.jsx)(n.p,{children:"A company had a huge warehouse full of documents (data). People were dropping files anywhere, updating them without telling anyone, and sometimes files went missing. It was chaotic."}),"\n",(0,s.jsxs)(n.p,{children:["Then they hired Delta Lake, a smart warehouse manager. Here\u2019s what Delta Lake did:-",(0,s.jsx)(n.br,{}),"\n","\u2981\tLogged every change with timestamps (so you could undo mistakes).",(0,s.jsx)(n.br,{}),"\n","\u2981\tChecked the format of all incoming documents to avoid corruption.",(0,s.jsx)(n.br,{}),"\n","\u2981\tEnabled people to edit files safely without overwriting others\u2019 work.",(0,s.jsx)(n.br,{}),"\n","\u2981\tMade searching for documents much faster with indexing."]}),"\n",(0,s.jsx)(n.p,{children:"Now, the company runs faster, cleaner, and with fewer mistakes. Delta Lake turned the messy warehouse into a well-run data system."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Profesional Explanation:"})}),"\n",(0,s.jsx)(n.p,{children:"Delta Lake is an open-source storage layer that brings ACID transactions, schema enforcement, and time travel (versioning) to your data lake. It allows you to use data lakes (like those built on S3, ADLS, or HDFS) with the reliability of a database, enabling scalable, reliable data pipelines and analytics and it is stored in formats like Parquet."}),"\n",(0,s.jsx)(n.p,{children:"Upserts \u2013 Adds new customers or updates old ones."}),"\n",(0,s.jsx)(n.p,{children:'Schema Evolution \u2013 Automatically handles new info (e.g., "birthday" added later).'}),"\n",(0,s.jsx)(n.p,{children:"Time Travel \u2013 Lets you check yesterday\u2019s list in case of mistakes."}),"\n",(0,s.jsx)(n.h2,{id:"code--real-world-scenario",children:"Code : Real-World Scenario"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Problem:"})," Managing Data on a Data Lake Without Delta Lake"]}),"\n",(0,s.jsx)(n.p,{children:"Let\u2019s walk through what it looks like to read, write, and update data without Delta Lake (just using Parquet on S3 or HDFS)."}),"\n",(0,s.jsx)(n.h2,{id:"upserts",children:"Upserts"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"\ud83d\udeab Without Delta Lake (Raw Parquet on S3)"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Read CSV file\r\ndf = spark.read.csv("/raw/sales.csv", header=True, inferSchema=True)\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Write to S3 in Parquet format (no transactional control)\r\ndf.write.mode("overwrite").parquet("s3://data-lake/silver/sales/")\n'})}),"\n",(0,s.jsx)(n.p,{children:"\u26a0\ufe0f Update/Upsert is Hard (No MERGE, No Transactions)"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"To update records, you'd have to:-"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Read existing Parquet data."}),"\n",(0,s.jsx)(n.li,{children:"Read new/updated data."}),"\n",(0,s.jsx)(n.li,{children:"Perform join & logic in Spark."}),"\n",(0,s.jsx)(n.li,{children:"Overwrite the entire dataset."}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Read existing Parquet\r\nexisting_df = spark.read.parquet("s3://data-lake/silver/sales/")\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Read new data\r\nupdates_df = spark.read.csv("/raw/updates.csv", header=True, inferSchema=True)\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Perform upsert manually\r\nfrom pyspark.sql.functions import col\r\n\r\n# Keep records that are NOT in the updates (i.e., unmatched by ID)\r\nunchanged_df = existing_df.join(updates_df, on="id", how="left_anti")\r\n\r\n# Combine updated + unchanged\r\nfinal_df = unchanged_df.unionByName(updates_df)\r\n\r\n# Overwrite full dataset (expensive, risky!)\r\nfinal_df.write.mode("overwrite").parquet("s3://data-lake/silver/sales/")\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"\u274c Drawbacks:-"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"No schema enforcement or evolution."}),"\n",(0,s.jsx)(n.li,{children:"No ACID guarantees \u2014 overwrites can cause data loss."}),"\n",(0,s.jsx)(n.li,{children:"Complex and inefficient upserts."}),"\n",(0,s.jsx)(n.li,{children:"No versioning or time travel."}),"\n",(0,s.jsx)(n.li,{children:"Difficult to audit or track changes."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"\u2705 With Delta Lake"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Ingest & Save as Delta\r\ndf = spark.read.csv("/raw/sales.csv", header=True, inferSchema=True)\r\ndf.write.format("delta").mode("overwrite").save("/data/silver/sales/")\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Perform Upsert with DeltaTable API"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from delta.tables import DeltaTable \r\n\r\n# Load existing Delta Table\r\nexisting = DeltaTable.forPath(spark, "/data/silver/sales/")\r\n\r\n# Load new updates\r\nnew_data = spark.read.csv("/raw/updates.csv", header=True, inferSchema=True)\r\n\r\n# Merge (Upsert)\r\nexisting.alias("old") \\\r\n.merge(new_data.alias("new"), "old.id = new.id") \\\r\n.whenMatchedUpdateAll() \\\r\n.whenNotMatchedInsertAll() \\\r\n.execute()\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"\u2705 Benefits:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"ACID transactions \u2705"}),"\n",(0,s.jsx)(n.li,{children:"Easy upserts/merges \u2705"}),"\n",(0,s.jsx)(n.li,{children:"Schema enforcement & evolution \u2705"}),"\n",(0,s.jsx)(n.li,{children:"Time travel/versioning \u2705"}),"\n",(0,s.jsx)(n.li,{children:"Efficient reads with indexing (Z-Ordering) \u2705"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Schema Evolution"})}),"\n",(0,s.jsx)("u",{children:"\ud83e\udde9 Problem:"}),"\n",(0,s.jsx)(n.p,{children:'New data has extra columns (e.g., "birthday"), and you want to append without manually altering the table schema.'}),"\n",(0,s.jsx)("u",{children:"\u2705 Solution:"}),"\n",(0,s.jsx)(n.p,{children:"Delta Lake supports automatic schema evolution with mergeSchema."}),"\n",(0,s.jsx)(n.p,{children:"\ud83d\udd27 Code:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'df = spark.read.csv("/mnt/raw/new_customers.csv", header=True)\r\n\r\ndf.write.format("delta") \\\r\n  .option("mergeSchema", "true") \\\r\n  .mode("append") \\\r\n  .save("/mnt/silver/customers/")\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"\ud83d\udcdd What it does:"})}),"\n",(0,s.jsx)(n.p,{children:'Detects new columns (like "birthday")\r\nMerges them into the existing schema automatically\r\nAvoids failures from schema mismatch'}),"\n",(0,s.jsx)(n.h2,{id:"schema-evolution--real-world-scenario",children:"Schema Evolution \u2013 Real-World Scenario"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Scenario:"}),"\r\nNew data has a new column (e.g., birthday).\r\nYou want to append it to an existing table without errors."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"\u274c Without Delta Lake (Parquet only)"})}),"\n",(0,s.jsxs)(n.p,{children:["Appending a file with new columns will fail or drop columns silently, depending on settings. You must:-",(0,s.jsx)(n.br,{}),"\n","Manually update schema\r\nReprocess or rewrite the entire dataset"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# This may fail or ignore \'birthday\' column\r\ndf = spark.read.csv("/mnt/raw/new_customers.csv", header=True)\r\n\r\ndf.write.mode("append").parquet("/mnt/silver/customers/")\r\n\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"\u26a0\ufe0f Problems:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"No schema validation"}),"\n",(0,s.jsx)(n.li,{children:"Manual handling required"}),"\n",(0,s.jsx)(n.li,{children:"Risk of silent data loss or inconsistency"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"\u2705 With Delta Lake (Schema Evolution Made Easy)"})}),"\n",(0,s.jsx)(n.p,{children:"Delta Lake automatically merges the new schema when enabled."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'df = spark.read.csv("/mnt/raw/new_customers.csv", header=True)\r\n\r\ndf.write.format("delta") \\\r\n  .option("mergeSchema", "true") \\\r\n  .mode("append") \\\r\n  .save("/mnt/silver/customers/")\r\n\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"\u2705 Benefits:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"New columns (like birthday) are added automatically"}),"\n",(0,s.jsx)(n.li,{children:"No data loss, no manual updates"}),"\n",(0,s.jsx)(n.li,{children:"Clean and scalable schema management"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"time-travel",children:"Time Travel"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\ud83e\udde9 Scenario:"}),"\r\nYou need to query historical data \u2014 an earlier version of your dataset \u2014 to recover from mistakes or audit changes."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"\u274c Without Delta Lake"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"No built-in versioning."}),"\n",(0,s.jsx)(n.li,{children:"To track changes, you\u2019d have to manually save copies or snapshots."}),"\n",(0,s.jsx)(n.li,{children:"Complex and error-prone to manage."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u2705 With Delta Lake (Built-in Time Travel)"}),"\r\nRead data as it was at a specific version or timestamp easily:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Read data at a specific version\r\nspark.read.format("delta") \\\r\n  .option("versionAsOf", 1) \\\r\n  .load("/mnt/silver/customers/")\r\n\r\n# Or read data as of a specific timestamp\r\nspark.read.format("delta") \\\r\n  .option("timestampAsOf", "2024-07-01") \\\r\n  .load("/mnt/silver/customers/")\r\n\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"\ud83d\udcdd Benefits:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Query any previous snapshot instantly"}),"\n",(0,s.jsx)(n.li,{children:"Undo errors or compare changes"}),"\n",(0,s.jsx)(n.li,{children:"Simplifies data auditing and debugging"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary-table",children:"Summary table"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Feature"}),(0,s.jsx)(n.th,{children:"Without Delta Lake"}),(0,s.jsx)(n.th,{children:"With Delta Lake"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Updates"})}),(0,s.jsx)(n.td,{children:"Manual overwrite"}),(0,s.jsxs)(n.td,{children:["Easy ",(0,s.jsx)(n.code,{children:"update()"})," or ",(0,s.jsx)(n.code,{children:"merge()"})]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Versioning"})}),(0,s.jsx)(n.td,{children:"Not supported"}),(0,s.jsx)(n.td,{children:"Yes: Time Travel supported"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Schema validation"})}),(0,s.jsx)(n.td,{children:"Manual, error-prone"}),(0,s.jsx)(n.td,{children:"Automatic enforcement"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Atomic operations"})}),(0,s.jsx)(n.td,{children:"Not guaranteed"}),(0,s.jsx)(n.td,{children:"ACID transactions supported"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Performance"})}),(0,s.jsx)(n.td,{children:"Slower for large datasets"}),(0,s.jsx)(n.td,{children:"Optimized via transaction logs"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"-1-minute-summary--delta-lake--foundation-of-reliable-data-pipelines",children:"\ud83d\udd11 1-Minute Summary:  Delta Lake \u2013 Foundation of Reliable Data Pipelines"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Step 1: Upserts (Without vs With Delta Lake)"}),(0,s.jsx)(n.br,{}),"\n",(0,s.jsx)("u",{children:"Without Delta:"})," Manual join, overwrite entire dataset \u2014 risky and inefficient.",(0,s.jsx)(n.br,{}),"\n",(0,s.jsx)("u",{children:"With Delta:"})," Use merge() to safely update or insert records."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Step 2: Schema Evolution"}),(0,s.jsx)(n.br,{}),"\n",(0,s.jsx)("u",{children:"Without Delta:"})," Fails or silently ignores new columns \u2014 requires manual fixes.",(0,s.jsx)(n.br,{}),"\n",(0,s.jsx)("u",{children:"With Delta:"})," Use mergeSchema to automatically handle new columns (e.g., birthday)."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Step 3: Time Travel"}),(0,s.jsx)(n.br,{}),"\n",(0,s.jsx)("u",{children:"Without Delta:"})," No versioning \u2014 must manually save snapshots.",(0,s.jsx)(n.br,{}),"\n",(0,s.jsx)("u",{children:"With Delta:"})," Use versionAsOf or timestampAsOf to query historical data instantly."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Step 4: Performance & Reliability"}),(0,s.jsx)(n.br,{}),"\n","Delta's transaction logs optimize reads, enable caching, and ensure safe concurrent writes."]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>l});var a=r(6540);const s={},t=a.createContext(s);function i(e){const n=a.useContext(t);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),a.createElement(t.Provider,{value:n},e.children)}}}]);