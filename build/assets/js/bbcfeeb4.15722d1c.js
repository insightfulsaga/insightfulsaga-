"use strict";(self.webpackChunkdatacraft_school=self.webpackChunkdatacraft_school||[]).push([[6444],{8453:(n,e,a)=>{a.d(e,{R:()=>t,x:()=>l});var r=a(6540);const i={},s=r.createContext(i);function t(n){const e=r.useContext(s);return r.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:t(n.components),r.createElement(s.Provider,{value:e},n.children)}},9443:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>o,contentTitle:()=>l,default:()=>c,frontMatter:()=>t,metadata:()=>r,toc:()=>p});const r=JSON.parse('{"id":"df-window-functions","title":"Window Functions in PySpark DataFrames","description":"Learn how to use PySpark window functions for ranking, running totals, cumulative sums, and time-based analytics in Databricks.","source":"@site/docs-pyspark/df-window-functions.md","sourceDirName":".","slug":"/df-window-functions","permalink":"/pyspark/df-window-functions","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"PySpark","permalink":"/pyspark/tags/py-spark"},{"inline":true,"label":"Apache Spark","permalink":"/pyspark/tags/apache-spark"},{"inline":true,"label":"Big Data","permalink":"/pyspark/tags/big-data"},{"inline":true,"label":"Spark Basics","permalink":"/pyspark/tags/spark-basics"},{"inline":true,"label":"Cluster Computing","permalink":"/pyspark/tags/cluster-computing"},{"inline":true,"label":"Spark Architecture","permalink":"/pyspark/tags/spark-architecture"},{"inline":true,"label":"Driver Program","permalink":"/pyspark/tags/driver-program"},{"inline":true,"label":"Executors","permalink":"/pyspark/tags/executors"},{"inline":true,"label":"Cluster Manager","permalink":"/pyspark/tags/cluster-manager"},{"inline":true,"label":"SparkSession","permalink":"/pyspark/tags/spark-session"},{"inline":true,"label":"SparkContext","permalink":"/pyspark/tags/spark-context"},{"inline":true,"label":"RDD","permalink":"/pyspark/tags/rdd"},{"inline":true,"label":"RDD Transformations","permalink":"/pyspark/tags/rdd-transformations"},{"inline":true,"label":"RDD Actions","permalink":"/pyspark/tags/rdd-actions"},{"inline":true,"label":"Key-Value RDD","permalink":"/pyspark/tags/key-value-rdd"},{"inline":true,"label":"RDD Caching","permalink":"/pyspark/tags/rdd-caching"},{"inline":true,"label":"DataFrame","permalink":"/pyspark/tags/data-frame"},{"inline":true,"label":"DataFrame API","permalink":"/pyspark/tags/data-frame-api"},{"inline":true,"label":"Column Operations","permalink":"/pyspark/tags/column-operations"},{"inline":true,"label":"DataFrame Joins","permalink":"/pyspark/tags/data-frame-joins"},{"inline":true,"label":"Aggregations","permalink":"/pyspark/tags/aggregations"},{"inline":true,"label":"GroupBy","permalink":"/pyspark/tags/group-by"},{"inline":true,"label":"Window Functions","permalink":"/pyspark/tags/window-functions"},{"inline":true,"label":"Missing Data Handling","permalink":"/pyspark/tags/missing-data-handling"},{"inline":true,"label":"Spark SQL","permalink":"/pyspark/tags/spark-sql"},{"inline":true,"label":"Temp Views","permalink":"/pyspark/tags/temp-views"},{"inline":true,"label":"Spark SQL Functions","permalink":"/pyspark/tags/spark-sql-functions"},{"inline":true,"label":"UDF","permalink":"/pyspark/tags/udf"},{"inline":true,"label":"UDAF","permalink":"/pyspark/tags/udaf"}],"version":"current","frontMatter":{"id":"df-window-functions","title":"Window Functions in PySpark DataFrames","sidebar_label":"Window Functions","description":"Learn how to use PySpark window functions for ranking, running totals, cumulative sums, and time-based analytics in Databricks.","keywords":["PySpark window functions","Spark row_number","running totals PySpark","partitionBy orderBy","cumulative sum Spark","Databricks analytics"],"tags":["PySpark","Apache Spark","Big Data","Spark Basics","Cluster Computing","Spark Architecture","Driver Program","Executors","Cluster Manager","SparkSession","SparkContext","RDD","RDD Transformations","RDD Actions","Key-Value RDD","RDD Caching","DataFrame","DataFrame API","Column Operations","DataFrame Joins","Aggregations","GroupBy","Window Functions","Missing Data Handling","Spark SQL","Temp Views","Spark SQL Functions","UDF","UDAF"]},"sidebar":"tutorialSidebar","previous":{"title":"Dataframe Aggregation","permalink":"/pyspark/pyspark/aggregation"},"next":{"title":"Handling Missing Data","permalink":"/pyspark/pyspark/missing-data"}}');var i=a(4848),s=a(8453);const t={id:"df-window-functions",title:"Window Functions in PySpark DataFrames",sidebar_label:"Window Functions",description:"Learn how to use PySpark window functions for ranking, running totals, cumulative sums, and time-based analytics in Databricks.",keywords:["PySpark window functions","Spark row_number","running totals PySpark","partitionBy orderBy","cumulative sum Spark","Databricks analytics"],tags:["PySpark","Apache Spark","Big Data","Spark Basics","Cluster Computing","Spark Architecture","Driver Program","Executors","Cluster Manager","SparkSession","SparkContext","RDD","RDD Transformations","RDD Actions","Key-Value RDD","RDD Caching","DataFrame","DataFrame API","Column Operations","DataFrame Joins","Aggregations","GroupBy","Window Functions","Missing Data Handling","Spark SQL","Temp Views","Spark SQL Functions","UDF","UDAF"]},l="Window Functions in PySpark DataFrames",o={},p=[{value:"Why Window Functions Matter",id:"why-window-functions-matter",level:2},{value:"1. Import Window Functions",id:"1-import-window-functions",level:2},{value:"2. Ranking Rows: row_number()",id:"2-ranking-rows-row_number",level:2},{value:"Story Example",id:"story-example",level:3},{value:"3. Cumulative / Running Totals",id:"3-cumulative--running-totals",level:2},{value:"4. Moving Averages",id:"4-moving-averages",level:2},{value:"Use Case",id:"use-case",level:3},{value:"5. Ranking With Ties: rank() vs dense_rank()",id:"5-ranking-with-ties-rank-vs-dense_rank",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"window-functions-in-pyspark-dataframes",children:"Window Functions in PySpark DataFrames"})}),"\n",(0,i.jsxs)(e.p,{children:["At ",(0,i.jsx)(e.strong,{children:"NeoMart"}),", analysts often need more than simple aggregations.",(0,i.jsx)(e.br,{}),"\n","They want ",(0,i.jsx)(e.strong,{children:"rankings, running totals, moving averages, and session-level analytics"}),".\r\nFor example:"]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Rank products by revenue per category"}),"\n",(0,i.jsx)(e.li,{children:"Calculate each customer\u2019s cumulative spend"}),"\n",(0,i.jsx)(e.li,{children:"Determine the last purchase date for loyalty scoring"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:["This is where ",(0,i.jsx)(e.strong,{children:"window functions"})," come in."]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"why-window-functions-matter",children:"Why Window Functions Matter"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["Allow calculations ",(0,i.jsx)(e.strong,{children:"over partitions of data"})]}),"\n",(0,i.jsxs)(e.li,{children:["Keep ",(0,i.jsx)(e.strong,{children:"row-level detail"})," while performing aggregations"]}),"\n",(0,i.jsxs)(e.li,{children:["Essential for ",(0,i.jsx)(e.strong,{children:"ranking, cumulative metrics, and analytics features"})]}),"\n",(0,i.jsx)(e.li,{children:"Perfect for dashboards, reporting, and feature engineering in ML"}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:["Unlike ",(0,i.jsx)(e.code,{children:"groupBy"}),", which reduces rows, window functions ",(0,i.jsx)(e.strong,{children:"retain all rows"}),"."]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"1-import-window-functions",children:"1. Import Window Functions"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"from pyspark.sql.window import Window\r\nfrom pyspark.sql.functions import row_number, sum, avg, rank, desc\n"})}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"2-ranking-rows-row_number",children:"2. Ranking Rows: row_number()"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'window_spec = Window.partitionBy("category").orderBy(desc("revenue"))\r\n\r\ndf.withColumn("rank", row_number().over(window_spec)).show()\n'})}),"\n",(0,i.jsx)(e.h3,{id:"story-example",children:"Story Example"}),"\n",(0,i.jsxs)(e.p,{children:["NeoMart wants ",(0,i.jsx)(e.strong,{children:"top 3 products per category"})," for promotion campaigns."]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"3-cumulative--running-totals",children:"3. Cumulative / Running Totals"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'window_spec = Window.partitionBy("customer_id").orderBy("date")\r\n\r\ndf.withColumn("running_total", sum("amount").over(window_spec)).show()\n'})}),"\n",(0,i.jsxs)(e.p,{children:["This tracks ",(0,i.jsx)(e.strong,{children:"each customer\u2019s cumulative spend over time"}),", essential for loyalty programs."]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"4-moving-averages",children:"4. Moving Averages"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'window_spec = Window.orderBy("date").rowsBetween(-3, 0)\r\n\r\ndf.withColumn("moving_avg", avg("amount").over(window_spec)).show()\n'})}),"\n",(0,i.jsx)(e.h3,{id:"use-case",children:"Use Case"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Smooth out daily sales volatility"}),"\n",(0,i.jsx)(e.li,{children:"Detect trends or anomalies"}),"\n"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"5-ranking-with-ties-rank-vs-dense_rank",children:"5. Ranking With Ties: rank() vs dense_rank()"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'window_spec = Window.partitionBy("category").orderBy(desc("revenue"))\r\n\r\ndf.withColumn("rank", rank().over(window_spec)).show()\r\ndf.withColumn("dense_rank", dense_rank().over(window_spec)).show()\n'})}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.code,{children:"rank()"})," \u2192 leaves gaps for ties"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.code,{children:"dense_rank()"})," \u2192 no gaps"]}),"\n"]}),"\n",(0,i.jsxs)(e.p,{children:["Useful for ",(0,i.jsx)(e.strong,{children:"leaderboards or top-n product ranking"}),"."]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(e.p,{children:"Window functions in PySpark:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["Operate ",(0,i.jsx)(e.strong,{children:"over partitions without reducing rows"})]}),"\n",(0,i.jsxs)(e.li,{children:["Enable ",(0,i.jsx)(e.strong,{children:"rankings, cumulative sums, moving averages"})]}),"\n",(0,i.jsxs)(e.li,{children:["Are ",(0,i.jsx)(e.strong,{children:"essential for BI, ML features, and advanced analytics"})]}),"\n",(0,i.jsx)(e.li,{children:"Retain the flexibility of DataFrame operations"}),"\n"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsxs)(e.p,{children:["Next, we\u2019ll cover ",(0,i.jsx)(e.strong,{children:"Handling Missing Data \u2014 Drop, Fill, Replace"}),", to clean and prepare datasets for analysis and modeling."]})]})}function c(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}}}]);