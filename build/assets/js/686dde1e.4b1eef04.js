"use strict";(self.webpackChunkdatacraft_school=self.webpackChunkdatacraft_school||[]).push([[4472],{6717:(e,r,a)=>{a.r(r),a.d(r,{assets:()=>o,contentTitle:()=>l,default:()=>c,frontMatter:()=>i,metadata:()=>s,toc:()=>p});const s=JSON.parse('{"id":"pyspark-first-job","title":"First PySpark Job \u2014 Hello World Example","description":"Learn how to write and run your first PySpark job with a hands-on \u201cHello World\u201d example, and understand the end-to-end workflow in Spark.","source":"@site/docs-pyspark/pyspark-first-job.md","sourceDirName":".","slug":"/pyspark-first-job","permalink":"/pyspark/pyspark-first-job","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"PySpark","permalink":"/pyspark/tags/py-spark"},{"inline":true,"label":"Apache Spark","permalink":"/pyspark/tags/apache-spark"},{"inline":true,"label":"Big Data","permalink":"/pyspark/tags/big-data"},{"inline":true,"label":"Spark Basics","permalink":"/pyspark/tags/spark-basics"},{"inline":true,"label":"Cluster Computing","permalink":"/pyspark/tags/cluster-computing"},{"inline":true,"label":"Spark Architecture","permalink":"/pyspark/tags/spark-architecture"},{"inline":true,"label":"Driver Program","permalink":"/pyspark/tags/driver-program"},{"inline":true,"label":"Executors","permalink":"/pyspark/tags/executors"},{"inline":true,"label":"Cluster Manager","permalink":"/pyspark/tags/cluster-manager"},{"inline":true,"label":"SparkSession","permalink":"/pyspark/tags/spark-session"},{"inline":true,"label":"SparkContext","permalink":"/pyspark/tags/spark-context"},{"inline":true,"label":"RDD","permalink":"/pyspark/tags/rdd"},{"inline":true,"label":"RDD Transformations","permalink":"/pyspark/tags/rdd-transformations"},{"inline":true,"label":"RDD Actions","permalink":"/pyspark/tags/rdd-actions"},{"inline":true,"label":"Key-Value RDD","permalink":"/pyspark/tags/key-value-rdd"},{"inline":true,"label":"RDD Caching","permalink":"/pyspark/tags/rdd-caching"},{"inline":true,"label":"DataFrame","permalink":"/pyspark/tags/data-frame"},{"inline":true,"label":"DataFrame API","permalink":"/pyspark/tags/data-frame-api"},{"inline":true,"label":"Column Operations","permalink":"/pyspark/tags/column-operations"},{"inline":true,"label":"DataFrame Joins","permalink":"/pyspark/tags/data-frame-joins"},{"inline":true,"label":"Aggregations","permalink":"/pyspark/tags/aggregations"},{"inline":true,"label":"GroupBy","permalink":"/pyspark/tags/group-by"},{"inline":true,"label":"Window Functions","permalink":"/pyspark/tags/window-functions"},{"inline":true,"label":"Missing Data Handling","permalink":"/pyspark/tags/missing-data-handling"},{"inline":true,"label":"Spark SQL","permalink":"/pyspark/tags/spark-sql"},{"inline":true,"label":"Temp Views","permalink":"/pyspark/tags/temp-views"},{"inline":true,"label":"Spark SQL Functions","permalink":"/pyspark/tags/spark-sql-functions"},{"inline":true,"label":"UDF","permalink":"/pyspark/tags/udf"},{"inline":true,"label":"UDAF","permalink":"/pyspark/tags/udaf"}],"version":"current","frontMatter":{"id":"pyspark-first-job","title":"First PySpark Job \u2014 Hello World Example","sidebar_label":"First PySpark Job","description":"Learn how to write and run your first PySpark job with a hands-on \u201cHello World\u201d example, and understand the end-to-end workflow in Spark.","tags":["PySpark","Apache Spark","Big Data","Spark Basics","Cluster Computing","Spark Architecture","Driver Program","Executors","Cluster Manager","SparkSession","SparkContext","RDD","RDD Transformations","RDD Actions","Key-Value RDD","RDD Caching","DataFrame","DataFrame API","Column Operations","DataFrame Joins","Aggregations","GroupBy","Window Functions","Missing Data Handling","Spark SQL","Temp Views","Spark SQL Functions","UDF","UDAF"]},"sidebar":"tutorialSidebar","previous":{"title":"SparkSession & SparkContext","permalink":"/pyspark/pyspark-spark-session-context"},"next":{"title":"RDD Basics","permalink":"/pyspark/rdd-basics"}}');var n=a(4848),t=a(8453);const i={id:"pyspark-first-job",title:"First PySpark Job \u2014 Hello World Example",sidebar_label:"First PySpark Job",description:"Learn how to write and run your first PySpark job with a hands-on \u201cHello World\u201d example, and understand the end-to-end workflow in Spark.",tags:["PySpark","Apache Spark","Big Data","Spark Basics","Cluster Computing","Spark Architecture","Driver Program","Executors","Cluster Manager","SparkSession","SparkContext","RDD","RDD Transformations","RDD Actions","Key-Value RDD","RDD Caching","DataFrame","DataFrame API","Column Operations","DataFrame Joins","Aggregations","GroupBy","Window Functions","Missing Data Handling","Spark SQL","Temp Views","Spark SQL Functions","UDF","UDAF"]},l="First PySpark Job \u2014 Hello World Example",o={},p=[{value:"Step 1: Initialize SparkSession",id:"step-1-initialize-sparksession",level:2},{value:"Step 2: Create Sample Data",id:"step-2-create-sample-data",level:2},{value:"Step 3: Perform a Simple Transformation",id:"step-3-perform-a-simple-transformation",level:2},{value:"Step 4: Show Results",id:"step-4-show-results",level:2},{value:"Step 5: Stop SparkSession",id:"step-5-stop-sparksession",level:2},{value:"Real-Life Example",id:"real-life-example",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2}];function d(e){const r={blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(r.header,{children:(0,n.jsx)(r.h1,{id:"first-pyspark-job--hello-world-example",children:"First PySpark Job \u2014 Hello World Example"})}),"\n",(0,n.jsxs)(r.p,{children:["Now that we understand ",(0,n.jsx)(r.strong,{children:"PySpark setup, SparkSession, and SparkContext"}),", it\u2019s time to write your ",(0,n.jsx)(r.strong,{children:"first PySpark job"}),". Think of this as the ",(0,n.jsx)(r.strong,{children:"Hello World"})," of distributed computing, but instead of printing text, we\u2019ll process some data in parallel!"]}),"\n",(0,n.jsx)(r.hr,{}),"\n",(0,n.jsx)(r.h2,{id:"step-1-initialize-sparksession",children:"Step 1: Initialize SparkSession"}),"\n",(0,n.jsxs)(r.p,{children:["Every PySpark job starts with a ",(0,n.jsx)(r.strong,{children:"SparkSession"}),":"]}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-python",children:'from pyspark.sql import SparkSession\r\n\r\nspark = SparkSession.builder \\\r\n    .appName("HelloWorldJob") \\\r\n    .master("local[*]") \\\r\n    .getOrCreate()\n'})}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"appName"}),": Name of your Spark job"]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"master"}),": Execution mode (",(0,n.jsx)(r.code,{children:"local[*]"})," uses all CPU cores)"]}),"\n"]}),"\n",(0,n.jsxs)(r.blockquote,{children:["\n",(0,n.jsxs)(r.p,{children:["SparkSession automatically creates a ",(0,n.jsx)(r.strong,{children:"SparkContext"})," under the hood."]}),"\n"]}),"\n",(0,n.jsx)(r.hr,{}),"\n",(0,n.jsx)(r.h2,{id:"step-2-create-sample-data",children:"Step 2: Create Sample Data"}),"\n",(0,n.jsxs)(r.p,{children:["We will create a ",(0,n.jsx)(r.strong,{children:"small dataset"})," to simulate a real-world scenario:"]}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-python",children:'data = [("Alice", 25), ("Bob", 30), ("Charlie", 28)]\r\ncolumns = ["Name", "Age"]\r\n\r\ndf = spark.createDataFrame(data, columns)\n'})}),"\n",(0,n.jsxs)(r.p,{children:["Here, we are using a ",(0,n.jsx)(r.strong,{children:"DataFrame"}),", which is Spark\u2019s high-level abstraction for structured data."]}),"\n",(0,n.jsx)(r.hr,{}),"\n",(0,n.jsx)(r.h2,{id:"step-3-perform-a-simple-transformation",children:"Step 3: Perform a Simple Transformation"}),"\n",(0,n.jsx)(r.p,{children:"Let\u2019s filter out people younger than 30:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-python",children:"filtered_df = df.filter(df.Age >= 30)\n"})}),"\n",(0,n.jsxs)(r.p,{children:["This is equivalent to ",(0,n.jsx)(r.strong,{children:"SQL WHERE clause"})," in Spark."]}),"\n",(0,n.jsx)(r.hr,{}),"\n",(0,n.jsx)(r.h2,{id:"step-4-show-results",children:"Step 4: Show Results"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-python",children:"filtered_df.show()\n"})}),"\n",(0,n.jsx)(r.p,{children:"Expected output:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{children:"+----+---+\r\n|Name|Age|\r\n+----+---+\r\n| Bob| 30|\r\n+----+---+\n"})}),"\n",(0,n.jsxs)(r.blockquote,{children:["\n",(0,n.jsxs)(r.p,{children:["Congratulations! You just ran your ",(0,n.jsx)(r.strong,{children:"first PySpark job"})," and applied a basic transformation."]}),"\n"]}),"\n",(0,n.jsx)(r.hr,{}),"\n",(0,n.jsx)(r.h2,{id:"step-5-stop-sparksession",children:"Step 5: Stop SparkSession"}),"\n",(0,n.jsxs)(r.p,{children:["Always stop the Spark session to ",(0,n.jsx)(r.strong,{children:"free resources"}),":"]}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-python",children:"spark.stop()\n"})}),"\n",(0,n.jsx)(r.hr,{}),"\n",(0,n.jsx)(r.h2,{id:"real-life-example",children:"Real-Life Example"}),"\n",(0,n.jsxs)(r.p,{children:["At ",(0,n.jsx)(r.strong,{children:"ShopVerse Retail"}),", new ETL jobs often start like this:"]}),"\n",(0,n.jsxs)(r.ol,{children:["\n",(0,n.jsxs)(r.li,{children:["Load ",(0,n.jsx)(r.strong,{children:"sample CSV files"})," into a DataFrame"]}),"\n",(0,n.jsxs)(r.li,{children:["Apply ",(0,n.jsx)(r.strong,{children:"basic transformations"})," (filter, groupBy, aggregate)"]}),"\n",(0,n.jsxs)(r.li,{children:["Verify results locally using ",(0,n.jsx)(r.strong,{children:(0,n.jsx)(r.code,{children:"show()"})})]}),"\n",(0,n.jsxs)(r.li,{children:["Deploy the same logic to a ",(0,n.jsx)(r.strong,{children:"cluster for production scale"})]}),"\n"]}),"\n",(0,n.jsxs)(r.p,{children:["This approach allows ",(0,n.jsx)(r.strong,{children:"safe experimentation"})," without affecting production pipelines."]}),"\n",(0,n.jsx)(r.hr,{}),"\n",(0,n.jsx)(r.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsxs)(r.li,{children:["Every PySpark job starts with a ",(0,n.jsx)(r.strong,{children:"SparkSession"}),"."]}),"\n",(0,n.jsxs)(r.li,{children:["Use ",(0,n.jsx)(r.strong,{children:"DataFrames"})," for structured data processing."]}),"\n",(0,n.jsxs)(r.li,{children:["Apply ",(0,n.jsx)(r.strong,{children:"transformations"})," (",(0,n.jsx)(r.code,{children:"filter"}),", ",(0,n.jsx)(r.code,{children:"map"}),", ",(0,n.jsx)(r.code,{children:"select"}),") to process data."]}),"\n",(0,n.jsxs)(r.li,{children:["Always ",(0,n.jsx)(r.strong,{children:"stop SparkSession"})," to release cluster resources."]}),"\n",(0,n.jsxs)(r.li,{children:["Start small with ",(0,n.jsx)(r.strong,{children:"sample data"})," before scaling to large datasets."]}),"\n"]}),"\n",(0,n.jsx)(r.hr,{}),"\n",(0,n.jsxs)(r.p,{children:["Next, we will explore ",(0,n.jsx)(r.strong,{children:"RDDs vs DataFrames vs Datasets"})," in detail to understand ",(0,n.jsx)(r.strong,{children:"which abstraction to choose for your Spark jobs"}),"."]}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{})})]})}function c(e={}){const{wrapper:r}={...(0,t.R)(),...e.components};return r?(0,n.jsx)(r,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},8453:(e,r,a)=>{a.d(r,{R:()=>i,x:()=>l});var s=a(6540);const n={},t=s.createContext(n);function i(e){const r=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function l(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:i(e.components),s.createElement(t.Provider,{value:r},e.children)}}}]);