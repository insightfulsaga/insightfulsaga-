"use strict";(self.webpackChunkdatacraft_school=self.webpackChunkdatacraft_school||[]).push([[294],{508:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>t,default:()=>d,frontMatter:()=>l,metadata:()=>i,toc:()=>p});const i=JSON.parse('{"id":"pyspark-mllib-overview","title":"MLlib Overview","description":"Imagine you\u2019re a data scientist in a high-tech lab, not just a data engineer. Data isn\u2019t sitting quietly in files\u2014it\u2019s streaming, growing, and changing constantly. You want to predict outcomes, classify users, or group behaviors, all at scale.","source":"@site/docs-pyspark/pyspark-mllib-overview.md","sourceDirName":".","slug":"/pyspark-mllib-overview","permalink":"/pyspark/pyspark-mllib-overview","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"PySpark","permalink":"/pyspark/tags/py-spark"},{"inline":true,"label":"Apache Spark","permalink":"/pyspark/tags/apache-spark"},{"inline":true,"label":"Big Data","permalink":"/pyspark/tags/big-data"},{"inline":true,"label":"Spark Basics","permalink":"/pyspark/tags/spark-basics"},{"inline":true,"label":"Cluster Computing","permalink":"/pyspark/tags/cluster-computing"},{"inline":true,"label":"Spark Architecture","permalink":"/pyspark/tags/spark-architecture"},{"inline":true,"label":"Driver Program","permalink":"/pyspark/tags/driver-program"},{"inline":true,"label":"Executors","permalink":"/pyspark/tags/executors"},{"inline":true,"label":"Cluster Manager","permalink":"/pyspark/tags/cluster-manager"},{"inline":true,"label":"SparkSession","permalink":"/pyspark/tags/spark-session"},{"inline":true,"label":"SparkContext","permalink":"/pyspark/tags/spark-context"},{"inline":true,"label":"RDD","permalink":"/pyspark/tags/rdd"},{"inline":true,"label":"RDD Transformations","permalink":"/pyspark/tags/rdd-transformations"},{"inline":true,"label":"RDD Actions","permalink":"/pyspark/tags/rdd-actions"},{"inline":true,"label":"Key-Value RDD","permalink":"/pyspark/tags/key-value-rdd"},{"inline":true,"label":"RDD Caching","permalink":"/pyspark/tags/rdd-caching"},{"inline":true,"label":"DataFrame","permalink":"/pyspark/tags/data-frame"},{"inline":true,"label":"DataFrame API","permalink":"/pyspark/tags/data-frame-api"},{"inline":true,"label":"Column Operations","permalink":"/pyspark/tags/column-operations"},{"inline":true,"label":"DataFrame Joins","permalink":"/pyspark/tags/data-frame-joins"},{"inline":true,"label":"Aggregations","permalink":"/pyspark/tags/aggregations"},{"inline":true,"label":"GroupBy","permalink":"/pyspark/tags/group-by"},{"inline":true,"label":"Window Functions","permalink":"/pyspark/tags/window-functions"},{"inline":true,"label":"Missing Data Handling","permalink":"/pyspark/tags/missing-data-handling"},{"inline":true,"label":"Spark SQL","permalink":"/pyspark/tags/spark-sql"},{"inline":true,"label":"Temp Views","permalink":"/pyspark/tags/temp-views"},{"inline":true,"label":"Spark SQL Functions","permalink":"/pyspark/tags/spark-sql-functions"},{"inline":true,"label":"UDF","permalink":"/pyspark/tags/udf"},{"inline":true,"label":"UDAF","permalink":"/pyspark/tags/udaf"},{"inline":true,"label":"Explode","permalink":"/pyspark/tags/explode"},{"inline":true,"label":"Arrays","permalink":"/pyspark/tags/arrays"},{"inline":true,"label":"StructType","permalink":"/pyspark/tags/struct-type"},{"inline":true,"label":"Complex Data Types","permalink":"/pyspark/tags/complex-data-types"},{"inline":true,"label":"Pivot","permalink":"/pyspark/tags/pivot"},{"inline":true,"label":"Unpivot","permalink":"/pyspark/tags/unpivot"},{"inline":true,"label":"Join Optimization","permalink":"/pyspark/tags/join-optimization"},{"inline":true,"label":"Sorting","permalink":"/pyspark/tags/sorting"},{"inline":true,"label":"Sampling","permalink":"/pyspark/tags/sampling"},{"inline":true,"label":"Partitioning","permalink":"/pyspark/tags/partitioning"},{"inline":true,"label":"Bucketing","permalink":"/pyspark/tags/bucketing"},{"inline":true,"label":"CSV","permalink":"/pyspark/tags/csv"},{"inline":true,"label":"JSON","permalink":"/pyspark/tags/json"},{"inline":true,"label":"Parquet","permalink":"/pyspark/tags/parquet"},{"inline":true,"label":"Avro","permalink":"/pyspark/tags/avro"},{"inline":true,"label":"Delta Lake","permalink":"/pyspark/tags/delta-lake"},{"inline":true,"label":"Performance Tuning","permalink":"/pyspark/tags/performance-tuning"},{"inline":true,"label":"Shuffle","permalink":"/pyspark/tags/shuffle"},{"inline":true,"label":"Narrow vs Wide Transformations","permalink":"/pyspark/tags/narrow-vs-wide-transformations"},{"inline":true,"label":"Spark UI","permalink":"/pyspark/tags/spark-ui"},{"inline":true,"label":"Catalyst Optimizer","permalink":"/pyspark/tags/catalyst-optimizer"},{"inline":true,"label":"Tungsten","permalink":"/pyspark/tags/tungsten"},{"inline":true,"label":"Repartition","permalink":"/pyspark/tags/repartition"},{"inline":true,"label":"Coalesce","permalink":"/pyspark/tags/coalesce"},{"inline":true,"label":"Broadcast Join","permalink":"/pyspark/tags/broadcast-join"},{"inline":true,"label":"Memory Management","permalink":"/pyspark/tags/memory-management"},{"inline":true,"label":"Structured Streaming","permalink":"/pyspark/tags/structured-streaming"},{"inline":true,"label":"Real-Time Data","permalink":"/pyspark/tags/real-time-data"},{"inline":true,"label":"Kafka","permalink":"/pyspark/tags/kafka"},{"inline":true,"label":"Streaming Sinks","permalink":"/pyspark/tags/streaming-sinks"},{"inline":true,"label":"Watermarking","permalink":"/pyspark/tags/watermarking"},{"inline":true,"label":"Checkpoints","permalink":"/pyspark/tags/checkpoints"},{"inline":true,"label":"MLlib","permalink":"/pyspark/tags/m-llib"},{"inline":true,"label":"Machine Learning","permalink":"/pyspark/tags/machine-learning"},{"inline":true,"label":"Regression","permalink":"/pyspark/tags/regression"},{"inline":true,"label":"Classification","permalink":"/pyspark/tags/classification"},{"inline":true,"label":"Clustering","permalink":"/pyspark/tags/clustering"},{"inline":true,"label":"Recommendation Systems","permalink":"/pyspark/tags/recommendation-systems"},{"inline":true,"label":"Feature Engineering","permalink":"/pyspark/tags/feature-engineering"},{"inline":true,"label":"Snowflake","permalink":"/pyspark/tags/snowflake"},{"inline":true,"label":"Hive","permalink":"/pyspark/tags/hive"},{"inline":true,"label":"Databricks","permalink":"/pyspark/tags/databricks"},{"inline":true,"label":"AWS EMR","permalink":"/pyspark/tags/aws-emr"},{"inline":true,"label":"GCP Dataproc","permalink":"/pyspark/tags/gcp-dataproc"},{"inline":true,"label":"Azure Synapse","permalink":"/pyspark/tags/azure-synapse"},{"inline":true,"label":"ETL","permalink":"/pyspark/tags/etl"},{"inline":true,"label":"Data Pipelines","permalink":"/pyspark/tags/data-pipelines"},{"inline":true,"label":"Data Processing","permalink":"/pyspark/tags/data-processing"},{"inline":true,"label":"Workflow Automation","permalink":"/pyspark/tags/workflow-automation"},{"inline":true,"label":"Data Quality","permalink":"/pyspark/tags/data-quality"},{"inline":true,"label":"Semi-Structured Data","permalink":"/pyspark/tags/semi-structured-data"},{"inline":true,"label":"Production Pipelines","permalink":"/pyspark/tags/production-pipelines"},{"inline":true,"label":"Use Cases","permalink":"/pyspark/tags/use-cases"}],"version":"current","frontMatter":{"id":"pyspark-mllib-overview","title":"MLlib Overview","sidebar_label":"MLlib","tags":["PySpark","Apache Spark","Big Data","Spark Basics","Cluster Computing","Spark Architecture","Driver Program","Executors","Cluster Manager","SparkSession","SparkContext","RDD","RDD Transformations","RDD Actions","Key-Value RDD","RDD Caching","DataFrame","DataFrame API","Column Operations","DataFrame Joins","Aggregations","GroupBy","Window Functions","Missing Data Handling","Spark SQL","Temp Views","Spark SQL Functions","UDF","UDAF","Explode","Arrays","StructType","Complex Data Types","Pivot","Unpivot","Join Optimization","Sorting","Sampling","Partitioning","Bucketing","CSV","JSON","Parquet","Avro","Delta Lake","Performance Tuning","Shuffle","Narrow vs Wide Transformations","Spark UI","Catalyst Optimizer","Tungsten","Repartition","Coalesce","Broadcast Join","Memory Management","Structured Streaming","Real-Time Data","Kafka","Streaming Sinks","Watermarking","Checkpoints","MLlib","Machine Learning","Regression","Classification","Clustering","Recommendation Systems","Feature Engineering","Snowflake","Hive","Databricks","AWS EMR","GCP Dataproc","Azure Synapse","ETL","Data Pipelines","Data Processing","Workflow Automation","Data Quality","Semi-Structured Data","Production Pipelines","Use Cases"]}}');var a=r(4848),s=r(8453);const l={id:"pyspark-mllib-overview",title:"MLlib Overview",sidebar_label:"MLlib",tags:["PySpark","Apache Spark","Big Data","Spark Basics","Cluster Computing","Spark Architecture","Driver Program","Executors","Cluster Manager","SparkSession","SparkContext","RDD","RDD Transformations","RDD Actions","Key-Value RDD","RDD Caching","DataFrame","DataFrame API","Column Operations","DataFrame Joins","Aggregations","GroupBy","Window Functions","Missing Data Handling","Spark SQL","Temp Views","Spark SQL Functions","UDF","UDAF","Explode","Arrays","StructType","Complex Data Types","Pivot","Unpivot","Join Optimization","Sorting","Sampling","Partitioning","Bucketing","CSV","JSON","Parquet","Avro","Delta Lake","Performance Tuning","Shuffle","Narrow vs Wide Transformations","Spark UI","Catalyst Optimizer","Tungsten","Repartition","Coalesce","Broadcast Join","Memory Management","Structured Streaming","Real-Time Data","Kafka","Streaming Sinks","Watermarking","Checkpoints","MLlib","Machine Learning","Regression","Classification","Clustering","Recommendation Systems","Feature Engineering","Snowflake","Hive","Databricks","AWS EMR","GCP Dataproc","Azure Synapse","ETL","Data Pipelines","Data Processing","Workflow Automation","Data Quality","Semi-Structured Data","Production Pipelines","Use Cases"]},t=void 0,o={},p=[{value:"1\ufe0f\u20e3 Why MLlib Matters",id:"1\ufe0f\u20e3-why-mllib-matters",level:2},{value:"2\ufe0f\u20e3 Key Components of MLlib",id:"2\ufe0f\u20e3-key-components-of-mllib",level:2},{value:"3\ufe0f\u20e3 Feature Engineering in PySpark",id:"3\ufe0f\u20e3-feature-engineering-in-pyspark",level:2},{value:"Example: Preparing Features",id:"example-preparing-features",level:3},{value:"4\ufe0f\u20e3 Pipelines &amp; Transformers",id:"4\ufe0f\u20e3-pipelines--transformers",level:2},{value:"Example: Building a Simple Pipeline",id:"example-building-a-simple-pipeline",level:3},{value:"5\ufe0f\u20e3 Classification Models",id:"5\ufe0f\u20e3-classification-models",level:2},{value:"Example: Logistic Regression",id:"example-logistic-regression",level:3},{value:"6\ufe0f\u20e3 Clustering Models (KMeans)",id:"6\ufe0f\u20e3-clustering-models-kmeans",level:2},{value:"Example: KMeans",id:"example-kmeans",level:3},{value:"7\ufe0f\u20e3 \ud83d\udd11 1-Minute Summary",id:"7\ufe0f\u20e3--1-minute-summary",level:2}];function c(e){const n={code:"code",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(n.p,{children:["Imagine you\u2019re a ",(0,a.jsx)(n.strong,{children:"data scientist in a high-tech lab"}),", not just a data engineer. Data isn\u2019t sitting quietly in files\u2014it\u2019s ",(0,a.jsx)(n.strong,{children:"streaming, growing, and changing constantly"}),". You want to ",(0,a.jsx)(n.strong,{children:"predict outcomes, classify users, or group behaviors"}),", all at ",(0,a.jsx)(n.strong,{children:"scale"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["PySpark\u2019s ",(0,a.jsx)(n.strong,{children:"MLlib"})," is your ",(0,a.jsx)(n.strong,{children:"distributed machine learning toolkit"}),". It\u2019s built to handle ",(0,a.jsx)(n.strong,{children:"millions of rows"})," without slowing down your workflow, combining ",(0,a.jsx)(n.strong,{children:"ease-of-use with Spark\u2019s power"}),"."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"1\ufe0f\u20e3-why-mllib-matters",children:"1\ufe0f\u20e3 Why MLlib Matters"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["MLlib provides ",(0,a.jsx)(n.strong,{children:"high-level, Spark-native APIs"})," for machine learning workflows."]}),"\n",(0,a.jsxs)(n.li,{children:["Seamlessly integrates with ",(0,a.jsx)(n.strong,{children:"PySpark DataFrames"}),", so you don\u2019t need to leave Spark for ML."]}),"\n",(0,a.jsxs)(n.li,{children:["Optimized for ",(0,a.jsx)(n.strong,{children:"distributed computing"}),"\u2014big datasets won\u2019t crash your laptop."]}),"\n",(0,a.jsxs)(n.li,{children:["Covers ",(0,a.jsx)(n.strong,{children:"feature engineering, pipelines, regression, classification, and clustering"}),"."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Think of MLlib like this:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Feature Engineering"})," \u2192 Lab instruments to refine raw data"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Pipelines"})," \u2192 Automated assembly lines to transform and train models"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Models"})," \u2192 Predictive engines to extract insights"]}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"2\ufe0f\u20e3-key-components-of-mllib",children:"2\ufe0f\u20e3 Key Components of MLlib"}),"\n",(0,a.jsx)(n.p,{children:"MLlib modules include:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Feature Engineering"})," \u2192 Transform raw data into meaningful features (",(0,a.jsx)(n.code,{children:"VectorAssembler"}),", ",(0,a.jsx)(n.code,{children:"StandardScaler"}),")"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Transformers & Pipelines"})," \u2192 Chain transformations and models (",(0,a.jsx)(n.code,{children:"Pipeline"}),", ",(0,a.jsx)(n.code,{children:"PipelineModel"}),")"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Regression Models"})," \u2192 Predict continuous values (",(0,a.jsx)(n.code,{children:"LinearRegression"}),", ",(0,a.jsx)(n.code,{children:"DecisionTreeRegressor"}),")"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Classification Models"})," \u2192 Predict categories (",(0,a.jsx)(n.code,{children:"LogisticRegression"}),", ",(0,a.jsx)(n.code,{children:"RandomForestClassifier"}),")"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Clustering Models"})," \u2192 Group similar data points (",(0,a.jsx)(n.code,{children:"KMeans"}),", ",(0,a.jsx)(n.code,{children:"BisectingKMeans"}),")"]}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"3\ufe0f\u20e3-feature-engineering-in-pyspark",children:"3\ufe0f\u20e3 Feature Engineering in PySpark"}),"\n",(0,a.jsxs)(n.p,{children:["Before training a model, you must ",(0,a.jsx)(n.strong,{children:"prepare your data"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Handle missing values"}),"\n",(0,a.jsx)(n.li,{children:"Convert categorical columns into numeric representations"}),"\n",(0,a.jsx)(n.li,{children:"Scale or normalize features for consistent ranges"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"example-preparing-features",children:"Example: Preparing Features"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from pyspark.ml.feature import VectorAssembler, StandardScaler\r\nfrom pyspark.sql import SparkSession\r\n\r\nspark = SparkSession.builder.appName("MLlib Feature Engineering").getOrCreate()\r\n\r\n# Load sample dataset\r\ndf = spark.read.csv("data/sales.csv", header=True, inferSchema=True)\r\n\r\n# Step 1: Combine numeric feature columns into a single vector\r\nassembler = VectorAssembler(\r\n    inputCols=["amount", "quantity", "discount"],\r\n    outputCol="features"\r\n)\r\ndf_features = assembler.transform(df)\r\n\r\n# Step 2: Scale features\r\nscaler = StandardScaler(inputCol="features", outputCol="scaled_features")\r\nscaler_model = scaler.fit(df_features)\r\ndf_scaled = scaler_model.transform(df_features)\r\n\r\ndf_scaled.select("features", "scaled_features").show(5)\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Explanation for beginners:"})}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"VectorAssembler"})," \u2192 Combines multiple numeric columns into one ",(0,a.jsx)(n.strong,{children:"feature vector"})," Spark ML can understand."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"StandardScaler"})," \u2192 Scales features to a ",(0,a.jsx)(n.strong,{children:"standard range"}),", crucial for algorithms sensitive to scale (like Logistic Regression, KMeans)."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"fit()"})," \u2192 Learns scaling parameters (mean and variance) from the data."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"transform()"})," \u2192 Applies the learned scaling to the dataset."]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["\ud83d\udca1 ",(0,a.jsx)(n.strong,{children:"Pro tip:"})," Always check for missing or inconsistent data before assembling features."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"4\ufe0f\u20e3-pipelines--transformers",children:"4\ufe0f\u20e3 Pipelines & Transformers"}),"\n",(0,a.jsxs)(n.p,{children:["Pipelines automate ML workflows: you can ",(0,a.jsx)(n.strong,{children:"chain feature transformations and models"})," together in one object. This makes experiments ",(0,a.jsx)(n.strong,{children:"reproducible and production-ready"}),"."]}),"\n",(0,a.jsx)(n.h3,{id:"example-building-a-simple-pipeline",children:"Example: Building a Simple Pipeline"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from pyspark.ml import Pipeline\r\nfrom pyspark.ml.regression import LinearRegression\r\n\r\n# Step 1: Define stages\r\nlr = LinearRegression(featuresCol="scaled_features", labelCol="amount")\r\npipeline = Pipeline(stages=[assembler, scaler, lr])\r\n\r\n# Step 2: Train pipeline\r\nmodel = pipeline.fit(df)\r\n\r\n# Step 3: Make predictions\r\npredictions = model.transform(df)\r\npredictions.select("amount", "prediction").show(5)\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Explanation:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Stages"})," \u2192 Each stage is either a ",(0,a.jsx)(n.strong,{children:"transformer"})," (data preprocessing) or an ",(0,a.jsx)(n.strong,{children:"estimator"})," (model training)."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"fit()"})," \u2192 Learns all transformations and model parameters."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"transform()"})," \u2192 Applies all transformations and generates predictions."]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["\ud83d\udca1 ",(0,a.jsx)(n.strong,{children:"Pro tip:"})," Pipelines reduce errors and ensure your workflow is ",(0,a.jsx)(n.strong,{children:"reusable"}),"."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"5\ufe0f\u20e3-classification-models",children:"5\ufe0f\u20e3 Classification Models"}),"\n",(0,a.jsxs)(n.p,{children:["Classification predicts ",(0,a.jsx)(n.strong,{children:"categories"}),", like customer segments or churn probability."]}),"\n",(0,a.jsx)(n.h3,{id:"example-logistic-regression",children:"Example: Logistic Regression"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from pyspark.ml.classification import LogisticRegression\r\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler\r\nfrom pyspark.sql import SparkSession\r\nfrom pyspark.sql.types import DoubleType\r\n\r\nspark = SparkSession.builder.appName("Classification").getOrCreate()\r\n\r\n# Assuming df_scaled from previous steps\r\nlr = LogisticRegression(featuresCol="scaled_features", labelCol="label")\r\nlr_model = lr.fit(df_scaled)\r\npredictions = lr_model.transform(df_scaled)\r\npredictions.select("label", "prediction", "probability").show(5)\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Key points:"})}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"labelCol"})," \u2192 Column containing ",(0,a.jsx)(n.strong,{children:"true categories"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"featuresCol"})," \u2192 Column containing ",(0,a.jsx)(n.strong,{children:"input features"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"prediction"})," \u2192 Model\u2019s predicted category."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"probability"})," \u2192 Probability for each class."]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["\ud83d\udca1 ",(0,a.jsx)(n.strong,{children:"Pro tip:"})," Always ",(0,a.jsx)(n.strong,{children:"scale features"})," for gradient-based algorithms for faster convergence."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"6\ufe0f\u20e3-clustering-models-kmeans",children:"6\ufe0f\u20e3 Clustering Models (KMeans)"}),"\n",(0,a.jsxs)(n.p,{children:["Clustering groups data points ",(0,a.jsx)(n.strong,{children:"without labels"})," (unsupervised learning)."]}),"\n",(0,a.jsx)(n.h3,{id:"example-kmeans",children:"Example: KMeans"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from pyspark.ml.clustering import KMeans\r\n\r\nkmeans = KMeans(featuresCol="scaled_features", k=3, seed=42)\r\nkmeans_model = kmeans.fit(df_scaled)\r\nclusters = kmeans_model.transform(df_scaled)\r\nclusters.select("features", "prediction").show(5)\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Explanation:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"k"})," \u2192 Number of clusters."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"prediction"})," \u2192 Cluster ID assigned to each row."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"fit()"})," \u2192 Learns cluster centers from the data."]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["\ud83d\udca1 ",(0,a.jsx)(n.strong,{children:"Pro tip:"})," Use the ",(0,a.jsx)(n.strong,{children:"Elbow Method"})," or ",(0,a.jsx)(n.strong,{children:"Silhouette Score"})," to choose the optimal number of clusters."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"7\ufe0f\u20e3--1-minute-summary",children:"7\ufe0f\u20e3 \ud83d\udd11 1-Minute Summary"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"MLlib"})," \u2192 Scalable, Spark-native machine learning library."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Feature Engineering"})," \u2192 Prepare data with ",(0,a.jsx)(n.code,{children:"VectorAssembler"})," & ",(0,a.jsx)(n.code,{children:"StandardScaler"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Pipelines"})," \u2192 Automate workflows for reproducible ML."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Regression & Classification"})," \u2192 Predict continuous values or categories."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Clustering"})," \u2192 Group similar data points for insights."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Rule of Thumb:"})," Clean, encode, and scale your data before modeling for best results."]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>l,x:()=>t});var i=r(6540);const a={},s=i.createContext(a);function l(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:l(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);