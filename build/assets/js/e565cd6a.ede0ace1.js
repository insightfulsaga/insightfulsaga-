"use strict";(self.webpackChunkdatacraft_school=self.webpackChunkdatacraft_school||[]).push([[9248],{6396:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>t,metadata:()=>s,toc:()=>p});const s=JSON.parse('{"id":"pyspark-interview-questions-part6","title":"Essential PySpark Interview Question & Answer(Explained Through Real-World Stories) \u2013 Part 6","description":"46. What are accumulators and broadcast variables?","source":"@site/docs-pyspark/pyspark-interview-questions-part6.md","sourceDirName":".","slug":"/pyspark-interview-questions-part6","permalink":"/pyspark/pyspark-interview-questions-part6","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"pyspark-intro","permalink":"/pyspark/tags/pyspark-intro"},{"inline":true,"label":"pyspark dataframe basics","permalink":"/pyspark/tags/pyspark-dataframe-basics"},{"inline":true,"label":"pyspark dataframe basics2","permalink":"/pyspark/tags/pyspark-dataframe-basics-2"},{"inline":true,"label":"pyspark filtering","permalink":"/pyspark/tags/pyspark-filtering"},{"inline":true,"label":"pyspark aggregation","permalink":"/pyspark/tags/pyspark-aggregation"},{"inline":true,"label":"pyspark missing","permalink":"/pyspark/tags/pyspark-missing"},{"inline":true,"label":"pyspark dates","permalink":"/pyspark/tags/pyspark-dates"},{"inline":true,"label":"pyspark joins","permalink":"/pyspark/tags/pyspark-joins"},{"inline":true,"label":"pyspark one liners","permalink":"/pyspark/tags/pyspark-one-liners"},{"inline":true,"label":"linear regression model","permalink":"/pyspark/tags/linear-regression-model"},{"inline":true,"label":"linear regression math","permalink":"/pyspark/tags/linear-regression-math"},{"inline":true,"label":"house price linear regression","permalink":"/pyspark/tags/house-price-linear-regression"},{"inline":true,"label":"logistic regression model","permalink":"/pyspark/tags/logistic-regression-model"},{"inline":true,"label":"logistic regression query","permalink":"/pyspark/tags/logistic-regression-query"},{"inline":true,"label":"logistic regression mini project","permalink":"/pyspark/tags/logistic-regression-mini-project"},{"inline":true,"label":"pyspark-interview-questions-part1","permalink":"/pyspark/tags/pyspark-interview-questions-part-1"},{"inline":true,"label":"pyspark-interview-questions-part2","permalink":"/pyspark/tags/pyspark-interview-questions-part-2"},{"inline":true,"label":"pyspark-interview-questions-part3","permalink":"/pyspark/tags/pyspark-interview-questions-part-3"},{"inline":true,"label":"pyspark-interview-questions-part4","permalink":"/pyspark/tags/pyspark-interview-questions-part-4"},{"inline":true,"label":"pyspark-interview-questions-part5","permalink":"/pyspark/tags/pyspark-interview-questions-part-5"}],"version":"current","sidebarPosition":6,"frontMatter":{"id":"pyspark-interview-questions-part6","title":"Essential PySpark Interview Question & Answer(Explained Through Real-World Stories) \u2013 Part 6","sidebar_label":"PySpark Interview Q&A(Story-Based) - 6","tags":["pyspark-intro","pyspark dataframe basics","pyspark dataframe basics2","pyspark filtering","pyspark aggregation","pyspark missing","pyspark dates","pyspark joins","pyspark one liners","linear regression model","linear regression math","house price linear regression","logistic regression model","logistic regression query","logistic regression mini project","pyspark-interview-questions-part1","pyspark-interview-questions-part2","pyspark-interview-questions-part3","pyspark-interview-questions-part4","pyspark-interview-questions-part5"],"keywords":["pyspark-intro","pyspark dataframe basics","pyspark dataframe basics2","pyspark filtering","pyspark aggregation","pyspark missing","pyspark dates","pyspark joins","pyspark one liners","linear regression model","linear regression math","house price linear regression","logistic regression model","logistic regression query","logistic regression mini project","pyspark-interview-questions-part1","pyspark-interview-questions-part2","pyspark-interview-questions-part3","pyspark-interview-questions-part4","pyspark-interview-questions-part5"],"sidebar_position":6}}');var a=n(4848),i=n(8453);const t={id:"pyspark-interview-questions-part6",title:"Essential PySpark Interview Question & Answer(Explained Through Real-World Stories) \u2013 Part 6",sidebar_label:"PySpark Interview Q&A(Story-Based) - 6",tags:["pyspark-intro","pyspark dataframe basics","pyspark dataframe basics2","pyspark filtering","pyspark aggregation","pyspark missing","pyspark dates","pyspark joins","pyspark one liners","linear regression model","linear regression math","house price linear regression","logistic regression model","logistic regression query","logistic regression mini project","pyspark-interview-questions-part1","pyspark-interview-questions-part2","pyspark-interview-questions-part3","pyspark-interview-questions-part4","pyspark-interview-questions-part5"],keywords:["pyspark-intro","pyspark dataframe basics","pyspark dataframe basics2","pyspark filtering","pyspark aggregation","pyspark missing","pyspark dates","pyspark joins","pyspark one liners","linear regression model","linear regression math","house price linear regression","logistic regression model","logistic regression query","logistic regression mini project","pyspark-interview-questions-part1","pyspark-interview-questions-part2","pyspark-interview-questions-part3","pyspark-interview-questions-part4","pyspark-interview-questions-part5"],sidebar_position:6},o=void 0,l={},p=[{value:"<strong>46. What are accumulators and broadcast variables?</strong>",id:"46-what-are-accumulators-and-broadcast-variables",level:3},{value:"<strong>47. How do you handle performance issues in PySpark?</strong>",id:"47-how-do-you-handle-performance-issues-in-pyspark",level:3},{value:"<strong>48. How do you work with complex nested JSON structures?</strong>",id:"48-how-do-you-work-with-complex-nested-json-structures",level:3},{value:"<strong>49. How do you convert between RDDs and DataFrames?</strong>",id:"49-how-do-you-convert-between-rdds-and-dataframes",level:3},{value:"<strong>50. How does PySpark integrate with Hadoop/HDFS?</strong>",id:"50-how-does-pyspark-integrate-with-hadoophdfs",level:3},{value:"<strong>51. Explain the difference between <code>map</code> and <code>flatMap</code>.</strong>",id:"51-explain-the-difference-between-map-and-flatmap",level:3},{value:"<strong>52. What are the limitations of PySpark compared to Spark with Scala?</strong>",id:"52-what-are-the-limitations-of-pyspark-compared-to-spark-with-scala",level:3},{value:"<strong>53. How do you implement custom partitioning in PySpark?</strong>",id:"53-how-do-you-implement-custom-partitioning-in-pyspark",level:3},{value:"<strong>54. How do you integrate PySpark with Delta Lake?</strong>",id:"54-how-do-you-integrate-pyspark-with-delta-lake",level:3},{value:"<strong>55. Explain checkpointing in PySpark and when it is used.</strong>",id:"55-explain-checkpointing-in-pyspark-and-when-it-is-used",level:3}];function d(e){const r={code:"code",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(r.h3,{id:"46-what-are-accumulators-and-broadcast-variables",children:(0,a.jsx)(r.strong,{children:"46. What are accumulators and broadcast variables?"})}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Story/Modern Tech Analogy:"})}),"\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"Accumulator"}),": Like a suggestion box in an office\u2014everyone can add to it, but only the manager (driver) reads the total."]}),"\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"Broadcast variable"}),": Like sending everyone a shared handbook\u2014distributed efficiently so everyone has a copy without repeated shipping."]}),"\n"]}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Professional Explanation:"})}),"\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"Accumulators"}),": Variables that workers can increment in parallel; only the driver can read their final value. Useful for counters or metrics."]}),"\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"Broadcast variables"}),": Read-only variables sent to all executors efficiently to avoid large data transfer in tasks (e.g., small lookup tables)."]}),"\n"]}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Example:"})}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"from pyspark.sql import SparkSession\r\nfrom pyspark import SparkContext\r\n\r\nsc = SparkContext.getOrCreate()\r\nacc = sc.accumulator(0)\r\nbc_var = sc.broadcast([1,2,3])\r\n\r\nrdd = sc.parallelize([1,2,3,4])\r\nrdd.foreach(lambda x: acc.add(x))\r\nprint(acc.value)  # 10\r\nprint(bc_var.value)  # [1, 2, 3]\n"})}),"\n",(0,a.jsx)(r.hr,{}),"\n",(0,a.jsx)(r.h3,{id:"47-how-do-you-handle-performance-issues-in-pyspark",children:(0,a.jsx)(r.strong,{children:"47. How do you handle performance issues in PySpark?"})}),"\n",(0,a.jsxs)(r.p,{children:[(0,a.jsx)(r.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nPerformance tuning is like traffic management: reduce congestion, balance load, and ensure smooth flow."]}),"\n",(0,a.jsxs)(r.p,{children:[(0,a.jsx)(r.strong,{children:"Professional Explanation:"}),"\r\nCommon approaches:"]}),"\n",(0,a.jsxs)(r.ol,{children:["\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"Reduce shuffles"}),": Avoid wide transformations if possible."]}),"\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"Partition tuning"}),": Ensure even distribution; avoid skew."]}),"\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"Cache/persist"}),": For repeated computations."]}),"\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"Broadcast small tables"}),": Use ",(0,a.jsx)(r.code,{children:"broadcast"})," for joins."]}),"\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"Use vectorized operations"}),": Pandas UDFs for efficiency."]}),"\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"Avoid unnecessary actions"}),": Trigger actions only when needed."]}),"\n"]}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Example:"})}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:'from pyspark.sql.functions import broadcast\r\n\r\ndf_large.join(broadcast(df_small), "id")  # Reduce shuffle\n'})}),"\n",(0,a.jsx)(r.hr,{}),"\n",(0,a.jsx)(r.h3,{id:"48-how-do-you-work-with-complex-nested-json-structures",children:(0,a.jsx)(r.strong,{children:"48. How do you work with complex nested JSON structures?"})}),"\n",(0,a.jsxs)(r.p,{children:[(0,a.jsx)(r.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nNested JSON is like Russian nesting dolls; you need to open each layer to reach the data inside."]}),"\n",(0,a.jsxs)(r.p,{children:[(0,a.jsx)(r.strong,{children:"Professional Explanation:"}),"\r\nUse ",(0,a.jsx)(r.code,{children:"from_json"}),", ",(0,a.jsx)(r.code,{children:"explode"}),", and ",(0,a.jsx)(r.code,{children:'col("struct.field")'})," to parse, flatten, and transform nested JSON. Spark\u2019s schema-on-read allows you to define the structure for efficient parsing."]}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Example:"})}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:'from pyspark.sql.functions import from_json, col, explode\r\nfrom pyspark.sql.types import StructType, ArrayType, StringType\r\n\r\nschema = StructType().add("name", StringType()).add("skills", ArrayType(StringType()))\r\ndf_parsed = df.withColumn("json_data", from_json(col("json_col"), schema))\r\ndf_parsed.select("json_data.name", explode("json_data.skills").alias("skill")).show()\n'})}),"\n",(0,a.jsx)(r.hr,{}),"\n",(0,a.jsx)(r.h3,{id:"49-how-do-you-convert-between-rdds-and-dataframes",children:(0,a.jsx)(r.strong,{children:"49. How do you convert between RDDs and DataFrames?"})}),"\n",(0,a.jsxs)(r.p,{children:[(0,a.jsx)(r.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nRDDs are like raw ingredients; DataFrames are like a plated dish ready for analysis. You can move between raw and structured forms depending on your need."]}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Professional Explanation:"})}),"\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"RDD \u2192 DataFrame"}),": Use ",(0,a.jsx)(r.code,{children:"toDF()"})," with or without a schema."]}),"\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"DataFrame \u2192 RDD"}),": Use ",(0,a.jsx)(r.code,{children:".rdd"})," to access the underlying RDD for low-level operations."]}),"\n"]}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Example:"})}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:'rdd = sc.parallelize([(1, "Alice"), (2, "Bob")])\r\ndf = rdd.toDF(["id", "name"])\r\nrdd2 = df.rdd\n'})}),"\n",(0,a.jsx)(r.hr,{}),"\n",(0,a.jsx)(r.h3,{id:"50-how-does-pyspark-integrate-with-hadoophdfs",children:(0,a.jsx)(r.strong,{children:"50. How does PySpark integrate with Hadoop/HDFS?"})}),"\n",(0,a.jsxs)(r.p,{children:[(0,a.jsx)(r.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nSpark is like a powerful car, HDFS is the highway system. Spark reads and writes directly to HDFS efficiently using distributed I/O."]}),"\n",(0,a.jsxs)(r.p,{children:[(0,a.jsx)(r.strong,{children:"Professional Explanation:"}),"\r\nPySpark supports ",(0,a.jsx)(r.strong,{children:"HDFS natively"}),", allowing you to read/write CSV, Parquet, JSON, ORC, etc., using paths like ",(0,a.jsx)(r.code,{children:"hdfs://namenode:port/path"}),". Spark handles distributed access and parallel I/O seamlessly."]}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Example:"})}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:'df = spark.read.csv("hdfs://namenode:9000/data/input.csv")\r\ndf.write.parquet("hdfs://namenode:9000/data/output.parquet")\n'})}),"\n",(0,a.jsx)(r.hr,{}),"\n",(0,a.jsx)(r.h3,{id:"51-explain-the-difference-between-map-and-flatmap",children:(0,a.jsxs)(r.strong,{children:["51. Explain the difference between ",(0,a.jsx)(r.code,{children:"map"})," and ",(0,a.jsx)(r.code,{children:"flatMap"}),"."]})}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Story/Modern Tech Analogy:"})}),"\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.code,{children:"map"}),": Like making one sandwich per order."]}),"\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.code,{children:"flatMap"}),": Like making multiple sandwiches per order and flattening them onto one tray."]}),"\n"]}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Professional Explanation:"})}),"\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"map"}),": One-to-one transformation; each input produces one output element."]}),"\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"flatMap"}),": One-to-many transformation; each input can produce multiple output elements (flattened into one RDD)."]}),"\n"]}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Example:"})}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"rdd = sc.parallelize([\"hello world\", \"pyspark\"])\r\nrdd.map(lambda x: x.split()).collect()     # [['hello', 'world'], ['pyspark']]\r\nrdd.flatMap(lambda x: x.split()).collect() # ['hello', 'world', 'pyspark']\n"})}),"\n",(0,a.jsx)(r.hr,{}),"\n",(0,a.jsx)(r.h3,{id:"52-what-are-the-limitations-of-pyspark-compared-to-spark-with-scala",children:(0,a.jsx)(r.strong,{children:"52. What are the limitations of PySpark compared to Spark with Scala?"})}),"\n",(0,a.jsxs)(r.p,{children:[(0,a.jsx)(r.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nPySpark is like a translated manual\u2014most features are there, but some instructions may be slightly slower or less native."]}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Professional Explanation:"})}),"\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsxs)(r.li,{children:["Python serialization (Pickle) introduces ",(0,a.jsx)(r.strong,{children:"overhead"}),"."]}),"\n",(0,a.jsxs)(r.li,{children:["Some ",(0,a.jsx)(r.strong,{children:"low-level Spark APIs"})," are only available in Scala."]}),"\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"Performance"})," may be slightly slower than Scala for very tight loops or heavy transformations."]}),"\n",(0,a.jsxs)(r.li,{children:["Some ",(0,a.jsx)(r.strong,{children:"advanced Spark features"})," (like certain Catalyst optimizations) are first-class in Scala."]}),"\n"]}),"\n",(0,a.jsx)(r.hr,{}),"\n",(0,a.jsx)(r.h3,{id:"53-how-do-you-implement-custom-partitioning-in-pyspark",children:(0,a.jsx)(r.strong,{children:"53. How do you implement custom partitioning in PySpark?"})}),"\n",(0,a.jsxs)(r.p,{children:[(0,a.jsx)(r.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nCustom partitioning is like assigning postal codes to parcels\u2014ensuring each delivery truck gets a balanced load."]}),"\n",(0,a.jsxs)(r.p,{children:[(0,a.jsx)(r.strong,{children:"Professional Explanation:"}),"\r\nYou can define a ",(0,a.jsx)(r.strong,{children:"custom Partitioner"})," when using RDDs or repartition a DataFrame using a column. Custom partitioning helps reduce skew and shuffle in joins and aggregations."]}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Example (RDD):"})}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"from pyspark import Partitioner\r\n\r\nclass MyPartitioner(Partitioner):\r\n    def __init__(self, num_parts):\r\n        self.num_parts = num_parts\r\n    def numPartitions(self):\r\n        return self.num_parts\r\n    def getPartition(self, key):\r\n        return hash(key) % self.num_parts\n"})}),"\n",(0,a.jsx)(r.hr,{}),"\n",(0,a.jsx)(r.h3,{id:"54-how-do-you-integrate-pyspark-with-delta-lake",children:(0,a.jsx)(r.strong,{children:"54. How do you integrate PySpark with Delta Lake?"})}),"\n",(0,a.jsxs)(r.p,{children:[(0,a.jsx)(r.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nDelta Lake is like adding a supercharged ledger to your warehouse\u2014it keeps a history, allows ACID transactions, and ensures consistency."]}),"\n",(0,a.jsxs)(r.p,{children:[(0,a.jsx)(r.strong,{children:"Professional Explanation:"}),"\r\nDelta Lake provides ",(0,a.jsx)(r.strong,{children:"ACID transactions"}),", ",(0,a.jsx)(r.strong,{children:"time travel"}),", and ",(0,a.jsx)(r.strong,{children:"schema enforcement"})," on Spark tables. PySpark can read/write Delta using the ",(0,a.jsx)(r.code,{children:"delta"})," format."]}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Example:"})}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:'df.write.format("delta").mode("overwrite").save("/delta/table")\r\ndf_delta = spark.read.format("delta").load("/delta/table")\n'})}),"\n",(0,a.jsx)(r.hr,{}),"\n",(0,a.jsx)(r.h3,{id:"55-explain-checkpointing-in-pyspark-and-when-it-is-used",children:(0,a.jsx)(r.strong,{children:"55. Explain checkpointing in PySpark and when it is used."})}),"\n",(0,a.jsxs)(r.p,{children:[(0,a.jsx)(r.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nCheckpointing is like saving your game progress\u2014if something crashes, you can resume without starting over."]}),"\n",(0,a.jsxs)(r.p,{children:[(0,a.jsx)(r.strong,{children:"Professional Explanation:"}),"\r\nCheckpointing saves RDDs/DataFrames to ",(0,a.jsx)(r.strong,{children:"reliable storage"})," (HDFS or S3) to ",(0,a.jsx)(r.strong,{children:"truncate lineage"}),". It is used to:"]}),"\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsx)(r.li,{children:"Break long dependency chains to prevent stack overflow."}),"\n",(0,a.jsx)(r.li,{children:"Ensure recovery from failures for long-running jobs."}),"\n"]}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Example:"})}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:'sc.setCheckpointDir("/checkpoint")\r\nrdd = rdd.checkpoint()\r\nrdd.count()  # triggers actual checkpoint\n'})})]})}function c(e={}){const{wrapper:r}={...(0,i.R)(),...e.components};return r?(0,a.jsx)(r,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,r,n)=>{n.d(r,{R:()=>t,x:()=>o});var s=n(6540);const a={},i=s.createContext(a);function t(e){const r=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function o(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),s.createElement(i.Provider,{value:r},e.children)}}}]);