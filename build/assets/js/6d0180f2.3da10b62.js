"use strict";(self.webpackChunkdatacraft_school=self.webpackChunkdatacraft_school||[]).push([[5524],{491:a=>{a.exports=JSON.parse('{"tag":{"label":"Spark Architecture","permalink":"/pyspark/tags/spark-architecture","allTagsPath":"/pyspark/tags","count":23,"items":[{"id":"df-aggregations","title":"Aggregations & GroupBy \u2014 Sum, Count, Avg, Max & Min","description":"Learn how to perform aggregations in PySpark using groupBy, sum, count, average, max, and min functions with practical Databricks examples.","permalink":"/pyspark/df-aggregations"},{"id":"complex-sql","title":"Complex SQL Queries in PySpark","description":"Learn how to write complex SQL queries in PySpark, including joins, subqueries, aggregations, and window functions for advanced analytics in Databricks.","permalink":"/pyspark/complex-sql"},{"id":"df-create-csv","title":"Creating DataFrames from CSV, JSON, Parquet & Hive Tables","description":"Learn how to create PySpark DataFrames from CSV, JSON, Parquet files, and Hive tables using Databricks and Spark best practices.","permalink":"/pyspark/df-create-csv"},{"id":"pyspark-aggregation","title":"Data Aggregation in PySpark DataFrames (Complete Guide)","description":"Learn how to perform data aggregation in PySpark using groupBy, agg, max, sum, avg, distinct, and sorting operations with real shipment dataset examples.","permalink":"/pyspark/pyspark/aggregation"},{"id":"df-api","title":"DataFrame API \u2014 Select, Filter, WithColumn & Drop","description":"Learn how to use PySpark DataFrame API operations such as select, filter, withColumn, and drop through practical Databricks examples and real-world use cases.","permalink":"/pyspark/df-api"},{"id":"pyspark-first-job","title":"First PySpark Job \u2014 Hello World Example","description":"Learn how to write and run your first PySpark job with a hands-on \u201cHello World\u201d example, and understand the end-to-end workflow in Spark.","permalink":"/pyspark/pyspark-first-job"},{"id":"df-missing-data","title":"Handling Missing Data \u2014 Drop, Fill & Replace","description":"Learn how to handle missing data in PySpark DataFrames using drop, fill, and replace operations with practical Databricks examples.","permalink":"/pyspark/df-missing-data"},{"id":"pyspark-missing","title":"Handling Missing Data in PySpark DataFrames (Complete Guide)","description":"Learn all techniques for handling missing or null data in PySpark DataFrames including dropping nulls, filling values, conditional replacement, and computing statistics.","permalink":"/pyspark/pyspark/missing-data"},{"id":"pyspark-installation","title":"Installing PySpark & Setting Up Environment","description":"Step-by-step guide to install PySpark, set up your development environment, and run your first Spark job for big data processing.","permalink":"/pyspark/pyspark-installation"},{"id":"pyspark-intro","title":"Introduction to PySpark \u2014 Why Spark & Big Data","description":"Learn why PySpark is a leading framework for big data processing, its importance in modern data engineering, and how it enables fast, scalable analytics.","permalink":"/pyspark/pyspark-intro"},{"id":"df-joins","title":"Joins in PySpark DataFrames (Full Beginner Guide)","description":"Learn all types of joins in PySpark DataFrames \u2014 inner, left, right, outer, semi, anti, and cross join with clear examples, code, and explanations.","permalink":"/pyspark/pyspark/joins"},{"id":"rdd-key-value","title":"Key-Value RDDs \u2014 reduceByKey, groupByKey & aggregate","description":"Learn how to use key-value RDDs in Spark with reduceByKey, groupByKey, and aggregate operations, complete with real-world Databricks examples and performance tips.","permalink":"/pyspark/rdd-key-value"},{"id":"pyspark-mllib-overview","title":"MLlib Overview","description":"Imagine you\u2019re a data scientist in a high-tech lab, not just a data engineer. Data isn\u2019t sitting quietly in files\u2014it\u2019s streaming, growing, and changing constantly. You want to predict outcomes, classify users, or group behaviors, all at scale.","permalink":"/pyspark/pyspark-mllib-overview"},{"id":"df-vs-spark-sql","title":"Performance Comparison \u2014 DataFrame API vs Spark SQL","description":"Understand the performance differences between PySpark DataFrame API and Spark SQL, with tips on when to use each approach for optimal performance in Databricks.","permalink":"/pyspark/df-vs-spark-sql"},{"id":"pyspark-architecture","title":"PySpark Architecture \u2014 Driver, Executor, and Cluster Modes","description":"Understand the PySpark architecture, including Driver, Executor, and cluster modes, to efficiently design distributed data processing workflows.","permalink":"/pyspark/pyspark-architecture"},{"id":"rdd-basics","title":"RDD Basics \u2014 Creation, Transformation & Actions","description":"Learn the fundamentals of RDDs in Apache Spark, including how to create them, apply transformations, trigger actions, and understand their importance in distributed data processing.","permalink":"/pyspark/rdd-basics"},{"id":"rdd-persistence-caching","title":"RDD Persistence & Caching \u2014 Memory Management in Spark","description":"Learn how Spark RDD caching and persistence work, why they matter for performance, and how to manage memory effectively in distributed data pipelines.","permalink":"/pyspark/rdd-persistence-caching"},{"id":"pyspark-rdd-vs-dataframe","title":"RDDs vs DataFrames vs Datasets \u2014 When to Use","description":"Understand the differences between RDDs, DataFrames, and Datasets in PySpark, and learn when to use each for efficient big data processing.","permalink":"/pyspark/pyspark-rdd-vs-dataframe"},{"id":"pyspark-spark-session-context","title":"SparkSession, SparkContext, and Configuration Basics","description":"Learn the core components of PySpark\u2014SparkSession, SparkContext, and configurations\u2014and how they form the foundation of big data processing.","permalink":"/pyspark/pyspark-spark-session-context"},{"id":"udfs-udafs","title":"UDFs & UDAFs \u2014 Custom Functions in SQL","description":"Learn how to create and use PySpark UDFs (User Defined Functions) and UDAFs (User Defined Aggregate Functions) to implement custom logic and aggregations in Spark SQL and DataFrames.","permalink":"/pyspark/udfs-udafs"},{"id":"spark-sql","title":"Using Spark SQL \u2014 Register Temp Views and Query","description":"Learn how to use Spark SQL in PySpark by registering temporary views and running SQL queries on DataFrames in Databricks.","permalink":"/pyspark/spark-sql"},{"id":"df-window-functions","title":"Window Functions in PySpark DataFrames","description":"Learn how to use PySpark window functions for ranking, running totals, cumulative sums, and time-based analytics in Databricks.","permalink":"/pyspark/df-window-functions"},{"id":"pyspark-dates","title":"Working with Dates and Timestamps in PySpark DataFrames (Full Guide)","description":"Complete guide to date and timestamp operations in PySpark, including extracting date components, aggregations, ratios, and SQL queries with real examples.","permalink":"/pyspark/pyspark/dates-and-timestamps"}],"unlisted":false}}')}}]);