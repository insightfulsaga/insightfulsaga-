"use strict";(self.webpackChunkdatacraft_school=self.webpackChunkdatacraft_school||[]).push([[1762],{7179:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>c,frontMatter:()=>i,metadata:()=>r,toc:()=>p});const r=JSON.parse('{"id":"pyspark-dates","title":"Working with Dates and Timestamps in PySpark DataFrames (Full Guide)","description":"Complete guide to date and timestamp operations in PySpark, including extracting date components, aggregations, ratios, and SQL queries with real examples.","source":"@site/docs-pyspark/pyspark-dates.md","sourceDirName":".","slug":"/pyspark/dates-and-timestamps","permalink":"/pyspark/pyspark/dates-and-timestamps","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"PySpark","permalink":"/pyspark/tags/py-spark"},{"inline":true,"label":"Apache Spark","permalink":"/pyspark/tags/apache-spark"},{"inline":true,"label":"Big Data","permalink":"/pyspark/tags/big-data"},{"inline":true,"label":"Spark Basics","permalink":"/pyspark/tags/spark-basics"},{"inline":true,"label":"Cluster Computing","permalink":"/pyspark/tags/cluster-computing"},{"inline":true,"label":"Spark Architecture","permalink":"/pyspark/tags/spark-architecture"},{"inline":true,"label":"Driver Program","permalink":"/pyspark/tags/driver-program"},{"inline":true,"label":"Executors","permalink":"/pyspark/tags/executors"},{"inline":true,"label":"Cluster Manager","permalink":"/pyspark/tags/cluster-manager"},{"inline":true,"label":"SparkSession","permalink":"/pyspark/tags/spark-session"},{"inline":true,"label":"SparkContext","permalink":"/pyspark/tags/spark-context"},{"inline":true,"label":"RDD","permalink":"/pyspark/tags/rdd"},{"inline":true,"label":"RDD Transformations","permalink":"/pyspark/tags/rdd-transformations"},{"inline":true,"label":"RDD Actions","permalink":"/pyspark/tags/rdd-actions"},{"inline":true,"label":"Key-Value RDD","permalink":"/pyspark/tags/key-value-rdd"},{"inline":true,"label":"RDD Caching","permalink":"/pyspark/tags/rdd-caching"},{"inline":true,"label":"DataFrame","permalink":"/pyspark/tags/data-frame"},{"inline":true,"label":"DataFrame API","permalink":"/pyspark/tags/data-frame-api"},{"inline":true,"label":"Column Operations","permalink":"/pyspark/tags/column-operations"},{"inline":true,"label":"DataFrame Joins","permalink":"/pyspark/tags/data-frame-joins"},{"inline":true,"label":"Aggregations","permalink":"/pyspark/tags/aggregations"},{"inline":true,"label":"GroupBy","permalink":"/pyspark/tags/group-by"},{"inline":true,"label":"Window Functions","permalink":"/pyspark/tags/window-functions"},{"inline":true,"label":"Missing Data Handling","permalink":"/pyspark/tags/missing-data-handling"},{"inline":true,"label":"Spark SQL","permalink":"/pyspark/tags/spark-sql"},{"inline":true,"label":"Temp Views","permalink":"/pyspark/tags/temp-views"},{"inline":true,"label":"Spark SQL Functions","permalink":"/pyspark/tags/spark-sql-functions"},{"inline":true,"label":"UDF","permalink":"/pyspark/tags/udf"},{"inline":true,"label":"UDAF","permalink":"/pyspark/tags/udaf"}],"version":"current","frontMatter":{"id":"pyspark-dates","title":"Working with Dates and Timestamps in PySpark DataFrames (Full Guide)","sidebar_label":"Dates & Time","slug":"/pyspark/dates-and-timestamps","description":"Complete guide to date and timestamp operations in PySpark, including extracting date components, aggregations, ratios, and SQL queries with real examples.","keywords":["pyspark date functions","pyspark timestamp","pyspark date formatting","date extraction pyspark","pyspark weekofyear","pyspark groupBy date","pyspark sql dates","demand planning pyspark"],"og:title":"Dates & Timestamps in PySpark \u2014 Complete Tutorial with Examples","og:description":"Learn how to work with dates and timestamps in PySpark DataFrames, including extraction, aggregations, ratios, and SQL date queries.","tags":["PySpark","Apache Spark","Big Data","Spark Basics","Cluster Computing","Spark Architecture","Driver Program","Executors","Cluster Manager","SparkSession","SparkContext","RDD","RDD Transformations","RDD Actions","Key-Value RDD","RDD Caching","DataFrame","DataFrame API","Column Operations","DataFrame Joins","Aggregations","GroupBy","Window Functions","Missing Data Handling","Spark SQL","Temp Views","Spark SQL Functions","UDF","UDAF"]},"sidebar":"tutorialSidebar","previous":{"title":"Handling Missing Data","permalink":"/pyspark/pyspark/missing-data"},"next":{"title":"Spark SQL Basics","permalink":"/pyspark/spark-sql"}}');var s=n(4848),t=n(8453);const i={id:"pyspark-dates",title:"Working with Dates and Timestamps in PySpark DataFrames (Full Guide)",sidebar_label:"Dates & Time",slug:"/pyspark/dates-and-timestamps",description:"Complete guide to date and timestamp operations in PySpark, including extracting date components, aggregations, ratios, and SQL queries with real examples.",keywords:["pyspark date functions","pyspark timestamp","pyspark date formatting","date extraction pyspark","pyspark weekofyear","pyspark groupBy date","pyspark sql dates","demand planning pyspark"],"og:title":"Dates & Timestamps in PySpark \u2014 Complete Tutorial with Examples","og:description":"Learn how to work with dates and timestamps in PySpark DataFrames, including extraction, aggregations, ratios, and SQL date queries.",tags:["PySpark","Apache Spark","Big Data","Spark Basics","Cluster Computing","Spark Architecture","Driver Program","Executors","Cluster Manager","SparkSession","SparkContext","RDD","RDD Transformations","RDD Actions","Key-Value RDD","RDD Caching","DataFrame","DataFrame API","Column Operations","DataFrame Joins","Aggregations","GroupBy","Window Functions","Missing Data Handling","Spark SQL","Temp Views","Spark SQL Functions","UDF","UDAF"]},l="Dates & Timestamps in PySpark DataFrames",d={},p=[{value:"Loading Data",id:"loading-data",level:2},{value:"Preview &amp; Schema",id:"preview--schema",level:2},{value:"\u2714\ufe0f What This Does",id:"\ufe0f-what-this-does",level:3},{value:"\u2714\ufe0f Meaning",id:"\ufe0f-meaning",level:3},{value:"\u2714\ufe0f Meaning",id:"\ufe0f-meaning-1",level:3},{value:"Query: Find Date(s) With Maximum Actual Units",id:"query-find-dates-with-maximum-actual-units",level:3},{value:"\u2714\ufe0f Meaning",id:"\ufe0f-meaning-2",level:3},{value:"\u2714\ufe0f Meaning",id:"\ufe0f-meaning-3",level:3}];function o(e){const a={blockquote:"blockquote",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(a.header,{children:(0,s.jsx)(a.h1,{id:"dates--timestamps-in-pyspark-dataframes",children:"Dates & Timestamps in PySpark DataFrames"})}),"\n",(0,s.jsxs)(a.p,{children:["Working with dates and timestamps is essential for analytics, forecasting, time-series modeling, and demand planning.",(0,s.jsx)(a.br,{}),"\n","PySpark provides powerful built-in functions to extract, transform, manipulate, and aggregate date-related fields."]}),"\n",(0,s.jsxs)(a.p,{children:["This guide walks you step-by-step through ",(0,s.jsx)(a.strong,{children:"date extraction"}),", ",(0,s.jsx)(a.strong,{children:"aggregations"}),", ",(0,s.jsx)(a.strong,{children:"SQL queries"}),", and ",(0,s.jsx)(a.strong,{children:"derived metrics"}),"."]}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h2,{id:"loading-data",children:"Loading Data"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"from pyspark.sql import SparkSession\r\n\r\nspark = SparkSession.builder.appName('dates').getOrCreate()\r\n\r\ndf = spark.read.csv('/path/to/demand_planning.csv', header=True, inferSchema=True)\n"})}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h2,{id:"preview--schema",children:"Preview & Schema"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"df.show()\n"})}),"\n",(0,s.jsx)(a.p,{children:"Example output:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-sql",children:"+----------+--------+-------------+-----------+------------+\r\n|      Date| Product|ForecastUnits|ActualUnits|LeadTimeDays|\r\n+----------+--------+-------------+-----------+------------+\r\n|2023-01-01|    Soap|          100|         95|           3|\r\n|2023-01-02|    Soap|          120|        110|           2|\r\n|2023-01-03| Shampoo|           80|         85|           4|\r\n|2023-01-04| Shampoo|           90|         88|           3|\r\n|2023-02-01|    Soap|          130|        125|           2|\r\n|2023-02-02| Shampoo|           85|         82|           4|\r\n|2023-03-01|    Soap|          140|        135|           3|\r\n|2023-03-05| Shampoo|          100|        102|           3|\r\n+----------+--------+-------------+-----------+------------+\n"})}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"df.printSchema()\n"})}),"\n",(0,s.jsx)(a.p,{children:"Output:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"root\r\n |-- Date: date (nullable = true)\r\n |-- Product: string (nullable = true)\r\n |-- ForecastUnits: integer (nullable = true)\r\n |-- ActualUnits: integer (nullable = true)\r\n |-- LeadTimeDays: integer (nullable = true)\n"})}),"\n",(0,s.jsxs)(a.blockquote,{children:["\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.strong,{children:"Note:"})," Some Spark versions read dates as strings. You can convert using:\r\n",(0,s.jsx)(a.code,{children:'df = df.withColumn("Date", to_date("Date"))'})]}),"\n"]}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h1,{id:"extracting-date-components-in-pyspark",children:"Extracting Date Components in PySpark"}),"\n",(0,s.jsx)(a.p,{children:"PySpark provides functions to extract day, month, year, and week numbers."}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"from pyspark.sql.functions import dayofmonth, month, year, weekofyear\n"})}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"df.select(\r\n    dayofmonth(df['Date']).alias('Day'),\r\n    month(df['Date']).alias('Month'),\r\n    year(df['Date']).alias('Year'),\r\n    weekofyear(df['Date']).alias('WeekOfYear')\r\n).show()\n"})}),"\n",(0,s.jsx)(a.p,{children:"Output:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-sql",children:"+---+-----+----+----------+\r\n|Day|Month|Year|WeekOfYear|\r\n+---+-----+----+----------+\r\n| 1 |  1  |2023|    52    |\r\n| 2 |  1  |2023|     1    |\r\n| 3 |  1  |2023|     1    |\r\n| 4 |  1  |2023|     1    |\r\n| 1 |  2  |2023|     5    |\r\n| 2 |  2  |2023|     5    |\r\n| 1 |  3  |2023|     9    |\r\n| 5 |  3  |2023|    10    |\r\n+---+-----+----+----------+\n"})}),"\n",(0,s.jsx)(a.h3,{id:"\ufe0f-what-this-does",children:"\u2714\ufe0f What This Does"}),"\n",(0,s.jsx)(a.p,{children:"Extracts different date components for time-series analysis, forecasting, and reporting."}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h1,{id:"aggregation-by-year",children:"Aggregation by Year"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"from pyspark.sql.functions import avg\n"})}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"df_with_year = df.withColumn('Year', year(df['Date']))\r\n\r\ndf_with_year.groupBy('Year') \\\r\n    .mean() \\\r\n    .select('Year', 'avg(ActualUnits)') \\\r\n    .show()\n"})}),"\n",(0,s.jsx)(a.p,{children:"Output:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-sql",children:"+----+------------------+\r\n|Year|avg(ActualUnits)  |\r\n+----+------------------+\r\n|2023|  103.375         |\r\n+----+------------------+\n"})}),"\n",(0,s.jsx)(a.h3,{id:"\ufe0f-meaning",children:"\u2714\ufe0f Meaning"}),"\n",(0,s.jsx)(a.p,{children:"Computes yearly averages\u2014useful for trend analysis and demand forecasting."}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h1,{id:"creating-ratios--derived-metrics",children:"Creating Ratios / Derived Metrics"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"df.select(\r\n    (df['ForecastUnits'] / df['ActualUnits']).alias('Forecast_to_Actual')\r\n).show()\n"})}),"\n",(0,s.jsx)(a.p,{children:"Output:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-sql",children:"+--------------------+\r\n|Forecast_to_Actual  |\r\n+--------------------+\r\n|1.0526315789473684  |\r\n|1.0909090909090908  |\r\n|0.9411764705882353  |\r\n|1.0227272727272727  |\r\n|1.04                |\r\n|1.0365853658536585  |\r\n|1.037037037037037   |\r\n|0.9803921568627451  |\r\n+--------------------+\n"})}),"\n",(0,s.jsx)(a.h3,{id:"\ufe0f-meaning-1",children:"\u2714\ufe0f Meaning"}),"\n",(0,s.jsx)(a.p,{children:"Shows forecasting accuracy by comparing forecast vs actual."}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h1,{id:"running-sql-queries-with-dates",children:"Running SQL Queries with Dates"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"df.createOrReplaceTempView('demand')\n"})}),"\n",(0,s.jsx)(a.h3,{id:"query-find-dates-with-maximum-actual-units",children:"Query: Find Date(s) With Maximum Actual Units"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'spark.sql("""\r\n  SELECT Date, ActualUnits \r\n  FROM demand \r\n  WHERE ActualUnits = (SELECT MAX(ActualUnits) FROM demand)\r\n""").show()\n'})}),"\n",(0,s.jsx)(a.p,{children:"Output:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-sql",children:"+----------+-----------+\r\n| Date     |ActualUnits|\r\n+----------+-----------+\r\n|2023-03-05|       102 |\r\n+----------+-----------+\n"})}),"\n",(0,s.jsx)(a.h3,{id:"\ufe0f-meaning-2",children:"\u2714\ufe0f Meaning"}),"\n",(0,s.jsx)(a.p,{children:"Returns the highest-demand day(s) using SQL subqueries."}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h1,{id:"more-aggregations--statistics",children:"More Aggregations & Statistics"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"from pyspark.sql.functions import round, max, min\n"})}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"df.select(\r\n    round(avg(df['ActualUnits']), 2).alias('AvgActual'),\r\n    max(df['ForecastUnits']).alias('MaxForecast'),\r\n    min(df['ForecastUnits']).alias('MinForecast')\r\n).show()\n"})}),"\n",(0,s.jsx)(a.p,{children:"Output:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-sql",children:"+-----------+-------------+-------------+\r\n| AvgActual | MaxForecast | MinForecast |\r\n+-----------+-------------+-------------+\r\n|   103.38  |     140     |     80      |\r\n+-----------+-------------+-------------+\n"})}),"\n",(0,s.jsx)(a.h3,{id:"\ufe0f-meaning-3",children:"\u2714\ufe0f Meaning"}),"\n",(0,s.jsx)(a.p,{children:"Computes multiple KPIs in a single query."}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h1,{id:"-1-minute-summary--dates--time-in-pyspark",children:"\ud83d\udfe6 1-Minute Summary \u2014 Dates & Time in PySpark"}),"\n",(0,s.jsxs)(a.table,{children:[(0,s.jsx)(a.thead,{children:(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.th,{children:"Code / Expression"}),(0,s.jsx)(a.th,{children:"What It Does"})]})}),(0,s.jsxs)(a.tbody,{children:[(0,s.jsxs)(a.tr,{children:[(0,s.jsxs)(a.td,{children:[(0,s.jsx)(a.code,{children:"dayofmonth()"}),", ",(0,s.jsx)(a.code,{children:"month()"}),", ",(0,s.jsx)(a.code,{children:"year()"}),", ",(0,s.jsx)(a.code,{children:"weekofyear()"})]}),(0,s.jsx)(a.td,{children:"Extract components from a Date column"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.code,{children:"withColumn('Year', year(...))"})}),(0,s.jsx)(a.td,{children:"Adds new year column"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.code,{children:"groupBy('Year').mean()"})}),(0,s.jsx)(a.td,{children:"Aggregates metrics by year"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.code,{children:"(ForecastUnits / ActualUnits)"})}),(0,s.jsx)(a.td,{children:"Computes forecast accuracy"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.code,{children:"createOrReplaceTempView()"})}),(0,s.jsx)(a.td,{children:"Registers DataFrame for SQL"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.code,{children:"SELECT ... WHERE ActualUnits = (MAX...)"})}),(0,s.jsx)(a.td,{children:"Finds rows with maximum demand"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsxs)(a.td,{children:[(0,s.jsx)(a.code,{children:"round(avg(), 2)"}),", ",(0,s.jsx)(a.code,{children:"max()"}),", ",(0,s.jsx)(a.code,{children:"min()"})]}),(0,s.jsx)(a.td,{children:"Summary statistics"})]})]})]}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.p,{children:"Next, we\u2019ll cover Using Spark SQL basic \u2014 Register Temp Views and Query."})]})}function c(e={}){const{wrapper:a}={...(0,t.R)(),...e.components};return a?(0,s.jsx)(a,{...e,children:(0,s.jsx)(o,{...e})}):o(e)}},8453:(e,a,n)=>{n.d(a,{R:()=>i,x:()=>l});var r=n(6540);const s={},t=r.createContext(s);function i(e){const a=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function l(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),r.createElement(t.Provider,{value:a},e.children)}}}]);