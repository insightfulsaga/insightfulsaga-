"use strict";(self.webpackChunkdatacraft_school=self.webpackChunkdatacraft_school||[]).push([[2409],{2733:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>i,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"pyspark-streaming-kafka","title":"Streaming with Kafka","description":"A storytelling, beginner-friendly yet complete guide on integrating PySpark Streaming with Apache Kafka for real-time pipelines. Includes schema handling, offsets, partitioning, watermarking, and production best practices.","source":"@site/docs-pyspark/pyspark-streaming-kafka.md","sourceDirName":".","slug":"/pyspark-streaming-kafka","permalink":"/pyspark/pyspark-streaming-kafka","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"pyspark-streaming-kafka","title":"Streaming with Kafka","sidebar_label":"Kafka Streaming","description":"A storytelling, beginner-friendly yet complete guide on integrating PySpark Streaming with Apache Kafka for real-time pipelines. Includes schema handling, offsets, partitioning, watermarking, and production best practices.","keywords":["spark kafka streaming","pyspark kafka streaming tutorial","kafka spark integration","spark structured streaming kafka","real-time data pipelines kafka","kafka consumer spark","spark streaming kafka example"]}}');var s=a(4848),t=a(8453);const i={id:"pyspark-streaming-kafka",title:"Streaming with Kafka",sidebar_label:"Kafka Streaming",description:"A storytelling, beginner-friendly yet complete guide on integrating PySpark Streaming with Apache Kafka for real-time pipelines. Includes schema handling, offsets, partitioning, watermarking, and production best practices.",keywords:["spark kafka streaming","pyspark kafka streaming tutorial","kafka spark integration","spark structured streaming kafka","real-time data pipelines kafka","kafka consumer spark","spark streaming kafka example"]},l="\ud83d\udef0\ufe0f Streaming with Kafka \u2014 Message Highways of Data City",o={},c=[{value:"\u2714 Always use JSON with schema",id:"-always-use-json-with-schema",level:3},{value:"\u2714 Use watermarks with aggregations",id:"-use-watermarks-with-aggregations",level:3},{value:"\u2714 Tune partition counts",id:"-tune-partition-counts",level:3},{value:"\u2714 Use Delta/Parquet as sink",id:"-use-deltaparquet-as-sink",level:3},{value:"\u2714 Monitor lag",id:"-monitor-lag",level:3},{value:"\u2714 Use autoscaling Spark clusters",id:"-use-autoscaling-spark-clusters",level:3}];function d(e){const n={br:"br",code:"code",em:"em",h1:"h1",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsxs)(n.h1,{id:"\ufe0f-streaming-with-kafka--message-highways-of-data-city",children:["\ud83d\udef0\ufe0f Streaming with Kafka \u2014 ",(0,s.jsx)(n.em,{children:"Message Highways of Data City"})]})}),"\n",(0,s.jsxs)(n.p,{children:["Imagine a city where every action \u2014 a click, swipe, payment, or sensor reading \u2014 becomes a ",(0,s.jsx)(n.strong,{children:"message"})," speeding through ultra-fast highways.",(0,s.jsx)(n.br,{}),"\n","These highways are managed by ",(0,s.jsx)(n.strong,{children:"Apache Kafka"}),", the city\u2019s transport department for events."]}),"\n",(0,s.jsx)(n.p,{children:"But raw events alone mean nothing."}),"\n",(0,s.jsxs)(n.p,{children:["Enter ",(0,s.jsx)(n.strong,{children:"PySpark"}),", the city's analyst, ready to read, understand, transform, and store these messages in real time."]}),"\n",(0,s.jsx)(n.p,{children:"Together they form one of the most powerful pipelines in Data Engineering."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h1,{id:"-what-is-kafka-the-simple-way",children:"\ud83d\udea6 What Is Kafka? (The Simple Way)"}),"\n",(0,s.jsx)(n.p,{children:"Kafka is:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["a ",(0,s.jsx)(n.strong,{children:"real-time messaging system"})]}),"\n",(0,s.jsxs)(n.li,{children:["a ",(0,s.jsx)(n.strong,{children:"high-throughput event pipeline"})]}),"\n",(0,s.jsxs)(n.li,{children:["a ",(0,s.jsx)(n.strong,{children:"distributed log system"})]}),"\n",(0,s.jsxs)(n.li,{children:["a ",(0,s.jsx)(n.strong,{children:"buffer between producers & consumers"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Producers \u2192 Kafka Topics \u2192 Consumers (Spark)"}),"\n",(0,s.jsxs)(n.p,{children:["Kafka stores messages in ",(0,s.jsx)(n.strong,{children:"topics"}),", split into ",(0,s.jsx)(n.strong,{children:"partitions"})," for parallel processing."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h1,{id:"-why-spark--kafka",children:"\ud83c\udfaf Why Spark + Kafka?"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Benefit"}),(0,s.jsx)(n.th,{children:"Explanation"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"\u26a1 High Throughput"}),(0,s.jsx)(n.td,{children:"Supports millions of events per second"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"\ud83d\udee1 Fault Tolerant"}),(0,s.jsx)(n.td,{children:"Handles crashes gracefully"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"\ud83d\udd04 Scalability"}),(0,s.jsx)(n.td,{children:"Add partitions/consumers to scale horizontally"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"\u23f1 Event-Time Analytics"}),(0,s.jsx)(n.td,{children:"Spark adds windows, watermarks & event-time"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"\ud83d\udd17 Ecosystem Integration"}),(0,s.jsx)(n.td,{children:"ML, ETL, batch, dashboards"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"This combination is used by Netflix, Uber, PayPal, and Airbnb for real-time ML, fraud detection, and streaming ETL."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h1,{id:"-kafka-message-structure-very-important",children:"\ud83e\uddf1 Kafka Message Structure (Very Important)"}),"\n",(0,s.jsx)(n.p,{children:"Kafka provides:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"key"})," (binary)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"value"})," (binary)"]}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"topic"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"partition"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"offset"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:"timestamp"})}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["In most pipelines, you primarily parse the ",(0,s.jsx)(n.strong,{children:"value"})," field."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h1,{id:"-reading-kafka-streams-pyspark-structured-streaming",children:"\ud83d\udd27 Reading Kafka Streams (PySpark Structured Streaming)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'df = spark \\\r\n  .readStream \\\r\n  .format("kafka") \\\r\n  .option("kafka.bootstrap.servers", "localhost:9092") \\\r\n  .option("subscribe", "events-topic") \\\r\n  .load()\r\n\r\ndf_parsed = df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)", "timestamp")\n'})}),"\n",(0,s.jsx)(n.p,{children:"\u2714 Spark automatically tracks Kafka offsets\r\n\u2714 Data arrives as a streaming DataFrame\r\n\u2714 Value is usually JSON, CSV, or delimited text"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h1,{id:"-parsing-kafka-messages-json-example",children:"\ud83e\udde9 Parsing Kafka Messages (JSON Example)"}),"\n",(0,s.jsxs)(n.p,{children:["Most real-world Kafka pipelines send ",(0,s.jsx)(n.strong,{children:"JSON messages"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from pyspark.sql.functions import *\r\n\r\nschema = "user STRING, action STRING, amount DOUBLE, event_time TIMESTAMP"\r\n\r\njson_df = df_parsed.select(\r\n    from_json(col("value"), schema).alias("data"),\r\n    "timestamp"\r\n)\r\n\r\nparsed = json_df.select("data.*", "timestamp")\n'})}),"\n",(0,s.jsx)(n.p,{children:"\u2714 Now all fields become real DataFrame columns\r\n\u2714 Schema validation prevents broken data"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h1,{id:"-using-event-time--watermarking",children:"\ud83d\udd52 Using Event-Time & Watermarking"}),"\n",(0,s.jsx)(n.p,{children:"Kafka messages may arrive late.\r\nSpark needs a watermark to manage state."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'result = parsed \\\r\n    .withWatermark("event_time", "10 minutes") \\\r\n    .groupBy(window("event_time", "5 minutes"), "user") \\\r\n    .count()\n'})}),"\n",(0,s.jsx)(n.p,{children:"\u2714 Handles late events\r\n\u2714 Prevents infinite memory usage\r\n\u2714 Enables time-based aggregation"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h1,{id:"-transforming-kafka-data",children:"\ud83d\udcdd Transforming Kafka Data"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'cleaned = parsed \\\r\n    .filter(col("user").isNotNull()) \\\r\n    .withColumn("action_upper", upper(col("action")))\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h1,{id:"-writing-back-to-kafka",children:"\ud83d\udce4 Writing Back to Kafka"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'cleaned \\\r\n  .selectExpr(\r\n    "CAST(user AS STRING) AS key",\r\n    "to_json(struct(*)) AS value"\r\n  ) \\\r\n  .writeStream \\\r\n  .format("kafka") \\\r\n  .option("kafka.bootstrap.servers", "localhost:9092") \\\r\n  .option("topic", "processed-events") \\\r\n  .option("checkpointLocation", "/tmp/kafka-checkpoint") \\\r\n  .start()\n'})}),"\n",(0,s.jsx)(n.p,{children:"\u2714 Supports exactly-once delivery\r\n\u2714 Automatically commits Kafka offsets"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h1,{id:"-subscribing-to-multiple-topics",children:"\ud83d\udccd Subscribing to Multiple Topics"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'.option("subscribe", "topic1,topic2")\n'})}),"\n",(0,s.jsxs)(n.p,{children:["Or use a ",(0,s.jsx)(n.strong,{children:"pattern"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'.option("subscribePattern", "events-*")\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h1,{id:"-manual-offset-control-advanced-but-important",children:"\ud83e\udded Manual Offset Control (Advanced but Important)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'.option("startingOffsets", "earliest")\n'})}),"\n",(0,s.jsx)(n.p,{children:"Values you can use:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:'"earliest"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.code,{children:'"latest"'})}),"\n",(0,s.jsx)(n.li,{children:"A JSON of specific partitions"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Perfect for ",(0,s.jsx)(n.strong,{children:"reprocessing old data"}),"."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h1,{id:"-kafka--spark-parallelism",children:"\ud83d\udce6 Kafka \u2192 Spark Parallelism"}),"\n",(0,s.jsx)(n.p,{children:"Kafka partitions = Spark parallelism."}),"\n",(0,s.jsx)(n.p,{children:"Example:"}),"\n",(0,s.jsxs)(n.p,{children:["Kafka topic has ",(0,s.jsx)(n.strong,{children:"12 partitions"}),"\r\n\u2192 Spark can process using ",(0,s.jsx)(n.strong,{children:"12 parallel tasks"})]}),"\n",(0,s.jsx)(n.p,{children:"If input is slow:"}),"\n",(0,s.jsx)(n.p,{children:"Increase partitions\r\nOR\r\nIncrease consumer group instances"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h1,{id:"-security-ssl--sasl",children:"\ud83d\udd10 Security (SSL & SASL)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'.option("kafka.security.protocol", "SASL_SSL")\r\n.option("kafka.sasl.mechanism", "PLAIN")\r\n.option("kafka.ssl.truststore.location", "/certs/kafka.jks")\n'})}),"\n",(0,s.jsx)(n.p,{children:"Supports:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"SSL"}),"\n",(0,s.jsx)(n.li,{children:"Kerberos"}),"\n",(0,s.jsx)(n.li,{children:"SASL"}),"\n",(0,s.jsx)(n.li,{children:"OAuth"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h1,{id:"\ufe0f-checkpointing-mandatory",children:"\ud83d\udee1\ufe0f Checkpointing (Mandatory)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'.option("checkpointLocation", "/path/to/checkpoints")\n'})}),"\n",(0,s.jsx)(n.p,{children:"Checkpoint stores:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Kafka offsets"}),"\n",(0,s.jsx)(n.li,{children:"Aggregation state"}),"\n",(0,s.jsx)(n.li,{children:"Watermark data"}),"\n",(0,s.jsx)(n.li,{children:"Query progress"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Without checkpoint \u2192 no exactly-once guarantees."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h1,{id:"-testing-kafka--spark-locally",children:"\ud83e\uddea Testing Kafka + Spark Locally"}),"\n",(0,s.jsx)(n.p,{children:"Use:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"nc -lk 9999\n"})}),"\n",(0,s.jsx)(n.p,{children:"or\r\nRun a local Kafka:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"bin/kafka-console-producer.sh --topic test --bootstrap-server localhost:9092\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h1,{id:"-production-best-practices",children:"\ud83d\ude80 Production Best Practices"}),"\n",(0,s.jsx)(n.h3,{id:"-always-use-json-with-schema",children:"\u2714 Always use JSON with schema"}),"\n",(0,s.jsx)(n.p,{children:"Avoid unstructured messages."}),"\n",(0,s.jsx)(n.h3,{id:"-use-watermarks-with-aggregations",children:"\u2714 Use watermarks with aggregations"}),"\n",(0,s.jsx)(n.p,{children:"Prevent memory buildup."}),"\n",(0,s.jsx)(n.h3,{id:"-tune-partition-counts",children:"\u2714 Tune partition counts"}),"\n",(0,s.jsx)(n.p,{children:"More partitions \u2192 more throughput."}),"\n",(0,s.jsx)(n.h3,{id:"-use-deltaparquet-as-sink",children:"\u2714 Use Delta/Parquet as sink"}),"\n",(0,s.jsx)(n.p,{children:"Avoid console sink in production."}),"\n",(0,s.jsx)(n.h3,{id:"-monitor-lag",children:"\u2714 Monitor lag"}),"\n",(0,s.jsx)(n.p,{children:"Kafka Consumer Lag = Health of your pipeline."}),"\n",(0,s.jsx)(n.h3,{id:"-use-autoscaling-spark-clusters",children:"\u2714 Use autoscaling Spark clusters"}),"\n",(0,s.jsx)(n.p,{children:"Handle spikes automatically."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h1,{id:"\ufe0f-full-working-example-clean-ready-to-use",children:"\ud83d\udee0\ufe0f Full Working Example (Clean, Ready-to-Use)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from pyspark.sql.functions import *\r\nfrom pyspark.sql.types import *\r\n\r\nschema = StructType([\r\n    StructField("user", StringType()),\r\n    StructField("action", StringType()),\r\n    StructField("amount", DoubleType()),\r\n    StructField("event_time", TimestampType())\r\n])\r\n\r\n# Read from Kafka\r\ndf = spark.readStream.format("kafka") \\\r\n    .option("kafka.bootstrap.servers", "localhost:9092") \\\r\n    .option("subscribe", "transactions") \\\r\n    .load()\r\n\r\n# Parse JSON\r\njson = df.select(\r\n    from_json(col("value").cast("string"), schema).alias("data"),\r\n    "timestamp"\r\n)\r\n\r\nparsed = json.select("data.*", "timestamp")\r\n\r\n# Aggregation with windows\r\nagg = parsed.withWatermark("event_time", "10 minutes") \\\r\n    .groupBy(window("event_time", "5 minutes"), "user") \\\r\n    .agg(sum("amount").alias("total_spent"))\r\n\r\n# Write back to Kafka\r\nagg.selectExpr("CAST(user AS STRING) AS key", "to_json(struct(*)) AS value") \\\r\n   .writeStream \\\r\n   .format("kafka") \\\r\n   .option("kafka.bootstrap.servers", "localhost:9092") \\\r\n   .option("topic", "user-spending") \\\r\n   .option("checkpointLocation", "/checkpoint/kafka-pipeline") \\\r\n   .start()\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h1,{id:"-summary",children:"\ud83c\udfc1 Summary"}),"\n",(0,s.jsx)(n.p,{children:"Kafka + PySpark Streaming gives you:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Real-time event ingestion"}),"\n",(0,s.jsx)(n.li,{children:"Scalable, fault-tolerant pipelines"}),"\n",(0,s.jsx)(n.li,{children:"Event-time windowing"}),"\n",(0,s.jsx)(n.li,{children:"Exactly-once delivery"}),"\n",(0,s.jsx)(n.li,{children:"JSON parsing & schema validation"}),"\n",(0,s.jsx)(n.li,{children:"State management via watermarks"}),"\n",(0,s.jsx)(n.li,{children:"High-throughput processing"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This is the backbone of modern data engineering."})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>i,x:()=>l});var r=a(6540);const s={},t=r.createContext(s);function i(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);