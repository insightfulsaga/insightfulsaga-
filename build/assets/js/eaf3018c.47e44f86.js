"use strict";(self.webpackChunkdatacraft_school=self.webpackChunkdatacraft_school||[]).push([[8142],{5772:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>c,frontMatter:()=>o,metadata:()=>s,toc:()=>p});const s=JSON.parse('{"id":"pyspark-interview-questions-part3","title":"Essential PySpark Interview Question & Answer(Explained Through Real-World Stories) \u2013 Part 3","description":"16. What are UDFs and how do you create them in PySpark?","source":"@site/docs-pyspark/pyspark-interview-questions-part3.md","sourceDirName":".","slug":"/pyspark-interview-questions-part3","permalink":"/pyspark/pyspark-interview-questions-part3","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"pyspark-intro","permalink":"/pyspark/tags/pyspark-intro"},{"inline":true,"label":"pyspark dataframe basics","permalink":"/pyspark/tags/pyspark-dataframe-basics"},{"inline":true,"label":"pyspark dataframe basics2","permalink":"/pyspark/tags/pyspark-dataframe-basics-2"},{"inline":true,"label":"pyspark filtering","permalink":"/pyspark/tags/pyspark-filtering"},{"inline":true,"label":"pyspark aggregation","permalink":"/pyspark/tags/pyspark-aggregation"},{"inline":true,"label":"pyspark missing","permalink":"/pyspark/tags/pyspark-missing"},{"inline":true,"label":"pyspark dates","permalink":"/pyspark/tags/pyspark-dates"},{"inline":true,"label":"pyspark joins","permalink":"/pyspark/tags/pyspark-joins"},{"inline":true,"label":"pyspark one liners","permalink":"/pyspark/tags/pyspark-one-liners"},{"inline":true,"label":"linear regression model","permalink":"/pyspark/tags/linear-regression-model"},{"inline":true,"label":"linear regression math","permalink":"/pyspark/tags/linear-regression-math"},{"inline":true,"label":"house price linear regression","permalink":"/pyspark/tags/house-price-linear-regression"},{"inline":true,"label":"logistic regression model","permalink":"/pyspark/tags/logistic-regression-model"},{"inline":true,"label":"logistic regression query","permalink":"/pyspark/tags/logistic-regression-query"},{"inline":true,"label":"logistic regression mini project","permalink":"/pyspark/tags/logistic-regression-mini-project"},{"inline":true,"label":"pyspark-interview-questions-part1","permalink":"/pyspark/tags/pyspark-interview-questions-part-1"},{"inline":true,"label":"pyspark-interview-questions-part2","permalink":"/pyspark/tags/pyspark-interview-questions-part-2"}],"version":"current","sidebarPosition":3,"frontMatter":{"id":"pyspark-interview-questions-part3","title":"Essential PySpark Interview Question & Answer(Explained Through Real-World Stories) \u2013 Part 3","sidebar_label":"PySpark Interview Q&A(Story-Based) - 3","tags":["pyspark-intro","pyspark dataframe basics","pyspark dataframe basics2","pyspark filtering","pyspark aggregation","pyspark missing","pyspark dates","pyspark joins","pyspark one liners","linear regression model","linear regression math","house price linear regression","logistic regression model","logistic regression query","logistic regression mini project","pyspark-interview-questions-part1","pyspark-interview-questions-part2"],"keywords":["pyspark-intro","pyspark dataframe basics","pyspark dataframe basics2","pyspark filtering","pyspark aggregation","pyspark missing","pyspark dates","pyspark joins","pyspark one liners","linear regression model","linear regression math","house price linear regression","logistic regression model","logistic regression query","logistic regression mini project","pyspark-interview-questions-part1","pyspark-interview-questions-part2"],"sidebar_position":3}}');var a=r(4848),i=r(8453);const o={id:"pyspark-interview-questions-part3",title:"Essential PySpark Interview Question & Answer(Explained Through Real-World Stories) \u2013 Part 3",sidebar_label:"PySpark Interview Q&A(Story-Based) - 3",tags:["pyspark-intro","pyspark dataframe basics","pyspark dataframe basics2","pyspark filtering","pyspark aggregation","pyspark missing","pyspark dates","pyspark joins","pyspark one liners","linear regression model","linear regression math","house price linear regression","logistic regression model","logistic regression query","logistic regression mini project","pyspark-interview-questions-part1","pyspark-interview-questions-part2"],keywords:["pyspark-intro","pyspark dataframe basics","pyspark dataframe basics2","pyspark filtering","pyspark aggregation","pyspark missing","pyspark dates","pyspark joins","pyspark one liners","linear regression model","linear regression math","house price linear regression","logistic regression model","logistic regression query","logistic regression mini project","pyspark-interview-questions-part1","pyspark-interview-questions-part2"],sidebar_position:3},t=void 0,l={},p=[{value:"<strong>16. What are UDFs and how do you create them in PySpark?</strong>",id:"16-what-are-udfs-and-how-do-you-create-them-in-pyspark",level:3},{value:"<strong>17. What are Pandas UDFs and why are they useful?</strong>",id:"17-what-are-pandas-udfs-and-why-are-they-useful",level:3},{value:"<strong>18. How do you perform aggregations in PySpark?</strong>",id:"18-how-do-you-perform-aggregations-in-pyspark",level:3},{value:"<strong>19. Explain groupBy and aggregate functions</strong>",id:"19-explain-groupby-and-aggregate-functions",level:3},{value:"<strong>20. How do you pivot a DataFrame in PySpark?</strong>",id:"20-how-do-you-pivot-a-dataframe-in-pyspark",level:3},{value:"<strong>21. How do you join two DataFrames? Explain different join types</strong>",id:"21-how-do-you-join-two-dataframes-explain-different-join-types",level:3},{value:"<strong>22. What are broadcast joins and when should you use them?</strong>",id:"22-what-are-broadcast-joins-and-when-should-you-use-them",level:3},{value:"<strong>23. What are window functions in PySpark? Give examples</strong>",id:"23-what-are-window-functions-in-pyspark-give-examples",level:3},{value:"<strong>24. How do you sort a DataFrame by column(s)?</strong>",id:"24-how-do-you-sort-a-dataframe-by-columns",level:3},{value:"<strong>25. How do you filter data using multiple conditions?</strong>",id:"25-how-do-you-filter-data-using-multiple-conditions",level:3}];function d(e){const n={code:"code",h3:"h3",hr:"hr",p:"p",pre:"pre",strong:"strong",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h3,{id:"16-what-are-udfs-and-how-do-you-create-them-in-pyspark",children:(0,a.jsx)(n.strong,{children:"16. What are UDFs and how do you create them in PySpark?"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nThink of PySpark as a factory assembly line. Each machine (built-in function) does a specific task automatically. But sometimes, you need a custom machine for a unique task\u2014this is your ",(0,a.jsx)(n.strong,{children:"User Defined Function (UDF)"}),". You design it, plug it in, and now your assembly line can handle tasks no standard machine can."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Professional Explanation:"}),"\r\nA UDF in PySpark is a way to apply custom logic to DataFrame columns that aren\u2019t covered by built-in functions. You define a Python function and register it as a UDF to use it in PySpark transformations."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Example:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from pyspark.sql.functions import udf\r\nfrom pyspark.sql.types import IntegerType\r\n\r\ndef square(x):\r\n    return x * x\r\n\r\nsquare_udf = udf(square, IntegerType())\r\ndf.withColumn("squared_value", square_udf(df["value"]))\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h3,{id:"17-what-are-pandas-udfs-and-why-are-they-useful",children:(0,a.jsx)(n.strong,{children:"17. What are Pandas UDFs and why are they useful?"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nImagine UDFs are solo musicians, one row at a time. ",(0,a.jsx)(n.strong,{children:"Pandas UDFs"})," are like an entire orchestra\u2014they process batches of data together using vectorized operations, making it way faster."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Professional Explanation:"}),"\r\nPandas UDFs leverage Apache Arrow to process batches of data, improving performance over traditional row-wise UDFs. They are especially useful for large datasets where efficiency matters."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Example:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from pyspark.sql.functions import pandas_udf\r\nimport pandas as pd\r\n\r\n@pandas_udf("long")\r\ndef add_one(s: pd.Series) -> pd.Series:\r\n    return s + 1\r\n\r\ndf.withColumn("new_value", add_one(df["value"]))\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h3,{id:"18-how-do-you-perform-aggregations-in-pyspark",children:(0,a.jsx)(n.strong,{children:"18. How do you perform aggregations in PySpark?"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nAggregations are like dashboards in a gaming app\u2014they summarize many actions into meaningful metrics, like total score, average time, or max level achieved."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Professional Explanation:"}),"\r\nAggregations in PySpark are done using ",(0,a.jsx)(n.code,{children:"agg()"})," or functions like ",(0,a.jsx)(n.code,{children:"sum"}),", ",(0,a.jsx)(n.code,{children:"avg"}),", ",(0,a.jsx)(n.code,{children:"count"}),", etc., to summarize data. They are commonly combined with ",(0,a.jsx)(n.code,{children:"groupBy"})," for grouped metrics."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Example:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from pyspark.sql.functions import avg, sum, count\r\ndf.groupBy("department").agg(\r\n    avg("salary").alias("avg_salary"),\r\n    sum("bonus").alias("total_bonus"),\r\n    count("*").alias("num_employees")\r\n)\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h3,{id:"19-explain-groupby-and-aggregate-functions",children:(0,a.jsx)(n.strong,{children:"19. Explain groupBy and aggregate functions"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nImagine a warehouse sorting toys by type (groupBy). Once grouped, you can count, sum, or find the average price per toy type (aggregate functions)."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Professional Explanation:"}),"\r\n",(0,a.jsx)(n.code,{children:"groupBy()"})," groups rows based on column values. Aggregation functions like ",(0,a.jsx)(n.code,{children:"sum()"}),", ",(0,a.jsx)(n.code,{children:"avg()"}),", ",(0,a.jsx)(n.code,{children:"count()"})," operate on these groups to produce summary statistics."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Example:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'df.groupBy("category").sum("price").show()\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h3,{id:"20-how-do-you-pivot-a-dataframe-in-pyspark",children:(0,a.jsx)(n.strong,{children:"20. How do you pivot a DataFrame in PySpark?"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nPivoting is like rotating a spreadsheet so rows become columns\u2014think of it as turning a long, thin LEGO tower into a flat display so you can see everything at a glance."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Professional Explanation:"}),"\r\nPivoting reshapes data by turning unique values from one column into new columns. You usually aggregate the values while pivoting."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Example:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'df.groupBy("date").pivot("product").sum("sales").show()\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h3,{id:"21-how-do-you-join-two-dataframes-explain-different-join-types",children:(0,a.jsx)(n.strong,{children:"21. How do you join two DataFrames? Explain different join types"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nJoining is like merging two contact lists. You can keep only mutual friends (inner join), everyone including unknown contacts (outer join), or pick the contacts from one list even if the other doesn\u2019t match (left/right join)."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Professional Explanation:"}),"\r\nPySpark supports ",(0,a.jsx)(n.code,{children:"inner"}),", ",(0,a.jsx)(n.code,{children:"left"}),", ",(0,a.jsx)(n.code,{children:"right"}),", ",(0,a.jsx)(n.code,{children:"full"}),", and ",(0,a.jsx)(n.code,{children:"semi/anti"})," joins. The choice depends on whether you want all rows, only matches, or exclusive rows from one side."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Example:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'df1.join(df2, df1.id == df2.id, "left").show()\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h3,{id:"22-what-are-broadcast-joins-and-when-should-you-use-them",children:(0,a.jsx)(n.strong,{children:"22. What are broadcast joins and when should you use them?"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nA broadcast join is like sending a tiny menu to every chef in a big kitchen instead of making them all come to one station\u2014faster when one dataset is small."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Professional Explanation:"}),"\r\nBroadcast joins replicate the smaller DataFrame to all nodes, avoiding expensive shuffles. Use them when one DataFrame is small and fits in memory."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Example:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from pyspark.sql.functions import broadcast\r\ndf_large.join(broadcast(df_small), "id").show()\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h3,{id:"23-what-are-window-functions-in-pyspark-give-examples",children:(0,a.jsx)(n.strong,{children:"23. What are window functions in PySpark? Give examples"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nWindow functions are like giving each employee a view of their own performance and their peers\u2019 performance, without collapsing the table\u2014like seeing both your score and the team average side by side."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Professional Explanation:"}),"\r\nWindow functions operate over a \u201cwindow\u201d of rows related to the current row, allowing ranking, cumulative sums, and moving averages without reducing the DataFrame."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Example:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from pyspark.sql.window import Window\r\nfrom pyspark.sql.functions import rank, desc\r\n\r\nwindow_spec = Window.partitionBy("department").orderBy(desc("salary"))\r\ndf.withColumn("rank", rank().over(window_spec)).show()\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h3,{id:"24-how-do-you-sort-a-dataframe-by-columns",children:(0,a.jsx)(n.strong,{children:"24. How do you sort a DataFrame by column(s)?"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nSorting is like arranging your Spotify playlist by mood or release date\u2014everything lines up neatly according to your chosen metric."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Professional Explanation:"}),"\r\nUse ",(0,a.jsx)(n.code,{children:"orderBy()"})," or ",(0,a.jsx)(n.code,{children:"sort()"})," to sort by one or more columns in ascending or descending order."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Example:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'df.orderBy("salary", ascending=False).show()\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h3,{id:"25-how-do-you-filter-data-using-multiple-conditions",children:(0,a.jsx)(n.strong,{children:"25. How do you filter data using multiple conditions?"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Story/Modern Tech Analogy:"}),"\r\nFiltering with multiple conditions is like using multiple search filters on Amazon\u2014price range + brand + rating\u2014to get exactly what you want."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Professional Explanation:"}),"\r\nYou can use ",(0,a.jsx)(n.code,{children:"filter()"})," or ",(0,a.jsx)(n.code,{children:"where()"})," with logical operators (",(0,a.jsx)(n.code,{children:"&"})," for AND, ",(0,a.jsx)(n.code,{children:"|"})," for OR) to filter rows based on multiple conditions."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Example:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'df.filter((df.salary > 50000) & (df.department == "IT")).show()\n'})})]})}function c(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>t});var s=r(6540);const a={},i=s.createContext(a);function o(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);