"use strict";(self.webpackChunkdatacraft_school=self.webpackChunkdatacraft_school||[]).push([[1010],{1325:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"pyspark-interview-questions-part1","title":"Essential PySpark Interview Question & Answer(Explained Through Real-World Stories) \u2013 Part 1","description":"Learn key PySpark interview questions and answers covering SparkSession, DataFrames, RDDs, and more. Includes code examples and comparisons with Pandas.","source":"@site/docs-pyspark/pyspark-interview-questions-part1.md","sourceDirName":".","slug":"/pyspark-interview-questions-part1","permalink":"/pyspark/pyspark-interview-questions-part1","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"pyspark-intro","permalink":"/pyspark/tags/pyspark-intro"},{"inline":true,"label":"pyspark dataframe basics","permalink":"/pyspark/tags/pyspark-dataframe-basics"},{"inline":true,"label":"pyspark dataframe basics2","permalink":"/pyspark/tags/pyspark-dataframe-basics-2"},{"inline":true,"label":"pyspark filtering","permalink":"/pyspark/tags/pyspark-filtering"},{"inline":true,"label":"pyspark aggregation","permalink":"/pyspark/tags/pyspark-aggregation"},{"inline":true,"label":"pyspark missing","permalink":"/pyspark/tags/pyspark-missing"},{"inline":true,"label":"pyspark dates","permalink":"/pyspark/tags/pyspark-dates"},{"inline":true,"label":"pyspark joins","permalink":"/pyspark/tags/pyspark-joins"},{"inline":true,"label":"pyspark one liners","permalink":"/pyspark/tags/pyspark-one-liners"},{"inline":true,"label":"linear regression model","permalink":"/pyspark/tags/linear-regression-model"},{"inline":true,"label":"linear regression math","permalink":"/pyspark/tags/linear-regression-math"},{"inline":true,"label":"house price linear regression","permalink":"/pyspark/tags/house-price-linear-regression"},{"inline":true,"label":"logistic regression model","permalink":"/pyspark/tags/logistic-regression-model"},{"inline":true,"label":"logistic regression query","permalink":"/pyspark/tags/logistic-regression-query"},{"inline":true,"label":"logistic regression mini project","permalink":"/pyspark/tags/logistic-regression-mini-project"},{"inline":true,"label":"PySpark Interview Questions","permalink":"/pyspark/tags/py-spark-interview-questions"}],"version":"current","sidebarPosition":1,"frontMatter":{"id":"pyspark-interview-questions-part1","title":"Essential PySpark Interview Question & Answer(Explained Through Real-World Stories) \u2013 Part 1","description":"Learn key PySpark interview questions and answers covering SparkSession, DataFrames, RDDs, and more. Includes code examples and comparisons with Pandas.","sidebar_label":"PySpark Interview Q&A(Story-Based) - 1","tags":["pyspark-intro","pyspark dataframe basics","pyspark dataframe basics2","pyspark filtering","pyspark aggregation","pyspark missing","pyspark dates","pyspark joins","pyspark one liners","linear regression model","linear regression math","house price linear regression","logistic regression model","logistic regression query","logistic regression mini project","PySpark Interview Questions"],"keywords":["PySpark Interview Questions","SparkSession PySpark","PySpark DataFrame","PySpark vs Pandas","RDD vs DataFrame","PySpark version check"],"sidebar_position":1}}');var a=n(4848),i=n(8453);const t={id:"pyspark-interview-questions-part1",title:"Essential PySpark Interview Question & Answer(Explained Through Real-World Stories) \u2013 Part 1",description:"Learn key PySpark interview questions and answers covering SparkSession, DataFrames, RDDs, and more. Includes code examples and comparisons with Pandas.",sidebar_label:"PySpark Interview Q&A(Story-Based) - 1",tags:["pyspark-intro","pyspark dataframe basics","pyspark dataframe basics2","pyspark filtering","pyspark aggregation","pyspark missing","pyspark dates","pyspark joins","pyspark one liners","linear regression model","linear regression math","house price linear regression","logistic regression model","logistic regression query","logistic regression mini project","PySpark Interview Questions"],keywords:["PySpark Interview Questions","SparkSession PySpark","PySpark DataFrame","PySpark vs Pandas","RDD vs DataFrame","PySpark version check"],sidebar_position:1},o=void 0,l={},d=[{value:"<strong>1. What is PySpark and how is it different from Pandas?</strong>",id:"1-what-is-pyspark-and-how-is-it-different-from-pandas",level:2},{value:"\u2705<strong>Key Differences Between PySpark and Pandas</strong>",id:"key-differences-between-pyspark-and-pandas",level:3},{value:"<strong>2. What is SparkSession in PySpark?</strong>",id:"2-what-is-sparksession-in-pyspark",level:2},{value:"<strong>2.1 How it replaced older contexts?</strong>",id:"21-how-it-replaced-older-contexts",level:3},{value:"<strong>2.2 What is the Core Responsibilities of SparkSession?</strong>",id:"22-what-is-the-core-responsibilities-of-sparksession",level:3},{value:"<strong>2.3 Explain the internal architecture of SparkSession in PySpark. How does it interact with SparkContext and SQLContext?</strong>",id:"23-explain-the-internal-architecture-of-sparksession-in-pyspark-how-does-it-interact-with-sparkcontext-and-sqlcontext",level:3},{value:"<strong>3. What is DataFrame and How do you create a DataFrame in PySpark?</strong>",id:"3-what-is-dataframe-and-how-do-you-create-a-dataframe-in-pyspark",level:2},{value:"\u2705You can create a DataFrame in PySpark in <strong>three main ways</strong>:",id:"you-can-create-a-dataframe-in-pyspark-in-three-main-ways",level:3},{value:"<strong>1\ufe0f\u20e3 From a Structured Data Source (like CSV, JSON, Parquet, etc.)</strong>",id:"1\ufe0f\u20e3-from-a-structured-data-source-like-csv-json-parquet-etc",level:4},{value:"<strong>2\ufe0f\u20e3 From a Python Collection (List, Dictionary, or RDD)</strong>",id:"2\ufe0f\u20e3-from-a-python-collection-list-dictionary-or-rdd",level:4},{value:"<strong>3\ufe0f\u20e3 From an External Database or Data Warehouse</strong>",id:"3\ufe0f\u20e3-from-an-external-database-or-data-warehouse",level:4},{value:"<strong>4. Explain the difference between RDD and DataFrame?</strong>",id:"4-explain-the-difference-between-rdd-and-dataframe",level:2},{value:"<strong>5. How do you check the version of PySpark?</strong>",id:"5-how-do-you-check-the-version-of-pyspark",level:2},{value:"<strong>1\ufe0f\u20e3 Check version using PySpark shell or script</strong>",id:"1\ufe0f\u20e3-check-version-using-pyspark-shell-or-script",level:3},{value:"<strong>2\ufe0f\u20e3 Check version from SparkSession</strong>",id:"2\ufe0f\u20e3-check-version-from-sparksession",level:3},{value:"<strong>3\ufe0f\u20e3 Check via Command Line (CLI)</strong>",id:"3\ufe0f\u20e3-check-via-command-line-cli",level:3},{value:"\ud83d\udccb <strong>Quick Summary Table</strong>",id:"-quick-summary-table",level:3}];function c(e){const r={code:"code",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(r.h2,{id:"1-what-is-pyspark-and-how-is-it-different-from-pandas",children:(0,a.jsx)(r.strong,{children:"1. What is PySpark and how is it different from Pandas?"})}),"\n",(0,a.jsxs)(r.p,{children:[(0,a.jsx)(r.strong,{children:"PySpark"})," is the ",(0,a.jsx)(r.strong,{children:"Python API for Apache Spark"}),", an open-source distributed computing framework designed to process large-scale data across multiple nodes in a cluster.\r\nIt provides an interface for leveraging Spark\u2019s capabilities (such as distributed data processing, fault tolerance, and in-memory computation) using Python syntax."]}),"\n",(0,a.jsxs)(r.h3,{id:"key-differences-between-pyspark-and-pandas",children:["\u2705",(0,a.jsx)(r.strong,{children:"Key Differences Between PySpark and Pandas"})]}),"\n",(0,a.jsxs)(r.table,{children:[(0,a.jsx)(r.thead,{children:(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.th,{children:"Feature"}),(0,a.jsx)(r.th,{children:(0,a.jsx)(r.strong,{children:"PySpark"})}),(0,a.jsx)(r.th,{children:(0,a.jsx)(r.strong,{children:"Pandas"})})]})}),(0,a.jsxs)(r.tbody,{children:[(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.strong,{children:"Data Size"})}),(0,a.jsxs)(r.td,{children:["Handles ",(0,a.jsx)(r.strong,{children:"massive datasets"})," (terabytes or petabytes) distributed across multiple machines"]}),(0,a.jsxs)(r.td,{children:["Works with ",(0,a.jsx)(r.strong,{children:"small to medium datasets"})," that fit in a single machine\u2019s memory"]})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.strong,{children:"Execution Model"})}),(0,a.jsxs)(r.td,{children:[(0,a.jsx)(r.strong,{children:"Lazy evaluation"})," \u2014 transformations are only executed when an action (like ",(0,a.jsx)(r.code,{children:".count()"})," or ",(0,a.jsx)(r.code,{children:".show()"}),") is called"]}),(0,a.jsxs)(r.td,{children:[(0,a.jsx)(r.strong,{children:"Eager evaluation"})," \u2014 operations execute immediately"]})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.strong,{children:"Performance"})}),(0,a.jsxs)(r.td,{children:["Optimized for ",(0,a.jsx)(r.strong,{children:"parallel, distributed processing"})," via Spark\u2019s execution engine"]}),(0,a.jsxs)(r.td,{children:[(0,a.jsx)(r.strong,{children:"Single-threaded"})," and runs on a single machine"]})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.strong,{children:"Environment"})}),(0,a.jsxs)(r.td,{children:["Requires a ",(0,a.jsx)(r.strong,{children:"Spark cluster or local Spark setup"})]}),(0,a.jsx)(r.td,{children:"Runs locally with no cluster requirement"})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.strong,{children:"Use Cases"})}),(0,a.jsx)(r.td,{children:"Big Data ETL, Data Engineering, Machine Learning Pipelines on distributed data"}),(0,a.jsx)(r.td,{children:"Data analysis, exploration, and quick prototyping"})]})]})]}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Example:"})}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:'# Pandas example\r\nimport pandas as pd\r\ndf = pd.read_csv("sales.csv")\r\ndf.groupby("region")["revenue"].sum()\r\n\r\n# PySpark example\r\nfrom pyspark.sql import SparkSession\r\nspark = SparkSession.builder.appName("SalesApp").getOrCreate()\r\ndf = spark.read.csv("sales.csv", header=True, inferSchema=True)\r\ndf.groupBy("region").sum("revenue").show()\n'})}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"\ud83d\udcac Summary:"})}),"\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"Pandas"})," is like a powerful notebook \u2014 excellent for data that fits in memory."]}),"\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.strong,{children:"PySpark"})," is like a distributed supercomputer \u2014 designed to handle big data efficiently and fault-tolerantly."]}),"\n"]}),"\n",(0,a.jsx)(r.h2,{id:"2-what-is-sparksession-in-pyspark",children:(0,a.jsx)(r.strong,{children:"2. What is SparkSession in PySpark?"})}),"\n",(0,a.jsxs)(r.p,{children:[(0,a.jsx)(r.strong,{children:"SparkSession"})," is the ",(0,a.jsx)(r.strong,{children:"entry point"})," to programming with PySpark.\r\nIt provides a unified interface to interact with all Spark functionalities, including DataFrame and SQL APIs, streaming, and machine learning."]}),"\n",(0,a.jsx)(r.h3,{id:"21-how-it-replaced-older-contexts",children:(0,a.jsx)(r.strong,{children:"2.1 How it replaced older contexts?"})}),"\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsxs)(r.li,{children:["Introduced in ",(0,a.jsx)(r.strong,{children:"Spark 2.0"}),", replacing older contexts (",(0,a.jsx)(r.code,{children:"SparkContext"}),", ",(0,a.jsx)(r.code,{children:"SQLContext"}),", and ",(0,a.jsx)(r.code,{children:"HiveContext"}),")."]}),"\n",(0,a.jsxs)(r.li,{children:["Acts as the ",(0,a.jsx)(r.strong,{children:"main gateway"})," to access Spark\u2019s core features."]}),"\n",(0,a.jsxs)(r.li,{children:["Handles the ",(0,a.jsx)(r.strong,{children:"configuration, resource management"}),", and ",(0,a.jsx)(r.strong,{children:"session state"})," for the Spark application."]}),"\n"]}),"\n",(0,a.jsx)(r.h3,{id:"22-what-is-the-core-responsibilities-of-sparksession",children:(0,a.jsx)(r.strong,{children:"2.2 What is the Core Responsibilities of SparkSession?"})}),"\n",(0,a.jsxs)(r.ol,{children:["\n",(0,a.jsxs)(r.li,{children:["\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Create and manage DataFrames"})}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:'df = spark.read.csv("data.csv", header=True, inferSchema=True)\n'})}),"\n"]}),"\n",(0,a.jsxs)(r.li,{children:["\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Run SQL queries"})}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:'spark.sql("SELECT * FROM customers WHERE age > 30").show()\n'})}),"\n"]}),"\n",(0,a.jsxs)(r.li,{children:["\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Access the Spark Context"})}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"sc = spark.sparkContext\n"})}),"\n"]}),"\n",(0,a.jsxs)(r.li,{children:["\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Manage configurations and catalogs"})}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:'spark.conf.set("spark.sql.shuffle.partitions", 50)\r\nspark.catalog.listTables()\n'})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(r.h3,{id:"23-explain-the-internal-architecture-of-sparksession-in-pyspark-how-does-it-interact-with-sparkcontext-and-sqlcontext",children:(0,a.jsx)(r.strong,{children:"2.3 Explain the internal architecture of SparkSession in PySpark. How does it interact with SparkContext and SQLContext?"})}),"\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsxs)(r.li,{children:["\n",(0,a.jsxs)(r.p,{children:[(0,a.jsx)(r.strong,{children:"SparkSession"})," internally creates or uses a ",(0,a.jsx)(r.strong,{children:"SparkContext"})," to connect with the cluster."]}),"\n"]}),"\n",(0,a.jsxs)(r.li,{children:["\n",(0,a.jsx)(r.p,{children:"It also manages:"}),"\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsxs)(r.li,{children:["The ",(0,a.jsx)(r.strong,{children:"SQLContext"})," (for structured queries)"]}),"\n",(0,a.jsxs)(r.li,{children:["The ",(0,a.jsx)(r.strong,{children:"Catalog"})," (for metadata and table management)"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"\ud83e\uddfe Example:"})}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:'from pyspark.sql import SparkSession\r\n\r\n# Create SparkSession\r\nspark = SparkSession.builder \\\r\n    .appName("ExampleSession") \\\r\n    .config("spark.some.config.option", "some-value") \\\r\n    .getOrCreate()\r\n\r\n# Load data\r\ndf = spark.read.json("data.json")\r\n\r\n# Perform SQL operations\r\ndf.createOrReplaceTempView("people")\r\nspark.sql("SELECT name FROM people WHERE age > 30").show()\n'})}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"\ud83d\udcac Summary:"})}),"\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.code,{children:"SparkSession"})," = ",(0,a.jsx)(r.strong,{children:"Unified Entry Point"})," for all Spark operations."]}),"\n",(0,a.jsx)(r.li,{children:"Simplifies working with structured data, SQL, and configuration."}),"\n",(0,a.jsx)(r.li,{children:"Without it, no PySpark job can run."}),"\n"]}),"\n",(0,a.jsx)(r.h2,{id:"3-what-is-dataframe-and-how-do-you-create-a-dataframe-in-pyspark",children:(0,a.jsx)(r.strong,{children:"3. What is DataFrame and How do you create a DataFrame in PySpark?"})}),"\n",(0,a.jsxs)(r.p,{children:["In ",(0,a.jsx)(r.strong,{children:"PySpark"}),", a ",(0,a.jsx)(r.strong,{children:"DataFrame"})," is a ",(0,a.jsx)(r.strong,{children:"distributed collection of data organized into named columns"}),", similar to a relational table in SQL or a DataFrame in Pandas \u2014 but capable of handling data at scale across a cluster."]}),"\n",(0,a.jsxs)(r.h3,{id:"you-can-create-a-dataframe-in-pyspark-in-three-main-ways",children:["\u2705You can create a DataFrame in PySpark in ",(0,a.jsx)(r.strong,{children:"three main ways"}),":"]}),"\n",(0,a.jsx)(r.h4,{id:"1\ufe0f\u20e3-from-a-structured-data-source-like-csv-json-parquet-etc",children:(0,a.jsx)(r.strong,{children:"1\ufe0f\u20e3 From a Structured Data Source (like CSV, JSON, Parquet, etc.)"})}),"\n",(0,a.jsxs)(r.p,{children:["Using the ",(0,a.jsx)(r.code,{children:"spark.read"})," API:"]}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:'from pyspark.sql import SparkSession\r\n\r\nspark = SparkSession.builder.appName("DataFrameExample").getOrCreate()\r\n\r\n# Read from CSV\r\ndf_csv = spark.read.csv("orders.csv", header=True, inferSchema=True)\r\n\r\n# Read from JSON\r\ndf_json = spark.read.json("orders.json")\r\n\r\ndf_csv.show(5)\n'})}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Explanation:"})}),"\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.code,{children:"header=True"})," \u2192 tells Spark to use the first row as column names"]}),"\n",(0,a.jsxs)(r.li,{children:[(0,a.jsx)(r.code,{children:"inferSchema=True"})," \u2192 automatically detects data types"]}),"\n",(0,a.jsx)(r.li,{children:"These files are distributed and loaded in parallel across worker nodes"}),"\n"]}),"\n",(0,a.jsx)(r.hr,{}),"\n",(0,a.jsx)(r.h4,{id:"2\ufe0f\u20e3-from-a-python-collection-list-dictionary-or-rdd",children:(0,a.jsx)(r.strong,{children:"2\ufe0f\u20e3 From a Python Collection (List, Dictionary, or RDD)"})}),"\n",(0,a.jsx)(r.p,{children:"You can manually create a small DataFrame using local Python data \u2014 useful for testing or demos:"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:'data = [("Alice", 25), ("Bob", 30), ("Catherine", 29)]\r\ncolumns = ["Name", "Age"]\r\n\r\ndf_manual = spark.createDataFrame(data, columns)\r\ndf_manual.show()\n'})}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Or from an RDD:"})}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"rdd = spark.sparkContext.parallelize(data)\r\ndf_rdd = rdd.toDF(columns)\n"})}),"\n",(0,a.jsx)(r.hr,{}),"\n",(0,a.jsx)(r.h4,{id:"3\ufe0f\u20e3-from-an-external-database-or-data-warehouse",children:(0,a.jsx)(r.strong,{children:"3\ufe0f\u20e3 From an External Database or Data Warehouse"})}),"\n",(0,a.jsx)(r.p,{children:"You can connect to JDBC sources such as MySQL, PostgreSQL, or Snowflake:"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:'df_db = spark.read.format("jdbc") \\\r\n    .option("url", "jdbc:mysql://localhost:3306/sales_db") \\\r\n    .option("dbtable", "orders") \\\r\n    .option("user", "root") \\\r\n    .option("password", "mypassword") \\\r\n    .load()\n'})}),"\n",(0,a.jsx)(r.h2,{id:"4-explain-the-difference-between-rdd-and-dataframe",children:(0,a.jsx)(r.strong,{children:"4. Explain the difference between RDD and DataFrame?"})}),"\n",(0,a.jsx)(r.p,{children:"In PySpark, both RDDs and DataFrames are fundamental abstractions for working with data \u2014\r\nbut they differ significantly in abstraction level, performance, and usability."}),"\n",(0,a.jsxs)(r.table,{children:[(0,a.jsx)(r.thead,{children:(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.th,{children:(0,a.jsx)(r.strong,{children:"Feature"})}),(0,a.jsx)(r.th,{children:(0,a.jsx)(r.strong,{children:"RDD (Resilient Distributed Dataset)"})}),(0,a.jsx)(r.th,{children:(0,a.jsx)(r.strong,{children:"DataFrame"})})]})}),(0,a.jsxs)(r.tbody,{children:[(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.strong,{children:"Definition"})}),(0,a.jsx)(r.td,{children:"A low-level distributed collection of objects; the fundamental Spark data structure."}),(0,a.jsxs)(r.td,{children:["A distributed collection of ",(0,a.jsx)(r.strong,{children:"rows"})," with ",(0,a.jsx)(r.strong,{children:"named columns"})," \u2014 built on top of RDDs."]})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.strong,{children:"Abstraction Level"})}),(0,a.jsx)(r.td,{children:"Low-level API \u2014 requires manual transformations and actions."}),(0,a.jsx)(r.td,{children:"High-level API \u2014 designed for structured data operations."})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.strong,{children:"Data Structure Type"})}),(0,a.jsx)(r.td,{children:"Unstructured \u2014 can hold any type of Python, Scala, or Java objects."}),(0,a.jsx)(r.td,{children:"Structured \u2014 data is organized in tabular form like a SQL table."})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.strong,{children:"Schema"})}),(0,a.jsx)(r.td,{children:"No schema; user must manage data types manually."}),(0,a.jsx)(r.td,{children:"Has an explicit schema (column names and data types)."})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.strong,{children:"Ease of Use"})}),(0,a.jsx)(r.td,{children:"Complex to code; requires more lines and transformations."}),(0,a.jsx)(r.td,{children:"User-friendly \u2014 supports SQL-like operations and DataFrame APIs."})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.strong,{children:"Optimization"})}),(0,a.jsx)(r.td,{children:"No automatic optimization; user must handle logic."}),(0,a.jsxs)(r.td,{children:["Automatically optimized by Spark\u2019s ",(0,a.jsx)(r.strong,{children:"Catalyst Optimizer"}),"."]})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.strong,{children:"Performance"})}),(0,a.jsx)(r.td,{children:"Slower due to lack of optimization and serialization overhead."}),(0,a.jsxs)(r.td,{children:["Faster due to ",(0,a.jsx)(r.strong,{children:"Tungsten execution engine"})," and optimized query planning."]})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.strong,{children:"Data Representation"})}),(0,a.jsx)(r.td,{children:"Distributed collection of Java/Python objects."}),(0,a.jsxs)(r.td,{children:["Distributed collection of ",(0,a.jsx)(r.strong,{children:"Rows"})," with ",(0,a.jsx)(r.strong,{children:"named columns"}),"."]})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.strong,{children:"Use Case"})}),(0,a.jsxs)(r.td,{children:["Best for ",(0,a.jsx)(r.strong,{children:"low-level transformations"}),", custom computations, or when schema is unknown."]}),(0,a.jsxs)(r.td,{children:["Best for ",(0,a.jsx)(r.strong,{children:"structured data processing"}),", ",(0,a.jsx)(r.strong,{children:"ETL"}),", and ",(0,a.jsx)(r.strong,{children:"SQL operations"}),"."]})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.strong,{children:"Interoperability"})}),(0,a.jsx)(r.td,{children:"Harder to integrate with SQL or MLlib directly."}),(0,a.jsxs)(r.td,{children:["Integrates seamlessly with ",(0,a.jsx)(r.strong,{children:"Spark SQL"}),", ",(0,a.jsx)(r.strong,{children:"MLlib"}),", and ",(0,a.jsx)(r.strong,{children:"GraphFrames"}),"."]})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.strong,{children:"API Type"})}),(0,a.jsx)(r.td,{children:"Functional (map, flatMap, filter, reduce)."}),(0,a.jsx)(r.td,{children:"Declarative (select, filter, groupBy, join)."})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:(0,a.jsx)(r.strong,{children:"Example Syntax"})}),(0,a.jsx)(r.td,{children:(0,a.jsx)(r.code,{children:"rdd.filter(lambda x: x > 5)"})}),(0,a.jsxs)(r.td,{children:[(0,a.jsx)(r.code,{children:"df.filter(df.value > 5)"})," or ",(0,a.jsx)(r.code,{children:'spark.sql("SELECT * FROM table WHERE value > 5")'})]})]})]})]}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"\ud83d\udcacSummary:"})}),"\n",(0,a.jsxs)(r.ul,{children:["\n",(0,a.jsx)(r.li,{children:"\u201cRDDs give you full control but demand more effort."}),"\n",(0,a.jsx)(r.li,{children:"DataFrames give you structured power and performance optimization with minimal code.\u201d"}),"\n"]}),"\n",(0,a.jsx)(r.h2,{id:"5-how-do-you-check-the-version-of-pyspark",children:(0,a.jsx)(r.strong,{children:"5. How do you check the version of PySpark?"})}),"\n",(0,a.jsxs)(r.p,{children:["You can check the ",(0,a.jsx)(r.strong,{children:"PySpark version"})," using any of the following methods, depending on your environment."]}),"\n",(0,a.jsx)(r.h3,{id:"1\ufe0f\u20e3-check-version-using-pyspark-shell-or-script",children:(0,a.jsx)(r.strong,{children:"1\ufe0f\u20e3 Check version using PySpark shell or script"})}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:"import pyspark\r\nprint(pyspark.__version__)\n"})}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Output Example:"})}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{children:"3.5.2\n"})}),"\n",(0,a.jsxs)(r.p,{children:["This prints the version of the ",(0,a.jsx)(r.strong,{children:"installed PySpark package"})," in your environment."]}),"\n",(0,a.jsx)(r.h3,{id:"2\ufe0f\u20e3-check-version-from-sparksession",children:(0,a.jsx)(r.strong,{children:"2\ufe0f\u20e3 Check version from SparkSession"})}),"\n",(0,a.jsx)(r.p,{children:"If you\u2019ve already created a SparkSession:"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-python",children:'from pyspark.sql import SparkSession\r\n\r\nspark = SparkSession.builder.appName("VersionCheck").getOrCreate()\r\nprint(spark.version)\n'})}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Output Example:"})}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{children:"3.5.2\n"})}),"\n",(0,a.jsxs)(r.p,{children:["Here, ",(0,a.jsx)(r.code,{children:"spark.version"})," gives the version of the ",(0,a.jsx)(r.strong,{children:"Spark runtime"})," (the engine your code is actually running on)."]}),"\n",(0,a.jsxs)(r.p,{children:["Sometimes, the installed PySpark version (",(0,a.jsx)(r.code,{children:"pyspark.__version__"}),") and Spark runtime version (",(0,a.jsx)(r.code,{children:"spark.version"}),") may differ slightly if your environment is misconfigured \u2014 checking both is a good practice."]}),"\n",(0,a.jsx)(r.h3,{id:"3\ufe0f\u20e3-check-via-command-line-cli",children:(0,a.jsx)(r.strong,{children:"3\ufe0f\u20e3 Check via Command Line (CLI)"})}),"\n",(0,a.jsx)(r.p,{children:"If you\u2019re using a local or cluster terminal:"}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{className:"language-bash",children:"pyspark --version\n"})}),"\n",(0,a.jsx)(r.p,{children:(0,a.jsx)(r.strong,{children:"Output Example:"})}),"\n",(0,a.jsx)(r.pre,{children:(0,a.jsx)(r.code,{children:"Welcome to\r\n      ____              __\r\n     / __/__  ___ _____/ /__\r\n    _\\ \\/ _ \\/ _ `/ __/  '_/\r\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.2\r\n      /_/\r\nUsing Scala version 2.12.15, Java HotSpot(TM) 64-Bit Server VM, 11.0.22\n"})}),"\n",(0,a.jsxs)(r.p,{children:["This shows both the ",(0,a.jsx)(r.strong,{children:"Spark version"})," and ",(0,a.jsx)(r.strong,{children:"Scala version"})," used by your PySpark installation."]}),"\n",(0,a.jsxs)(r.h3,{id:"-quick-summary-table",children:["\ud83d\udccb ",(0,a.jsx)(r.strong,{children:"Quick Summary Table"})]}),"\n",(0,a.jsxs)(r.table,{children:[(0,a.jsx)(r.thead,{children:(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.th,{children:(0,a.jsx)(r.strong,{children:"Method"})}),(0,a.jsx)(r.th,{children:(0,a.jsx)(r.strong,{children:"Command"})}),(0,a.jsx)(r.th,{children:(0,a.jsx)(r.strong,{children:"Output Example"})}),(0,a.jsx)(r.th,{children:(0,a.jsx)(r.strong,{children:"Checks"})})]})}),(0,a.jsxs)(r.tbody,{children:[(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:"Python Package"}),(0,a.jsx)(r.td,{children:(0,a.jsx)(r.code,{children:"pyspark.__version__"})}),(0,a.jsx)(r.td,{children:(0,a.jsx)(r.code,{children:"3.5.2"})}),(0,a.jsx)(r.td,{children:"Installed PySpark package version"})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:"SparkSession"}),(0,a.jsx)(r.td,{children:(0,a.jsx)(r.code,{children:"spark.version"})}),(0,a.jsx)(r.td,{children:(0,a.jsx)(r.code,{children:"3.5.2"})}),(0,a.jsx)(r.td,{children:"Running Spark engine version"})]}),(0,a.jsxs)(r.tr,{children:[(0,a.jsx)(r.td,{children:"Command Line"}),(0,a.jsx)(r.td,{children:(0,a.jsx)(r.code,{children:"pyspark --version"})}),(0,a.jsx)(r.td,{children:"Version details + Scala info"}),(0,a.jsx)(r.td,{children:"CLI environment setup"})]})]})]})]})}function h(e={}){const{wrapper:r}={...(0,i.R)(),...e.components};return r?(0,a.jsx)(r,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,r,n)=>{n.d(r,{R:()=>t,x:()=>o});var s=n(6540);const a={},i=s.createContext(a);function t(e){const r=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function o(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),s.createElement(i.Provider,{value:r},e.children)}}}]);