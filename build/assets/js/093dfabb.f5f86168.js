"use strict";(self.webpackChunkdatacraft_school=self.webpackChunkdatacraft_school||[]).push([[4284],{2874:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>o,contentTitle:()=>l,default:()=>c,frontMatter:()=>i,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"pyspark-aggregation","title":"Data Aggregation in PySpark DataFrames (Complete Guide)","description":"Learn how to perform data aggregation in PySpark using groupBy, agg, max, sum, avg, distinct, and sorting operations with real shipment dataset examples.","source":"@site/docs-pyspark/pyspark-aggregation.md","sourceDirName":".","slug":"/pyspark/aggregation","permalink":"/pyspark/pyspark/aggregation","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"PySpark","permalink":"/pyspark/tags/py-spark"},{"inline":true,"label":"Apache Spark","permalink":"/pyspark/tags/apache-spark"},{"inline":true,"label":"Big Data","permalink":"/pyspark/tags/big-data"},{"inline":true,"label":"Spark Basics","permalink":"/pyspark/tags/spark-basics"},{"inline":true,"label":"Cluster Computing","permalink":"/pyspark/tags/cluster-computing"},{"inline":true,"label":"Spark Architecture","permalink":"/pyspark/tags/spark-architecture"},{"inline":true,"label":"Driver Program","permalink":"/pyspark/tags/driver-program"},{"inline":true,"label":"Executors","permalink":"/pyspark/tags/executors"},{"inline":true,"label":"Cluster Manager","permalink":"/pyspark/tags/cluster-manager"},{"inline":true,"label":"SparkSession","permalink":"/pyspark/tags/spark-session"},{"inline":true,"label":"SparkContext","permalink":"/pyspark/tags/spark-context"},{"inline":true,"label":"RDD","permalink":"/pyspark/tags/rdd"},{"inline":true,"label":"RDD Transformations","permalink":"/pyspark/tags/rdd-transformations"},{"inline":true,"label":"RDD Actions","permalink":"/pyspark/tags/rdd-actions"},{"inline":true,"label":"Key-Value RDD","permalink":"/pyspark/tags/key-value-rdd"},{"inline":true,"label":"RDD Caching","permalink":"/pyspark/tags/rdd-caching"}],"version":"current","frontMatter":{"id":"pyspark-aggregation","title":"Data Aggregation in PySpark DataFrames (Complete Guide)","sidebar_label":"Dataframe Aggregation","slug":"/pyspark/aggregation","description":"Learn how to perform data aggregation in PySpark using groupBy, agg, max, sum, avg, distinct, and sorting operations with real shipment dataset examples.","keywords":["pyspark aggregation","pyspark groupBy","pyspark avg sum max","pyspark countDistinct","pyspark sorting","spark dataframe aggregation","pyspark shipments dataset","big data transformations"],"og:title":"Data Aggregation in PySpark \u2014 Full Tutorial with Examples","og:description":"A complete guide to performing data aggregation in PySpark including groupBy, agg, max, sum, avg, distinct counts, and sorting.","tags":["PySpark","Apache Spark","Big Data","Spark Basics","Cluster Computing","Spark Architecture","Driver Program","Executors","Cluster Manager","SparkSession","SparkContext","RDD","RDD Transformations","RDD Actions","Key-Value RDD","RDD Caching"]},"sidebar":"tutorialSidebar","previous":{"title":"DataFrame Joins","permalink":"/pyspark/pyspark/joins"},"next":{"title":"Window Functions","permalink":"/pyspark/df-window-functions"}}');var s=n(4848),t=n(8453);const i={id:"pyspark-aggregation",title:"Data Aggregation in PySpark DataFrames (Complete Guide)",sidebar_label:"Dataframe Aggregation",slug:"/pyspark/aggregation",description:"Learn how to perform data aggregation in PySpark using groupBy, agg, max, sum, avg, distinct, and sorting operations with real shipment dataset examples.",keywords:["pyspark aggregation","pyspark groupBy","pyspark avg sum max","pyspark countDistinct","pyspark sorting","spark dataframe aggregation","pyspark shipments dataset","big data transformations"],"og:title":"Data Aggregation in PySpark \u2014 Full Tutorial with Examples","og:description":"A complete guide to performing data aggregation in PySpark including groupBy, agg, max, sum, avg, distinct counts, and sorting.",tags:["PySpark","Apache Spark","Big Data","Spark Basics","Cluster Computing","Spark Architecture","Driver Program","Executors","Cluster Manager","SparkSession","SparkContext","RDD","RDD Transformations","RDD Actions","Key-Value RDD","RDD Caching"]},l="Data Aggregation in PySpark DataFrames",o={},d=[{value:"\u2714\ufe0f Why This Is Needed",id:"\ufe0f-why-this-is-needed",level:3},{value:"1. Group by Company (no aggregation yet)",id:"1-group-by-company-no-aggregation-yet",level:2},{value:"\u2714\ufe0f Why We Need This",id:"\ufe0f-why-we-need-this",level:3},{value:"2. Average of all numeric columns per Company",id:"2-average-of-all-numeric-columns-per-company",level:2},{value:"\u2714\ufe0f Why We Need This",id:"\ufe0f-why-we-need-this-1",level:3},{value:"3. Count of shipments per Company",id:"3-count-of-shipments-per-company",level:2},{value:"\u2714\ufe0f Why We Need This",id:"\ufe0f-why-we-need-this-2",level:3},{value:"4. Maximum values per Company",id:"4-maximum-values-per-company",level:2},{value:"5. Total sales across all rows",id:"5-total-sales-across-all-rows",level:2},{value:"6. Maximum sale value",id:"6-maximum-sale-value",level:2},{value:"7. Max Sales per Company (group then aggregate)",id:"7-max-sales-per-company-group-then-aggregate",level:2},{value:"8. Average Sales using <code>avg()</code>",id:"8-average-sales-using-avg",level:2},{value:"9. Count distinct Sales values",id:"9-count-distinct-sales-values",level:2},{value:"10. Sort by Sales (ascending)",id:"10-sort-by-sales-ascending",level:2},{value:"11. Sort by Sales (descending)",id:"11-sort-by-sales-descending",level:2}];function p(e){const a={br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(a.header,{children:(0,s.jsx)(a.h1,{id:"data-aggregation-in-pyspark-dataframes",children:"Data Aggregation in PySpark DataFrames"})}),"\n",(0,s.jsxs)(a.p,{children:["Data aggregation is one of the most powerful operations in PySpark.",(0,s.jsx)(a.br,{}),"\n","Whether you're analyzing shipments, sales, demand planning, logs, or events \u2014 aggregation is essential for summarizing large datasets."]}),"\n",(0,s.jsxs)(a.p,{children:["This guide walks through ",(0,s.jsx)(a.strong,{children:"groupBy"}),", ",(0,s.jsx)(a.strong,{children:"aggregations"}),", ",(0,s.jsx)(a.strong,{children:"sorting"}),", and ",(0,s.jsx)(a.strong,{children:"built-in PySpark aggregation functions"})," with a real shipments dataset."]}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h1,{id:"loading-the-shipments-dataset",children:"Loading the Shipments Dataset"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"from pyspark.sql import SparkSession\r\n\r\nspark = SparkSession.builder.appName('agg').getOrCreate()\r\n\r\ndf = spark.read.csv('/path/to/shipments.csv', header=True, inferSchema=True)\n"})}),"\n",(0,s.jsx)(a.h3,{id:"\ufe0f-why-this-is-needed",children:"\u2714\ufe0f Why This Is Needed"}),"\n",(0,s.jsx)(a.p,{children:"To start a Spark session, load the dataset, and auto-infer column types."}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h1,{id:"preview--schema",children:"Preview & Schema"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"df.show()\n"})}),"\n",(0,s.jsx)(a.p,{children:"Example dataset:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-text",children:"+----------+--------+----------+-----+-----+\r\n|ShipmentID|Company | Product  |Units|Sales|\r\n+----------+--------+----------+-----+-----+\r\n|     S001 | FedEx  |  Soap    | 100 |1500 |\r\n|     S002 | FedEx  | Shampoo  | 200 |3000 |\r\n|     S003 |BlueDart|  Bread   | 150 |1800 |\r\n|     S004 |  DHL   |Toothpaste| 120 |2400 |\r\n|     S005 |  DHL   |  Rice    | 300 |6000 |\r\n|     S006 |BlueDart|Chocolate | 180 |3600 |\r\n|     S007 | FedEx  |  Juice   | 130 |2600 |\r\n|     S008 |  DHL   | Cereal   | 220 |4400 |\r\n|     S009 |BlueDart|  Soda    | 110 |2200 |\r\n|     S010 | FedEx  |Facewash  | 140 |2800 |\r\n+----------+--------+----------+-----+-----+\n"})}),"\n",(0,s.jsx)(a.p,{children:"Schema:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"df.printSchema()\n"})}),"\n",(0,s.jsx)(a.p,{children:"Output:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"root\r\n |-- ShipmentID: string (nullable = true)\r\n |-- Company: string (nullable = true)\r\n |-- Product: string (nullable = true)\r\n |-- Units: integer (nullable = true)\r\n |-- Sales: integer (nullable = true)\n"})}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h1,{id:"aggregation-examples",children:"Aggregation Examples"}),"\n",(0,s.jsx)(a.h2,{id:"1-group-by-company-no-aggregation-yet",children:"1. Group by Company (no aggregation yet)"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'df.groupBy("Company")\n'})}),"\n",(0,s.jsx)(a.p,{children:"Returns:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{children:"GroupedData object \u2014 must apply aggregation such as .count(), .avg(), .max(), etc.\n"})}),"\n",(0,s.jsx)(a.h3,{id:"\ufe0f-why-we-need-this",children:"\u2714\ufe0f Why We Need This"}),"\n",(0,s.jsx)(a.p,{children:"To prepare the data for summarization by company."}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h2,{id:"2-average-of-all-numeric-columns-per-company",children:"2. Average of all numeric columns per Company"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'df.groupBy("Company").mean().show()\n'})}),"\n",(0,s.jsx)(a.p,{children:"Output:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-sql",children:"+--------+-----------+-----------+\r\n|Company |avg(Units) |avg(Sales) |\r\n+--------+-----------+-----------+\r\n|BlueDart|  146.6667 | 2533.3333 |\r\n|   FedEx|  142.5    | 2475.0    |\r\n|     DHL|  213.3333 | 4266.6667 |\r\n+--------+-----------+-----------+\n"})}),"\n",(0,s.jsx)(a.h3,{id:"\ufe0f-why-we-need-this-1",children:"\u2714\ufe0f Why We Need This"}),"\n",(0,s.jsx)(a.p,{children:"To quickly compute per-company performance metrics."}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h2,{id:"3-count-of-shipments-per-company",children:"3. Count of shipments per Company"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'df.groupBy("Company").count().show()\n'})}),"\n",(0,s.jsx)(a.p,{children:"Output:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-sql",children:"+--------+-----+\r\n|Company |count|\r\n+--------+-----+\r\n|BlueDart|  3  |\r\n|   FedEx|  4  |\r\n|     DHL|  3  |\r\n+--------+-----+\n"})}),"\n",(0,s.jsx)(a.h3,{id:"\ufe0f-why-we-need-this-2",children:"\u2714\ufe0f Why We Need This"}),"\n",(0,s.jsx)(a.p,{children:"To understand shipment volume distribution."}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h2,{id:"4-maximum-values-per-company",children:"4. Maximum values per Company"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'df.groupBy("Company").max().show()\n'})}),"\n",(0,s.jsx)(a.p,{children:"Output:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-sql",children:"+--------+-----------+-----------+\r\n|Company |max(Units) |max(Sales) |\r\n+--------+-----------+-----------+\r\n|BlueDart|    180    |   3600    |\r\n|   FedEx|    200    |   3000    |\r\n|     DHL|    300    |   6000    |\r\n+--------+-----------+-----------+\n"})}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h2,{id:"5-total-sales-across-all-rows",children:"5. Total sales across all rows"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"df.agg({'Sales': 'sum'}).show()\n"})}),"\n",(0,s.jsx)(a.p,{children:"Output:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-sql",children:"+----------+\r\n|sum(Sales)|\r\n+----------+\r\n|  30300   |\r\n+----------+\n"})}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h2,{id:"6-maximum-sale-value",children:"6. Maximum sale value"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"df.agg({'Sales': 'max'}).show()\n"})}),"\n",(0,s.jsx)(a.p,{children:"Output:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-sql",children:"+----------+\r\n|max(Sales)|\r\n+----------+\r\n|   6000   |\r\n+----------+\n"})}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h2,{id:"7-max-sales-per-company-group-then-aggregate",children:"7. Max Sales per Company (group then aggregate)"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"group_data = df.groupBy(\"Company\")\r\ngroup_data.agg({'Sales': 'max'}).show()\n"})}),"\n",(0,s.jsx)(a.p,{children:"Output:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-sql",children:"+--------+----------+\r\n|Company |max(Sales)|\r\n+--------+----------+\r\n|BlueDart|   3600   |\r\n|   FedEx|   3000   |\r\n|     DHL|   6000   |\r\n+--------+----------+\n"})}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h1,{id:"using-built-in-functions",children:"Using Built-In Functions"}),"\n",(0,s.jsxs)(a.h2,{id:"8-average-sales-using-avg",children:["8. Average Sales using ",(0,s.jsx)(a.code,{children:"avg()"})]}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"from pyspark.sql.functions import avg\r\n\r\ndf.select(avg('Sales').alias('Average Sales')).show()\n"})}),"\n",(0,s.jsx)(a.p,{children:"Output:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-sql",children:"+-------------+\r\n|Average Sales|\r\n+-------------+\r\n|    3030.0   |\r\n+-------------+\n"})}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h2,{id:"9-count-distinct-sales-values",children:"9. Count distinct Sales values"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"from pyspark.sql.functions import countDistinct\r\n\r\ndf.select(countDistinct('Sales')).show()\n"})}),"\n",(0,s.jsx)(a.p,{children:"Output:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-sql",children:"+---------------------+\r\n|count(DISTINCT Sales)|\r\n+---------------------+\r\n|         10          |\r\n+---------------------+\n"})}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h1,{id:"sorting--ordering-data",children:"Sorting & Ordering Data"}),"\n",(0,s.jsx)(a.h2,{id:"10-sort-by-sales-ascending",children:"10. Sort by Sales (ascending)"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"df.orderBy('Sales').show()\n"})}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h2,{id:"11-sort-by-sales-descending",children:"11. Sort by Sales (descending)"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"df.orderBy(df['Sales'].desc()).show()\n"})}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.h1,{id:"-1-minute-summary--pyspark-data-aggregation",children:"\ud83d\udfe6 1-Minute Summary \u2014 PySpark Data Aggregation"}),"\n",(0,s.jsxs)(a.table,{children:[(0,s.jsx)(a.thead,{children:(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.th,{children:"Code Snippet"}),(0,s.jsx)(a.th,{children:"What It Does"})]})}),(0,s.jsxs)(a.tbody,{children:[(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.code,{children:'df.groupBy("Company")'})}),(0,s.jsx)(a.td,{children:"Groups rows by company"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.code,{children:"groupBy().mean()"})}),(0,s.jsx)(a.td,{children:"Calculates average of numeric columns"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.code,{children:"groupBy().count()"})}),(0,s.jsx)(a.td,{children:"Counts number of shipments per company"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.code,{children:"groupBy().max()"})}),(0,s.jsx)(a.td,{children:"Gets max of each numeric column"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.code,{children:"df.agg({'Sales': 'sum'})"})}),(0,s.jsx)(a.td,{children:"Total sales across dataset"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.code,{children:"df.agg({'Sales': 'max'})"})}),(0,s.jsx)(a.td,{children:"Maximum sale value"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.code,{children:"groupBy().agg({'Sales': 'max'})"})}),(0,s.jsx)(a.td,{children:"Max sales per company"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.code,{children:"avg(), countDistinct()"})}),(0,s.jsx)(a.td,{children:"Built-in aggregation functions"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.code,{children:"orderBy('Sales')"})}),(0,s.jsx)(a.td,{children:"Sorts ascending"})]}),(0,s.jsxs)(a.tr,{children:[(0,s.jsx)(a.td,{children:(0,s.jsx)(a.code,{children:"orderBy(df['Sales'].desc())"})}),(0,s.jsx)(a.td,{children:"Sorts descending"})]})]})]}),"\n",(0,s.jsx)(a.hr,{}),"\n",(0,s.jsx)(a.p,{children:"Next, we\u2019ll explore Window Functions in PySpark DataFrames, enabling running totals, rankings, and time-based calculations."})]})}function c(e={}){const{wrapper:a}={...(0,t.R)(),...e.components};return a?(0,s.jsx)(a,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},8453:(e,a,n)=>{n.d(a,{R:()=>i,x:()=>l});var r=n(6540);const s={},t=r.createContext(s);function i(e){const a=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function l(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),r.createElement(t.Provider,{value:a},e.children)}}}]);